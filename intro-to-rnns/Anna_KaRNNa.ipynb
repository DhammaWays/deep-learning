{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
=======
   "execution_count": 2,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
=======
   "execution_count": 3,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 4,
   "metadata": {},
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 4,
=======
     "execution_count": 3,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
=======
     "execution_count": 4,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
=======
   "execution_count": 5,
   "metadata": {},
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
<<<<<<< HEAD
       "array([80, 77, 26,  7,  4, 11, 81, 27, 38, 54, 54, 54, 74, 26,  7,  7, 44,\n",
       "       27, 53, 26, 73,  8, 58,  8, 11,  0, 27, 26, 81, 11, 27, 26, 58, 58,\n",
       "       27, 26, 58,  8, 36, 11, 41, 27, 11,  1, 11, 81, 44, 27, 64, 57, 77,\n",
       "       26,  7,  7, 44, 27, 53, 26, 73,  8, 58, 44, 27,  8,  0, 27, 64, 57,\n",
       "       77, 26,  7,  7, 44, 27,  8, 57, 27,  8,  4,  0, 27, 71, 52, 57, 54,\n",
       "       52, 26, 44, 23, 54, 54, 45,  1, 11, 81, 44,  4, 77,  8, 57])"
      ]
     },
     "execution_count": 5,
=======
       "array([46, 19, 18, 44, 49, 58,  6, 75, 60, 61, 61, 61, 47, 18, 44, 44, 80,\n",
       "       75, 23, 18, 50, 25, 57, 25, 58,  9, 75, 18,  6, 58, 75, 18, 57, 57,\n",
       "       75, 18, 57, 25, 13, 58, 42, 75, 58, 62, 58,  6, 80, 75, 76, 65, 19,\n",
       "       18, 44, 44, 80, 75, 23, 18, 50, 25, 57, 80, 75, 25,  9, 75, 76, 65,\n",
       "       19, 18, 44, 44, 80, 75, 25, 65, 75, 25, 49,  9, 75, 10, 28, 65, 61,\n",
       "       28, 18, 80, 37, 61, 61, 26, 62, 58,  6, 80, 49, 19, 25, 65], dtype=int32)"
      ]
     },
     "execution_count": 4,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
=======
       "array([55, 63, 69, 22,  6, 76, 45,  5, 16, 35, 35, 35,  2, 69, 22, 22, 33,\n",
       "        5, 58, 69,  1, 29, 43, 29, 76, 78,  5, 69, 45, 76,  5, 69, 43, 43,\n",
       "        5, 69, 43, 29, 17, 76, 34,  5, 76, 48, 76, 45, 33,  5, 39, 12, 63,\n",
       "       69, 22, 22, 33,  5, 58, 69,  1, 29, 43, 33,  5, 29, 78,  5, 39, 12,\n",
       "       63, 69, 22, 22, 33,  5, 29, 12,  5, 29,  6, 78,  5, 52, 36, 12, 35,\n",
       "       36, 69, 33, 61, 35, 35, 44, 48, 76, 45, 33,  6, 63, 29, 12], dtype=int32)"
      ]
     },
     "execution_count": 5,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "Here is where we'll make our mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this:\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "<br>\n",
    "We have our text encoded as integers as one long array in `encoded`. Let's create a function that will give us an iterator for our batches. I like using [generator functions](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/) to do this. Then we can pass `encoded` into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array `arr`, you divide the length of `arr` by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split `arr` into $N$ sequences. You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (`n_seqs` below), let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by `n_steps`. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "```python\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "```\n",
    "where `x` is the input batch and `y` is the target batch.\n",
    "\n",
    "The way I like to do this window is use `range` to take steps of size `n_steps` from $0$ to `arr.shape[1]`, the total number of steps in each sequence. That way, the integers you get from `range` always point to the start of a batch, and each window is `n_steps` wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    # Get the batch size and number of batches we can make\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[55 63 69 22  6 76 45  5 16 35]\n",
      " [ 5 69  1  5 12 52  6  5 56 52]\n",
      " [48 29 12 61 35 35  8 64 76 78]\n",
      " [12  5 24 39 45 29 12 56  5 63]\n",
      " [ 5 29  6  5 29 78 28  5 78 29]\n",
      " [ 5 13  6  5 36 69 78 35 52 12]\n",
      " [63 76 12  5 18 52  1 76  5 58]\n",
      " [34  5 73 39  6  5 12 52 36  5]\n",
      " [ 6  5 29 78 12 79  6 61  5 59]\n",
      " [ 5 78 69 29 24  5  6 52  5 63]]\n",
      "\n",
      "y\n",
      " [[63 69 22  6 76 45  5 16 35 35]\n",
      " [69  1  5 12 52  6  5 56 52 29]\n",
      " [29 12 61 35 35  8 64 76 78 28]\n",
      " [ 5 24 39 45 29 12 56  5 63 29]\n",
      " [29  6  5 29 78 28  5 78 29 45]\n",
      " [13  6  5 36 69 78 35 52 12 43]\n",
      " [76 12  5 18 52  1 76  5 58 52]\n",
      " [ 5 73 39  6  5 12 52 36  5 78]\n",
      " [ 5 29 78 12 79  6 61  5 59 63]\n",
      " [78 69 29 24  5  6 52  5 63 76]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "array([[80, 77, 26,  7,  4, 11, 81, 27, 38, 54, 54, 54, 74, 26,  7,  7, 44,\n",
       "        27, 53, 26, 73,  8, 58,  8, 11,  0, 27, 26, 81, 11, 27, 26, 58, 58,\n",
       "        27, 26, 58,  8, 36, 11, 41, 27, 11,  1, 11, 81, 44, 27, 64, 57],\n",
       "       [27, 26, 73, 27, 57, 71,  4, 27, 75, 71,  8, 57, 75, 27,  4, 71, 27,\n",
       "         0,  4, 26, 44, 19, 40, 27, 26, 57,  0, 52, 11, 81, 11, 60, 27, 39,\n",
       "        57, 57, 26, 19, 27,  0, 73,  8, 58,  8, 57, 75, 19, 27, 20, 64],\n",
       "       [ 1,  8, 57, 23, 54, 54, 40, 76, 11,  0, 19, 27,  8,  4, 66,  0, 27,\n",
       "         0, 11,  4,  4, 58, 11, 60, 23, 27, 63, 77, 11, 27,  7, 81,  8, 82,\n",
       "        11, 27,  8,  0, 27, 73, 26, 75, 57,  8, 53,  8, 82, 11, 57,  4],\n",
       "       [57, 27, 60, 64, 81,  8, 57, 75, 27, 77,  8,  0, 27, 82, 71, 57,  1,\n",
       "        11, 81,  0, 26,  4,  8, 71, 57, 27, 52,  8,  4, 77, 27, 77,  8,  0,\n",
       "        54, 20, 81, 71,  4, 77, 11, 81, 27, 52, 26,  0, 27,  4, 77,  8],\n",
       "       [27,  8,  4, 27,  8,  0, 19, 27,  0,  8, 81, 61, 40, 27,  0, 26,  8,\n",
       "        60, 27,  4, 77, 11, 27, 71, 58, 60, 27, 73, 26, 57, 19, 27, 75, 11,\n",
       "         4,  4,  8, 57, 75, 27, 64,  7, 19, 27, 26, 57, 60, 54, 82, 81],\n",
       "       [27, 70,  4, 27, 52, 26,  0, 54, 71, 57, 58, 44, 27, 52, 77, 11, 57,\n",
       "        27,  4, 77, 11, 27,  0, 26, 73, 11, 27, 11,  1, 11, 57,  8, 57, 75,\n",
       "        27, 77, 11, 27, 82, 26, 73, 11, 27,  4, 71, 27,  4, 77, 11,  8],\n",
       "       [77, 11, 57, 27, 82, 71, 73, 11, 27, 53, 71, 81, 27, 73, 11, 19, 40,\n",
       "        27,  0, 77, 11, 27,  0, 26,  8, 60, 19, 27, 26, 57, 60, 27, 52, 11,\n",
       "        57,  4, 27, 20, 26, 82, 36, 27,  8, 57,  4, 71, 27,  4, 77, 11],\n",
       "       [41, 27, 20, 64,  4, 27, 57, 71, 52, 27,  0, 77, 11, 27, 52, 71, 64,\n",
       "        58, 60, 27, 81, 11, 26, 60,  8, 58, 44, 27, 77, 26,  1, 11, 27,  0,\n",
       "        26, 82, 81,  8, 53,  8, 82, 11, 60, 19, 27, 57, 71,  4, 27, 73],\n",
       "       [ 4, 27,  8,  0, 57, 66,  4, 23, 27, 63, 77, 11, 44, 66, 81, 11, 27,\n",
       "         7, 81, 71,  7, 81,  8, 11,  4, 71, 81,  0, 27, 71, 53, 27, 26, 27,\n",
       "         0, 71, 81,  4, 19, 54, 20, 64,  4, 27, 52, 11, 66, 81, 11, 27],\n",
       "       [27,  0, 26,  8, 60, 27,  4, 71, 27, 77, 11, 81,  0, 11, 58, 53, 19,\n",
       "        27, 26, 57, 60, 27, 20, 11, 75, 26, 57, 27, 26, 75, 26,  8, 57, 27,\n",
       "        53, 81, 71, 73, 27,  4, 77, 11, 27, 20, 11, 75,  8, 57, 57,  8]])"
=======
       "array([[46, 19, 18, 44, 49, 58,  6, 75, 60, 61, 61, 61, 47, 18, 44, 44, 80,\n",
       "        75, 23, 18, 50, 25, 57, 25, 58,  9, 75, 18,  6, 58, 75, 18, 57, 57,\n",
       "        75, 18, 57, 25, 13, 58, 42, 75, 58, 62, 58,  6, 80, 75, 76, 65],\n",
       "       [75, 18, 50, 75, 65, 10, 49, 75, 27, 10, 25, 65, 27, 75, 49, 10, 75,\n",
       "         9, 49, 18, 80, 41,  8, 75, 18, 65,  9, 28, 58,  6, 58, 77, 75, 22,\n",
       "        65, 65, 18, 41, 75,  9, 50, 25, 57, 25, 65, 27, 41, 75, 29, 76],\n",
       "       [62, 25, 65, 37, 61, 61,  8, 51, 58,  9, 41, 75, 25, 49, 66,  9, 75,\n",
       "         9, 58, 49, 49, 57, 58, 77, 37, 75, 21, 19, 58, 75, 44,  6, 25,  3,\n",
       "        58, 75, 25,  9, 75, 50, 18, 27, 65, 25, 23, 25,  3, 58, 65, 49],\n",
       "       [65, 75, 77, 76,  6, 25, 65, 27, 75, 19, 25,  9, 75,  3, 10, 65, 62,\n",
       "        58,  6,  9, 18, 49, 25, 10, 65, 75, 28, 25, 49, 19, 75, 19, 25,  9,\n",
       "        61, 29,  6, 10, 49, 19, 58,  6, 75, 28, 18,  9, 75, 49, 19, 25],\n",
       "       [75, 25, 49, 75, 25,  9, 41, 75,  9, 25,  6, 43,  8, 75,  9, 18, 25,\n",
       "        77, 75, 49, 19, 58, 75, 10, 57, 77, 75, 50, 18, 65, 41, 75, 27, 58,\n",
       "        49, 49, 25, 65, 27, 75, 76, 44, 41, 75, 18, 65, 77, 61,  3,  6],\n",
       "       [75, 35, 49, 75, 28, 18,  9, 61, 10, 65, 57, 80, 75, 28, 19, 58, 65,\n",
       "        75, 49, 19, 58, 75,  9, 18, 50, 58, 75, 58, 62, 58, 65, 25, 65, 27,\n",
       "        75, 19, 58, 75,  3, 18, 50, 58, 75, 49, 10, 75, 49, 19, 58, 25],\n",
       "       [19, 58, 65, 75,  3, 10, 50, 58, 75, 23, 10,  6, 75, 50, 58, 41,  8,\n",
       "        75,  9, 19, 58, 75,  9, 18, 25, 77, 41, 75, 18, 65, 77, 75, 28, 58,\n",
       "        65, 49, 75, 29, 18,  3, 13, 75, 25, 65, 49, 10, 75, 49, 19, 58],\n",
       "       [42, 75, 29, 76, 49, 75, 65, 10, 28, 75,  9, 19, 58, 75, 28, 10, 76,\n",
       "        57, 77, 75,  6, 58, 18, 77, 25, 57, 80, 75, 19, 18, 62, 58, 75,  9,\n",
       "        18,  3,  6, 25, 23, 25,  3, 58, 77, 41, 75, 65, 10, 49, 75, 50],\n",
       "       [49, 75, 25,  9, 65, 66, 49, 37, 75, 21, 19, 58, 80, 66,  6, 58, 75,\n",
       "        44,  6, 10, 44,  6, 25, 58, 49, 10,  6,  9, 75, 10, 23, 75, 18, 75,\n",
       "         9, 10,  6, 49, 41, 61, 29, 76, 49, 75, 28, 58, 66,  6, 58, 75],\n",
       "       [75,  9, 18, 25, 77, 75, 49, 10, 75, 19, 58,  6,  9, 58, 57, 23, 41,\n",
       "        75, 18, 65, 77, 75, 29, 58, 27, 18, 65, 75, 18, 27, 18, 25, 65, 75,\n",
       "        23,  6, 10, 50, 75, 49, 19, 58, 75, 29, 58, 27, 25, 65, 65, 25]], dtype=int32)"
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
=======
    "If you implemented `get_batches` correctly, the above output should look something like \n",
    "```\n",
    "x\n",
    " [[55 63 69 22  6 76 45  5 16 35]\n",
    " [ 5 69  1  5 12 52  6  5 56 52]\n",
    " [48 29 12 61 35 35  8 64 76 78]\n",
    " [12  5 24 39 45 29 12 56  5 63]\n",
    " [ 5 29  6  5 29 78 28  5 78 29]\n",
    " [ 5 13  6  5 36 69 78 35 52 12]\n",
    " [63 76 12  5 18 52  1 76  5 58]\n",
    " [34  5 73 39  6  5 12 52 36  5]\n",
    " [ 6  5 29 78 12 79  6 61  5 59]\n",
    " [ 5 78 69 29 24  5  6 52  5 63]]\n",
    "\n",
    "y\n",
    " [[63 69 22  6 76 45  5 16 35 35]\n",
    " [69  1  5 12 52  6  5 56 52 29]\n",
    " [29 12 61 35 35  8 64 76 78 28]\n",
    " [ 5 24 39 45 29 12 56  5 63 29]\n",
    " [29  6  5 29 78 28  5 78 29 45]\n",
    " [13  6  5 36 69 78 35 52 12 43]\n",
    " [76 12  5 18 52  1 76  5 58 52]\n",
    " [ 5 73 39  6  5 12 52 36  5 78]\n",
    " [ 5 29 78 12 79  6 61  5 59 63]\n",
    " [78 69 29 24  5  6 52  5 63 76]]\n",
    " ```\n",
    " although the exact numbers will be different. Check to make sure the data is shifted over one step for `y`."
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is where you'll build the network. We'll break it up into parts so it's easier to reason about each bit. Then we can connect them up into the whole network.\n",
    "\n",
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "\n",
    "### Inputs\n",
    "\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called `keep_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    ''' Define placeholders for inputs, targets, and dropout \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size: Batch size, number of sequences per batch\n",
    "        num_steps: Number of sequence steps in a batch\n",
    "        \n",
    "    '''\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "```python\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "```\n",
    "\n",
    "where `num_units` is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with \n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "```\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/MultiRNNCell). With this, you pass in a list of cells and it will send the output of one cell into the next cell. For example,\n",
    "\n",
    "```python\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "```\n",
    "\n",
    "This might look a little weird if you know Python well because this will create a list of the same `cell` object. However, TensorFlow will create different weight matrices for all `cell` objects. Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "```python\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "```\n",
    "\n",
    "Below, we implement the `build_lstm` function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    ### Build the LSTM Cell\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$.\n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with `tf.variable_scope(scope_name)` because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        x: Input tensor\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # That is, the shape should be batch_size*num_steps rows by lstm_size columns\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through `tf.nn.softmax_cross_entropy_with_logits` and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per batch_size per step\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the network\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Run each sequence step through the RNN and collect the outputs\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "#batch_size = 100\n",
    "#num_steps = 100 \n",
    "#lstm_size = 512\n",
    "#num_layers = 2\n",
    "#learning_rate = 0.001\n",
    "#keep_prob = 0.5\n",
    "batch_size = 100\n",
    "num_steps = 50 \n",
    "lstm_size = 128\n",
=======
    "batch_size = 10\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "keep_prob = 0.5"
=======
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time for training\n",
    "\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 13,
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
=======
   "execution_count": 21,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "Epoch 1/10  Iteration 1/3570 Training loss: 4.4201 1.0210 sec/batch\n",
      "Epoch 1/10  Iteration 2/3570 Training loss: 4.3722 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 3/3570 Training loss: 4.1060 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 4/3570 Training loss: 3.9203 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 5/3570 Training loss: 3.7981 0.1363 sec/batch\n",
      "Epoch 1/10  Iteration 6/3570 Training loss: 3.7093 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 7/3570 Training loss: 3.6392 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 8/3570 Training loss: 3.5918 0.1362 sec/batch\n",
      "Epoch 1/10  Iteration 9/3570 Training loss: 3.5521 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 10/3570 Training loss: 3.5167 0.1352 sec/batch\n",
      "Epoch 1/10  Iteration 11/3570 Training loss: 3.4878 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 12/3570 Training loss: 3.4610 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 13/3570 Training loss: 3.4370 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 14/3570 Training loss: 3.4185 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 15/3570 Training loss: 3.3982 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 16/3570 Training loss: 3.3834 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 17/3570 Training loss: 3.3686 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 18/3570 Training loss: 3.3547 0.1351 sec/batch\n",
      "Epoch 1/10  Iteration 19/3570 Training loss: 3.3425 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 20/3570 Training loss: 3.3320 0.1440 sec/batch\n",
      "Epoch 1/10  Iteration 21/3570 Training loss: 3.3239 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 22/3570 Training loss: 3.3159 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 23/3570 Training loss: 3.3082 0.1410 sec/batch\n",
      "Epoch 1/10  Iteration 24/3570 Training loss: 3.3020 0.1460 sec/batch\n",
      "Epoch 1/10  Iteration 25/3570 Training loss: 3.2954 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 26/3570 Training loss: 3.2891 0.1420 sec/batch\n",
      "Epoch 1/10  Iteration 27/3570 Training loss: 3.2821 0.1470 sec/batch\n",
      "Epoch 1/10  Iteration 28/3570 Training loss: 3.2762 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 29/3570 Training loss: 3.2707 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 30/3570 Training loss: 3.2650 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 31/3570 Training loss: 3.2598 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 32/3570 Training loss: 3.2546 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 33/3570 Training loss: 3.2500 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 34/3570 Training loss: 3.2452 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 35/3570 Training loss: 3.2406 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 36/3570 Training loss: 3.2365 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 37/3570 Training loss: 3.2323 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 38/3570 Training loss: 3.2280 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 39/3570 Training loss: 3.2234 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 40/3570 Training loss: 3.2190 0.1363 sec/batch\n",
      "Epoch 1/10  Iteration 41/3570 Training loss: 3.2144 0.1396 sec/batch\n",
      "Epoch 1/10  Iteration 42/3570 Training loss: 3.2101 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 43/3570 Training loss: 3.2061 0.1591 sec/batch\n",
      "Epoch 1/10  Iteration 44/3570 Training loss: 3.2019 0.1450 sec/batch\n",
      "Epoch 1/10  Iteration 45/3570 Training loss: 3.1976 0.1420 sec/batch\n",
      "Epoch 1/10  Iteration 46/3570 Training loss: 3.1930 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 47/3570 Training loss: 3.1882 0.1520 sec/batch\n",
      "Epoch 1/10  Iteration 48/3570 Training loss: 3.1835 0.1375 sec/batch\n",
      "Epoch 1/10  Iteration 49/3570 Training loss: 3.1779 0.1500 sec/batch\n",
      "Epoch 1/10  Iteration 50/3570 Training loss: 3.1723 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 51/3570 Training loss: 3.1663 0.1440 sec/batch\n",
      "Epoch 1/10  Iteration 52/3570 Training loss: 3.1607 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 53/3570 Training loss: 3.1555 0.1490 sec/batch\n",
      "Epoch 1/10  Iteration 54/3570 Training loss: 3.1505 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 55/3570 Training loss: 3.1444 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 56/3570 Training loss: 3.1387 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 57/3570 Training loss: 3.1333 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 58/3570 Training loss: 3.1270 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 59/3570 Training loss: 3.1204 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 60/3570 Training loss: 3.1136 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 61/3570 Training loss: 3.1076 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 62/3570 Training loss: 3.1019 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 63/3570 Training loss: 3.0963 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 64/3570 Training loss: 3.0900 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 65/3570 Training loss: 3.0840 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 66/3570 Training loss: 3.0778 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 67/3570 Training loss: 3.0716 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 68/3570 Training loss: 3.0663 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 69/3570 Training loss: 3.0601 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 70/3570 Training loss: 3.0543 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 71/3570 Training loss: 3.0482 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 72/3570 Training loss: 3.0427 0.1494 sec/batch\n",
      "Epoch 1/10  Iteration 73/3570 Training loss: 3.0369 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 74/3570 Training loss: 3.0309 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 75/3570 Training loss: 3.0258 0.1420 sec/batch\n",
      "Epoch 1/10  Iteration 76/3570 Training loss: 3.0203 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 77/3570 Training loss: 3.0151 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 78/3570 Training loss: 3.0098 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 79/3570 Training loss: 3.0047 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 80/3570 Training loss: 2.9989 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 81/3570 Training loss: 2.9935 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 82/3570 Training loss: 2.9881 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 83/3570 Training loss: 2.9829 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 84/3570 Training loss: 2.9775 0.1371 sec/batch\n",
      "Epoch 1/10  Iteration 85/3570 Training loss: 2.9722 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 86/3570 Training loss: 2.9665 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 87/3570 Training loss: 2.9618 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 88/3570 Training loss: 2.9565 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 89/3570 Training loss: 2.9511 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 90/3570 Training loss: 2.9463 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 91/3570 Training loss: 2.9411 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 92/3570 Training loss: 2.9359 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 93/3570 Training loss: 2.9311 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 94/3570 Training loss: 2.9262 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 95/3570 Training loss: 2.9214 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 96/3570 Training loss: 2.9167 0.1381 sec/batch\n",
      "Epoch 1/10  Iteration 97/3570 Training loss: 2.9118 0.1450 sec/batch\n",
      "Epoch 1/10  Iteration 98/3570 Training loss: 2.9074 0.1515 sec/batch\n",
      "Epoch 1/10  Iteration 99/3570 Training loss: 2.9026 0.1450 sec/batch\n",
      "Epoch 1/10  Iteration 100/3570 Training loss: 2.8979 0.1338 sec/batch\n",
      "Epoch 1/10  Iteration 101/3570 Training loss: 2.8931 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 102/3570 Training loss: 2.8885 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 103/3570 Training loss: 2.8842 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 104/3570 Training loss: 2.8795 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 105/3570 Training loss: 2.8750 0.1445 sec/batch\n",
      "Epoch 1/10  Iteration 106/3570 Training loss: 2.8705 0.1440 sec/batch\n",
      "Epoch 1/10  Iteration 107/3570 Training loss: 2.8661 0.1570 sec/batch\n",
      "Epoch 1/10  Iteration 108/3570 Training loss: 2.8617 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 109/3570 Training loss: 2.8576 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 110/3570 Training loss: 2.8533 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 111/3570 Training loss: 2.8491 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 112/3570 Training loss: 2.8448 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 113/3570 Training loss: 2.8407 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 114/3570 Training loss: 2.8367 0.1342 sec/batch\n",
      "Epoch 1/10  Iteration 115/3570 Training loss: 2.8332 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 116/3570 Training loss: 2.8296 0.1440 sec/batch\n",
      "Epoch 1/10  Iteration 117/3570 Training loss: 2.8255 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 118/3570 Training loss: 2.8217 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 119/3570 Training loss: 2.8179 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 120/3570 Training loss: 2.8140 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 121/3570 Training loss: 2.8102 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 122/3570 Training loss: 2.8061 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 123/3570 Training loss: 2.8024 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 124/3570 Training loss: 2.7988 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 125/3570 Training loss: 2.7949 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 126/3570 Training loss: 2.7912 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 127/3570 Training loss: 2.7875 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 128/3570 Training loss: 2.7840 0.1493 sec/batch\n",
      "Epoch 1/10  Iteration 129/3570 Training loss: 2.7804 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 130/3570 Training loss: 2.7767 0.1460 sec/batch\n",
      "Epoch 1/10  Iteration 131/3570 Training loss: 2.7730 0.1550 sec/batch\n",
      "Epoch 1/10  Iteration 132/3570 Training loss: 2.7692 0.1480 sec/batch\n",
      "Epoch 1/10  Iteration 133/3570 Training loss: 2.7656 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 134/3570 Training loss: 2.7619 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 135/3570 Training loss: 2.7582 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 136/3570 Training loss: 2.7546 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 137/3570 Training loss: 2.7509 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 138/3570 Training loss: 2.7475 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 139/3570 Training loss: 2.7443 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 140/3570 Training loss: 2.7408 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 141/3570 Training loss: 2.7374 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 142/3570 Training loss: 2.7342 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 143/3570 Training loss: 2.7308 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 144/3570 Training loss: 2.7275 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 145/3570 Training loss: 2.7245 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 146/3570 Training loss: 2.7212 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 147/3570 Training loss: 2.7180 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 148/3570 Training loss: 2.7147 0.1464 sec/batch\n",
      "Epoch 1/10  Iteration 149/3570 Training loss: 2.7115 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 150/3570 Training loss: 2.7084 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 151/3570 Training loss: 2.7052 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 152/3570 Training loss: 2.7021 0.1332 sec/batch\n",
      "Epoch 1/10  Iteration 153/3570 Training loss: 2.6990 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 154/3570 Training loss: 2.6961 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 155/3570 Training loss: 2.6934 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 156/3570 Training loss: 2.6904 0.1410 sec/batch\n",
      "Epoch 1/10  Iteration 157/3570 Training loss: 2.6874 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 158/3570 Training loss: 2.6845 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 159/3570 Training loss: 2.6818 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 160/3570 Training loss: 2.6790 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 161/3570 Training loss: 2.6760 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 162/3570 Training loss: 2.6731 0.1371 sec/batch\n",
      "Epoch 1/10  Iteration 163/3570 Training loss: 2.6705 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 164/3570 Training loss: 2.6675 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 165/3570 Training loss: 2.6648 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 166/3570 Training loss: 2.6620 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 167/3570 Training loss: 2.6590 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 168/3570 Training loss: 2.6564 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 169/3570 Training loss: 2.6538 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 170/3570 Training loss: 2.6510 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 171/3570 Training loss: 2.6484 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 172/3570 Training loss: 2.6458 0.1455 sec/batch\n",
      "Epoch 1/10  Iteration 173/3570 Training loss: 2.6435 0.1600 sec/batch\n",
      "Epoch 1/10  Iteration 174/3570 Training loss: 2.6411 0.1354 sec/batch\n",
      "Epoch 1/10  Iteration 175/3570 Training loss: 2.6387 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 176/3570 Training loss: 2.6358 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 177/3570 Training loss: 2.6332 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 178/3570 Training loss: 2.6306 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 179/3570 Training loss: 2.6280 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 180/3570 Training loss: 2.6255 0.1351 sec/batch\n",
      "Epoch 1/10  Iteration 181/3570 Training loss: 2.6231 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 182/3570 Training loss: 2.6208 0.1381 sec/batch\n",
      "Epoch 1/10  Iteration 183/3570 Training loss: 2.6184 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 184/3570 Training loss: 2.6158 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 185/3570 Training loss: 2.6134 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 186/3570 Training loss: 2.6110 0.1371 sec/batch\n",
      "Epoch 1/10  Iteration 187/3570 Training loss: 2.6085 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 188/3570 Training loss: 2.6059 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 189/3570 Training loss: 2.6034 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 190/3570 Training loss: 2.6009 0.1371 sec/batch\n",
      "Epoch 1/10  Iteration 191/3570 Training loss: 2.5985 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 192/3570 Training loss: 2.5963 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 193/3570 Training loss: 2.5941 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 194/3570 Training loss: 2.5918 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 195/3570 Training loss: 2.5897 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 196/3570 Training loss: 2.5875 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 197/3570 Training loss: 2.5855 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 198/3570 Training loss: 2.5834 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 199/3570 Training loss: 2.5813 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 200/3570 Training loss: 2.5793 0.1407 sec/batch\n",
      "Validation loss: 2.00223 Saving checkpoint!\n",
      "Epoch 1/10  Iteration 201/3570 Training loss: 2.5775 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 202/3570 Training loss: 2.5754 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 203/3570 Training loss: 2.5731 0.1830 sec/batch\n",
      "Epoch 1/10  Iteration 204/3570 Training loss: 2.5710 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 205/3570 Training loss: 2.5688 0.1494 sec/batch\n",
      "Epoch 1/10  Iteration 206/3570 Training loss: 2.5667 0.1510 sec/batch\n",
      "Epoch 1/10  Iteration 207/3570 Training loss: 2.5645 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 208/3570 Training loss: 2.5624 0.1504 sec/batch\n",
      "Epoch 1/10  Iteration 209/3570 Training loss: 2.5602 0.1425 sec/batch\n",
      "Epoch 1/10  Iteration 210/3570 Training loss: 2.5580 0.1374 sec/batch\n",
      "Epoch 1/10  Iteration 211/3570 Training loss: 2.5560 0.1847 sec/batch\n",
      "Epoch 1/10  Iteration 212/3570 Training loss: 2.5540 0.1480 sec/batch\n",
      "Epoch 1/10  Iteration 213/3570 Training loss: 2.5519 0.1785 sec/batch\n",
      "Epoch 1/10  Iteration 214/3570 Training loss: 2.5499 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 215/3570 Training loss: 2.5480 0.1470 sec/batch\n",
      "Epoch 1/10  Iteration 216/3570 Training loss: 2.5459 0.1432 sec/batch\n",
      "Epoch 1/10  Iteration 217/3570 Training loss: 2.5439 0.1450 sec/batch\n",
      "Epoch 1/10  Iteration 218/3570 Training loss: 2.5419 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 219/3570 Training loss: 2.5397 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 220/3570 Training loss: 2.5377 0.1440 sec/batch\n",
      "Epoch 1/10  Iteration 221/3570 Training loss: 2.5358 0.1718 sec/batch\n",
      "Epoch 1/10  Iteration 222/3570 Training loss: 2.5338 0.1486 sec/batch\n",
      "Epoch 1/10  Iteration 223/3570 Training loss: 2.5319 0.1394 sec/batch\n",
      "Epoch 1/10  Iteration 224/3570 Training loss: 2.5299 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 225/3570 Training loss: 2.5279 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 226/3570 Training loss: 2.5261 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 227/3570 Training loss: 2.5241 0.1509 sec/batch\n",
      "Epoch 1/10  Iteration 228/3570 Training loss: 2.5222 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 229/3570 Training loss: 2.5201 0.1633 sec/batch\n",
      "Epoch 1/10  Iteration 230/3570 Training loss: 2.5182 0.1379 sec/batch\n",
      "Epoch 1/10  Iteration 231/3570 Training loss: 2.5163 0.1499 sec/batch\n",
      "Epoch 1/10  Iteration 232/3570 Training loss: 2.5143 0.1445 sec/batch\n",
      "Epoch 1/10  Iteration 233/3570 Training loss: 2.5124 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 234/3570 Training loss: 2.5106 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 235/3570 Training loss: 2.5089 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 236/3570 Training loss: 2.5071 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 237/3570 Training loss: 2.5053 0.1527 sec/batch\n",
      "Epoch 1/10  Iteration 238/3570 Training loss: 2.5034 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 239/3570 Training loss: 2.5017 0.2253 sec/batch\n",
      "Epoch 1/10  Iteration 240/3570 Training loss: 2.4997 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 241/3570 Training loss: 2.4979 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 242/3570 Training loss: 2.4962 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 243/3570 Training loss: 2.4947 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 244/3570 Training loss: 2.4931 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 245/3570 Training loss: 2.4916 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 246/3570 Training loss: 2.4900 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 247/3570 Training loss: 2.4883 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 248/3570 Training loss: 2.4868 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 249/3570 Training loss: 2.4853 0.2820 sec/batch\n",
      "Epoch 1/10  Iteration 250/3570 Training loss: 2.4837 0.1410 sec/batch\n",
      "Epoch 1/10  Iteration 251/3570 Training loss: 2.4820 0.3970 sec/batch\n",
      "Epoch 1/10  Iteration 252/3570 Training loss: 2.4808 0.1690 sec/batch\n",
      "Epoch 1/10  Iteration 253/3570 Training loss: 2.4791 0.2065 sec/batch\n",
      "Epoch 1/10  Iteration 254/3570 Training loss: 2.4775 0.1650 sec/batch\n",
      "Epoch 1/10  Iteration 255/3570 Training loss: 2.4761 0.4840 sec/batch\n",
      "Epoch 1/10  Iteration 256/3570 Training loss: 2.4745 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 257/3570 Training loss: 2.4730 0.1460 sec/batch\n",
      "Epoch 1/10  Iteration 258/3570 Training loss: 2.4716 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 259/3570 Training loss: 2.4700 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 260/3570 Training loss: 2.4685 0.1420 sec/batch\n",
      "Epoch 1/10  Iteration 261/3570 Training loss: 2.4669 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 262/3570 Training loss: 2.4653 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 263/3570 Training loss: 2.4638 0.1580 sec/batch\n",
      "Epoch 1/10  Iteration 264/3570 Training loss: 2.4622 0.1420 sec/batch\n",
      "Epoch 1/10  Iteration 265/3570 Training loss: 2.4607 0.1948 sec/batch\n",
      "Epoch 1/10  Iteration 266/3570 Training loss: 2.4591 0.1452 sec/batch\n",
      "Epoch 1/10  Iteration 267/3570 Training loss: 2.4575 0.1523 sec/batch\n",
      "Epoch 1/10  Iteration 268/3570 Training loss: 2.4559 0.1816 sec/batch\n",
      "Epoch 1/10  Iteration 269/3570 Training loss: 2.4546 0.4011 sec/batch\n",
      "Epoch 1/10  Iteration 270/3570 Training loss: 2.4532 0.1446 sec/batch\n",
      "Epoch 1/10  Iteration 271/3570 Training loss: 2.4518 0.1383 sec/batch\n",
      "Epoch 1/10  Iteration 272/3570 Training loss: 2.4503 0.1379 sec/batch\n",
      "Epoch 1/10  Iteration 273/3570 Training loss: 2.4490 0.2610 sec/batch\n",
      "Epoch 1/10  Iteration 274/3570 Training loss: 2.4475 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 275/3570 Training loss: 2.4462 0.1533 sec/batch\n",
      "Epoch 1/10  Iteration 276/3570 Training loss: 2.4447 0.1520 sec/batch\n",
      "Epoch 1/10  Iteration 277/3570 Training loss: 2.4431 0.1580 sec/batch\n",
      "Epoch 1/10  Iteration 278/3570 Training loss: 2.4417 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 279/3570 Training loss: 2.4400 0.1447 sec/batch\n",
      "Epoch 1/10  Iteration 280/3570 Training loss: 2.4387 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 281/3570 Training loss: 2.4371 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 282/3570 Training loss: 2.4357 0.1510 sec/batch\n",
      "Epoch 1/10  Iteration 283/3570 Training loss: 2.4343 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 284/3570 Training loss: 2.4328 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 285/3570 Training loss: 2.4312 0.1355 sec/batch\n",
      "Epoch 1/10  Iteration 286/3570 Training loss: 2.4297 0.1540 sec/batch\n",
      "Epoch 1/10  Iteration 287/3570 Training loss: 2.4283 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 288/3570 Training loss: 2.4269 0.1364 sec/batch\n",
      "Epoch 1/10  Iteration 289/3570 Training loss: 2.4256 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 290/3570 Training loss: 2.4241 0.1400 sec/batch\n",
      "Epoch 1/10  Iteration 291/3570 Training loss: 2.4224 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 292/3570 Training loss: 2.4209 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 293/3570 Training loss: 2.4195 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 294/3570 Training loss: 2.4179 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 295/3570 Training loss: 2.4166 0.1384 sec/batch\n",
      "Epoch 1/10  Iteration 296/3570 Training loss: 2.4152 0.1410 sec/batch\n",
      "Epoch 1/10  Iteration 297/3570 Training loss: 2.4139 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 298/3570 Training loss: 2.4125 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 299/3570 Training loss: 2.4112 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 300/3570 Training loss: 2.4098 0.1440 sec/batch\n",
      "Epoch 1/10  Iteration 301/3570 Training loss: 2.4084 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 302/3570 Training loss: 2.4070 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 303/3570 Training loss: 2.4055 0.1540 sec/batch\n",
      "Epoch 1/10  Iteration 304/3570 Training loss: 2.4041 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 305/3570 Training loss: 2.4027 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 306/3570 Training loss: 2.4013 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 307/3570 Training loss: 2.4000 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 308/3570 Training loss: 2.3986 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 309/3570 Training loss: 2.3974 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 310/3570 Training loss: 2.3962 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 311/3570 Training loss: 2.3948 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 312/3570 Training loss: 2.3935 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 313/3570 Training loss: 2.3922 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 314/3570 Training loss: 2.3910 0.1320 sec/batch\n",
      "Epoch 1/10  Iteration 315/3570 Training loss: 2.3897 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 316/3570 Training loss: 2.3885 0.1398 sec/batch\n",
      "Epoch 1/10  Iteration 317/3570 Training loss: 2.3873 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 318/3570 Training loss: 2.3861 0.1349 sec/batch\n",
      "Epoch 1/10  Iteration 319/3570 Training loss: 2.3848 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 320/3570 Training loss: 2.3835 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 321/3570 Training loss: 2.3822 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 322/3570 Training loss: 2.3810 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 323/3570 Training loss: 2.3798 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 324/3570 Training loss: 2.3786 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 325/3570 Training loss: 2.3773 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 326/3570 Training loss: 2.3760 0.1780 sec/batch\n",
      "Epoch 1/10  Iteration 327/3570 Training loss: 2.3747 0.1620 sec/batch\n",
      "Epoch 1/10  Iteration 328/3570 Training loss: 2.3735 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 329/3570 Training loss: 2.3723 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 330/3570 Training loss: 2.3711 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 331/3570 Training loss: 2.3699 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 332/3570 Training loss: 2.3687 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 333/3570 Training loss: 2.3676 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 334/3570 Training loss: 2.3664 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 335/3570 Training loss: 2.3651 0.1360 sec/batch\n",
      "Epoch 1/10  Iteration 336/3570 Training loss: 2.3638 0.1430 sec/batch\n",
      "Epoch 1/10  Iteration 337/3570 Training loss: 2.3626 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 338/3570 Training loss: 2.3612 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 339/3570 Training loss: 2.3599 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 340/3570 Training loss: 2.3589 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 341/3570 Training loss: 2.3578 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 342/3570 Training loss: 2.3567 0.1410 sec/batch\n",
      "Epoch 1/10  Iteration 343/3570 Training loss: 2.3556 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 344/3570 Training loss: 2.3545 0.1330 sec/batch\n",
      "Epoch 1/10  Iteration 345/3570 Training loss: 2.3533 0.1380 sec/batch\n",
      "Epoch 1/10  Iteration 346/3570 Training loss: 2.3522 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 347/3570 Training loss: 2.3512 0.1350 sec/batch\n",
      "Epoch 1/10  Iteration 348/3570 Training loss: 2.3501 0.1390 sec/batch\n",
      "Epoch 1/10  Iteration 349/3570 Training loss: 2.3490 0.1332 sec/batch\n",
      "Epoch 1/10  Iteration 350/3570 Training loss: 2.3479 0.1418 sec/batch\n",
      "Epoch 1/10  Iteration 351/3570 Training loss: 2.3467 0.1368 sec/batch\n",
      "Epoch 1/10  Iteration 352/3570 Training loss: 2.3457 0.1394 sec/batch\n",
      "Epoch 1/10  Iteration 353/3570 Training loss: 2.3447 0.1370 sec/batch\n",
      "Epoch 1/10  Iteration 354/3570 Training loss: 2.3436 0.1340 sec/batch\n",
      "Epoch 1/10  Iteration 355/3570 Training loss: 2.3425 0.1361 sec/batch\n",
      "Epoch 1/10  Iteration 356/3570 Training loss: 2.3414 0.1373 sec/batch\n",
      "Epoch 1/10  Iteration 357/3570 Training loss: 2.3403 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 358/3570 Training loss: 2.0043 0.1334 sec/batch\n",
      "Epoch 2/10  Iteration 359/3570 Training loss: 1.9571 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 360/3570 Training loss: 1.9585 0.1347 sec/batch\n",
      "Epoch 2/10  Iteration 361/3570 Training loss: 1.9455 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 362/3570 Training loss: 1.9353 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 363/3570 Training loss: 1.9310 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 364/3570 Training loss: 1.9315 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 365/3570 Training loss: 1.9332 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 366/3570 Training loss: 1.9368 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 367/3570 Training loss: 1.9367 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 368/3570 Training loss: 1.9397 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 369/3570 Training loss: 1.9389 0.1348 sec/batch\n",
      "Epoch 2/10  Iteration 370/3570 Training loss: 1.9376 0.1334 sec/batch\n",
      "Epoch 2/10  Iteration 371/3570 Training loss: 1.9355 0.1347 sec/batch\n",
      "Epoch 2/10  Iteration 372/3570 Training loss: 1.9345 0.1392 sec/batch\n",
      "Epoch 2/10  Iteration 373/3570 Training loss: 1.9328 0.1346 sec/batch\n",
      "Epoch 2/10  Iteration 374/3570 Training loss: 1.9326 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 375/3570 Training loss: 1.9322 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 376/3570 Training loss: 1.9311 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 377/3570 Training loss: 1.9318 0.1420 sec/batch\n",
      "Epoch 2/10  Iteration 378/3570 Training loss: 1.9324 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 379/3570 Training loss: 1.9326 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 380/3570 Training loss: 1.9314 0.1420 sec/batch\n",
      "Epoch 2/10  Iteration 381/3570 Training loss: 1.9311 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 382/3570 Training loss: 1.9315 0.1630 sec/batch\n",
      "Epoch 2/10  Iteration 383/3570 Training loss: 1.9329 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 384/3570 Training loss: 1.9324 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 385/3570 Training loss: 1.9326 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 386/3570 Training loss: 1.9321 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 387/3570 Training loss: 1.9327 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 388/3570 Training loss: 1.9325 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 389/3570 Training loss: 1.9321 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 390/3570 Training loss: 1.9314 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 391/3570 Training loss: 1.9304 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 392/3570 Training loss: 1.9291 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 393/3570 Training loss: 1.9288 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 394/3570 Training loss: 1.9293 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 395/3570 Training loss: 1.9296 0.1530 sec/batch\n",
      "Epoch 2/10  Iteration 396/3570 Training loss: 1.9298 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 397/3570 Training loss: 1.9288 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 398/3570 Training loss: 1.9282 0.1410 sec/batch\n",
      "Epoch 2/10  Iteration 399/3570 Training loss: 1.9270 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 400/3570 Training loss: 1.9262 0.1337 sec/batch\n",
      "Validation loss: 1.74567 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 401/3570 Training loss: 1.9271 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 402/3570 Training loss: 1.9261 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 403/3570 Training loss: 1.9258 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 404/3570 Training loss: 1.9253 0.1420 sec/batch\n",
      "Epoch 2/10  Iteration 405/3570 Training loss: 1.9247 0.1373 sec/batch\n",
      "Epoch 2/10  Iteration 406/3570 Training loss: 1.9248 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 407/3570 Training loss: 1.9241 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 408/3570 Training loss: 1.9234 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 409/3570 Training loss: 1.9224 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 410/3570 Training loss: 1.9224 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 411/3570 Training loss: 1.9223 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 412/3570 Training loss: 1.9216 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 413/3570 Training loss: 1.9213 0.1375 sec/batch\n",
      "Epoch 2/10  Iteration 414/3570 Training loss: 1.9213 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 415/3570 Training loss: 1.9207 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 416/3570 Training loss: 1.9201 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 417/3570 Training loss: 1.9195 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 418/3570 Training loss: 1.9193 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 419/3570 Training loss: 1.9197 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 420/3570 Training loss: 1.9198 0.1460 sec/batch\n",
      "Epoch 2/10  Iteration 421/3570 Training loss: 1.9184 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 422/3570 Training loss: 1.9189 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 423/3570 Training loss: 1.9186 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 424/3570 Training loss: 1.9179 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 425/3570 Training loss: 1.9180 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 426/3570 Training loss: 1.9182 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 427/3570 Training loss: 1.9180 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 428/3570 Training loss: 1.9184 0.1373 sec/batch\n",
      "Epoch 2/10  Iteration 429/3570 Training loss: 1.9183 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 430/3570 Training loss: 1.9176 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 431/3570 Training loss: 1.9172 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 432/3570 Training loss: 1.9170 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 433/3570 Training loss: 1.9169 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 434/3570 Training loss: 1.9172 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 435/3570 Training loss: 1.9167 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 436/3570 Training loss: 1.9164 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 437/3570 Training loss: 1.9157 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 438/3570 Training loss: 1.9149 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 439/3570 Training loss: 1.9143 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 440/3570 Training loss: 1.9143 0.1420 sec/batch\n",
      "Epoch 2/10  Iteration 441/3570 Training loss: 1.9141 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 442/3570 Training loss: 1.9135 0.1324 sec/batch\n",
      "Epoch 2/10  Iteration 443/3570 Training loss: 1.9127 0.1422 sec/batch\n",
      "Epoch 2/10  Iteration 444/3570 Training loss: 1.9131 0.1388 sec/batch\n",
      "Epoch 2/10  Iteration 445/3570 Training loss: 1.9126 0.1376 sec/batch\n",
      "Epoch 2/10  Iteration 446/3570 Training loss: 1.9122 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 447/3570 Training loss: 1.9121 0.1336 sec/batch\n",
      "Epoch 2/10  Iteration 448/3570 Training loss: 1.9116 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 449/3570 Training loss: 1.9107 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 450/3570 Training loss: 1.9105 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 451/3570 Training loss: 1.9101 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 452/3570 Training loss: 1.9098 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 453/3570 Training loss: 1.9097 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 454/3570 Training loss: 1.9094 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 455/3570 Training loss: 1.9091 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 456/3570 Training loss: 1.9091 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 457/3570 Training loss: 1.9091 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 458/3570 Training loss: 1.9089 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 459/3570 Training loss: 1.9083 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 460/3570 Training loss: 1.9087 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 461/3570 Training loss: 1.9078 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 462/3570 Training loss: 1.9073 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 463/3570 Training loss: 1.9068 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 464/3570 Training loss: 1.9063 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 465/3570 Training loss: 1.9058 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 466/3570 Training loss: 1.9052 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 467/3570 Training loss: 1.9048 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 468/3570 Training loss: 1.9044 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 469/3570 Training loss: 1.9041 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 470/3570 Training loss: 1.9038 0.1324 sec/batch\n",
      "Epoch 2/10  Iteration 471/3570 Training loss: 1.9036 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 472/3570 Training loss: 1.9038 0.1348 sec/batch\n",
      "Epoch 2/10  Iteration 473/3570 Training loss: 1.9035 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 474/3570 Training loss: 1.9031 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 475/3570 Training loss: 1.9030 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 476/3570 Training loss: 1.9029 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 477/3570 Training loss: 1.9026 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 478/3570 Training loss: 1.9022 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 479/3570 Training loss: 1.9017 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 480/3570 Training loss: 1.9016 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 481/3570 Training loss: 1.9012 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 482/3570 Training loss: 1.9008 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 483/3570 Training loss: 1.9005 0.1660 sec/batch\n",
      "Epoch 2/10  Iteration 484/3570 Training loss: 1.9001 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 485/3570 Training loss: 1.8999 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 486/3570 Training loss: 1.8996 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 487/3570 Training loss: 1.8990 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 488/3570 Training loss: 1.8985 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 489/3570 Training loss: 1.8980 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 490/3570 Training loss: 1.8977 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 491/3570 Training loss: 1.8972 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 492/3570 Training loss: 1.8967 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 493/3570 Training loss: 1.8963 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 494/3570 Training loss: 1.8959 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 495/3570 Training loss: 1.8955 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 496/3570 Training loss: 1.8953 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 497/3570 Training loss: 1.8949 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 498/3570 Training loss: 1.8947 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 499/3570 Training loss: 1.8944 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 500/3570 Training loss: 1.8940 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 501/3570 Training loss: 1.8935 0.1430 sec/batch\n",
      "Epoch 2/10  Iteration 502/3570 Training loss: 1.8933 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 503/3570 Training loss: 1.8929 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 504/3570 Training loss: 1.8925 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 505/3570 Training loss: 1.8919 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 506/3570 Training loss: 1.8914 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 507/3570 Training loss: 1.8909 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 508/3570 Training loss: 1.8904 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 509/3570 Training loss: 1.8900 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 510/3570 Training loss: 1.8896 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 511/3570 Training loss: 1.8892 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 512/3570 Training loss: 1.8891 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 513/3570 Training loss: 1.8889 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 514/3570 Training loss: 1.8885 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 515/3570 Training loss: 1.8882 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 516/3570 Training loss: 1.8882 0.1351 sec/batch\n",
      "Epoch 2/10  Iteration 517/3570 Training loss: 1.8879 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 518/3570 Training loss: 1.8874 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 519/3570 Training loss: 1.8872 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 520/3570 Training loss: 1.8871 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 521/3570 Training loss: 1.8867 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 522/3570 Training loss: 1.8865 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 523/3570 Training loss: 1.8862 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 524/3570 Training loss: 1.8857 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 525/3570 Training loss: 1.8855 0.1331 sec/batch\n",
      "Epoch 2/10  Iteration 526/3570 Training loss: 1.8850 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 527/3570 Training loss: 1.8846 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 528/3570 Training loss: 1.8843 0.1338 sec/batch\n",
      "Epoch 2/10  Iteration 529/3570 Training loss: 1.8839 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 530/3570 Training loss: 1.8837 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 531/3570 Training loss: 1.8834 0.1331 sec/batch\n",
      "Epoch 2/10  Iteration 532/3570 Training loss: 1.8832 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 533/3570 Training loss: 1.8823 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 534/3570 Training loss: 1.8818 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 535/3570 Training loss: 1.8814 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 536/3570 Training loss: 1.8810 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 537/3570 Training loss: 1.8806 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 538/3570 Training loss: 1.8804 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 539/3570 Training loss: 1.8802 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 540/3570 Training loss: 1.8799 0.1410 sec/batch\n",
      "Epoch 2/10  Iteration 541/3570 Training loss: 1.8795 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 542/3570 Training loss: 1.8792 0.1394 sec/batch\n",
      "Epoch 2/10  Iteration 543/3570 Training loss: 1.8788 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 544/3570 Training loss: 1.8783 0.1417 sec/batch\n",
      "Epoch 2/10  Iteration 545/3570 Training loss: 1.8777 0.1406 sec/batch\n",
      "Epoch 2/10  Iteration 546/3570 Training loss: 1.8774 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 547/3570 Training loss: 1.8770 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 548/3570 Training loss: 1.8766 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 549/3570 Training loss: 1.8765 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 550/3570 Training loss: 1.8764 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 551/3570 Training loss: 1.8760 0.1436 sec/batch\n",
      "Epoch 2/10  Iteration 552/3570 Training loss: 1.8760 0.1385 sec/batch\n",
      "Epoch 2/10  Iteration 553/3570 Training loss: 1.8757 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 554/3570 Training loss: 1.8755 0.1374 sec/batch\n",
      "Epoch 2/10  Iteration 555/3570 Training loss: 1.8754 0.1477 sec/batch\n",
      "Epoch 2/10  Iteration 556/3570 Training loss: 1.8751 0.1382 sec/batch\n",
      "Epoch 2/10  Iteration 557/3570 Training loss: 1.8750 0.1410 sec/batch\n",
      "Epoch 2/10  Iteration 558/3570 Training loss: 1.8749 0.1426 sec/batch\n",
      "Epoch 2/10  Iteration 559/3570 Training loss: 1.8747 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 560/3570 Training loss: 1.8741 0.1430 sec/batch\n",
      "Epoch 2/10  Iteration 561/3570 Training loss: 1.8740 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 562/3570 Training loss: 1.8735 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 563/3570 Training loss: 1.8734 0.1371 sec/batch\n",
      "Epoch 2/10  Iteration 564/3570 Training loss: 1.8730 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 565/3570 Training loss: 1.8725 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 566/3570 Training loss: 1.8721 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 567/3570 Training loss: 1.8716 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 568/3570 Training loss: 1.8714 0.1369 sec/batch\n",
      "Epoch 2/10  Iteration 569/3570 Training loss: 1.8712 0.1345 sec/batch\n",
      "Epoch 2/10  Iteration 570/3570 Training loss: 1.8709 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 571/3570 Training loss: 1.8706 0.1450 sec/batch\n",
      "Epoch 2/10  Iteration 572/3570 Training loss: 1.8703 0.1384 sec/batch\n",
      "Epoch 2/10  Iteration 573/3570 Training loss: 1.8699 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 574/3570 Training loss: 1.8697 0.1357 sec/batch\n",
      "Epoch 2/10  Iteration 575/3570 Training loss: 1.8695 0.1394 sec/batch\n",
      "Epoch 2/10  Iteration 576/3570 Training loss: 1.8690 0.1430 sec/batch\n",
      "Epoch 2/10  Iteration 577/3570 Training loss: 1.8687 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 578/3570 Training loss: 1.8685 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 579/3570 Training loss: 1.8682 0.1362 sec/batch\n",
      "Epoch 2/10  Iteration 580/3570 Training loss: 1.8680 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 581/3570 Training loss: 1.8679 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 582/3570 Training loss: 1.8676 0.1470 sec/batch\n",
      "Epoch 2/10  Iteration 583/3570 Training loss: 1.8675 0.1403 sec/batch\n",
      "Epoch 2/10  Iteration 584/3570 Training loss: 1.8672 0.1430 sec/batch\n",
      "Epoch 2/10  Iteration 585/3570 Training loss: 1.8669 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 586/3570 Training loss: 1.8666 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 587/3570 Training loss: 1.8663 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 588/3570 Training loss: 1.8660 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 589/3570 Training loss: 1.8657 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 590/3570 Training loss: 1.8653 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 591/3570 Training loss: 1.8651 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 592/3570 Training loss: 1.8650 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 593/3570 Training loss: 1.8648 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 594/3570 Training loss: 1.8647 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 595/3570 Training loss: 1.8643 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 596/3570 Training loss: 1.8641 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 597/3570 Training loss: 1.8636 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 598/3570 Training loss: 1.8635 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 599/3570 Training loss: 1.8634 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 600/3570 Training loss: 1.8631 0.1370 sec/batch\n",
      "Validation loss: 1.6276 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 601/3570 Training loss: 1.8634 0.1420 sec/batch\n",
      "Epoch 2/10  Iteration 602/3570 Training loss: 1.8633 0.1320 sec/batch\n",
      "Epoch 2/10  Iteration 603/3570 Training loss: 1.8631 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 604/3570 Training loss: 1.8629 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 605/3570 Training loss: 1.8627 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 606/3570 Training loss: 1.8626 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 607/3570 Training loss: 1.8624 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 608/3570 Training loss: 1.8622 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 609/3570 Training loss: 1.8622 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 610/3570 Training loss: 1.8617 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 611/3570 Training loss: 1.8615 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 612/3570 Training loss: 1.8614 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 613/3570 Training loss: 1.8611 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 614/3570 Training loss: 1.8609 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 615/3570 Training loss: 1.8607 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 616/3570 Training loss: 1.8604 0.1408 sec/batch\n",
      "Epoch 2/10  Iteration 617/3570 Training loss: 1.8602 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 618/3570 Training loss: 1.8598 0.1410 sec/batch\n",
      "Epoch 2/10  Iteration 619/3570 Training loss: 1.8595 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 620/3570 Training loss: 1.8593 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 621/3570 Training loss: 1.8592 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 622/3570 Training loss: 1.8590 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 623/3570 Training loss: 1.8588 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 624/3570 Training loss: 1.8587 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 625/3570 Training loss: 1.8583 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 626/3570 Training loss: 1.8583 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 627/3570 Training loss: 1.8582 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 628/3570 Training loss: 1.8580 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 629/3570 Training loss: 1.8578 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 630/3570 Training loss: 1.8577 0.1412 sec/batch\n",
      "Epoch 2/10  Iteration 631/3570 Training loss: 1.8575 0.1405 sec/batch\n",
      "Epoch 2/10  Iteration 632/3570 Training loss: 1.8574 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 633/3570 Training loss: 1.8571 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 634/3570 Training loss: 1.8566 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 635/3570 Training loss: 1.8564 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 636/3570 Training loss: 1.8560 0.1430 sec/batch\n",
      "Epoch 2/10  Iteration 637/3570 Training loss: 1.8559 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 638/3570 Training loss: 1.8555 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 639/3570 Training loss: 1.8552 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 640/3570 Training loss: 1.8551 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 641/3570 Training loss: 1.8548 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 642/3570 Training loss: 1.8544 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 643/3570 Training loss: 1.8541 0.1440 sec/batch\n",
      "Epoch 2/10  Iteration 644/3570 Training loss: 1.8539 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 645/3570 Training loss: 1.8537 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 646/3570 Training loss: 1.8535 0.1351 sec/batch\n",
      "Epoch 2/10  Iteration 647/3570 Training loss: 1.8531 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 648/3570 Training loss: 1.8527 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 649/3570 Training loss: 1.8525 0.1456 sec/batch\n",
      "Epoch 2/10  Iteration 650/3570 Training loss: 1.8523 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 651/3570 Training loss: 1.8520 0.1392 sec/batch\n",
      "Epoch 2/10  Iteration 652/3570 Training loss: 1.8519 0.1413 sec/batch\n",
      "Epoch 2/10  Iteration 653/3570 Training loss: 1.8516 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 654/3570 Training loss: 1.8515 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 655/3570 Training loss: 1.8513 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 656/3570 Training loss: 1.8511 0.1410 sec/batch\n",
      "Epoch 2/10  Iteration 657/3570 Training loss: 1.8507 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 658/3570 Training loss: 1.8505 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 659/3570 Training loss: 1.8503 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 660/3570 Training loss: 1.8499 0.1388 sec/batch\n",
      "Epoch 2/10  Iteration 661/3570 Training loss: 1.8495 0.1450 sec/batch\n",
      "Epoch 2/10  Iteration 662/3570 Training loss: 1.8493 0.1398 sec/batch\n",
      "Epoch 2/10  Iteration 663/3570 Training loss: 1.8490 0.1408 sec/batch\n",
      "Epoch 2/10  Iteration 664/3570 Training loss: 1.8488 0.1487 sec/batch\n",
      "Epoch 2/10  Iteration 665/3570 Training loss: 1.8486 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 666/3570 Training loss: 1.8484 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 667/3570 Training loss: 1.8483 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 668/3570 Training loss: 1.8480 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 669/3570 Training loss: 1.8478 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 670/3570 Training loss: 1.8477 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 671/3570 Training loss: 1.8476 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 672/3570 Training loss: 1.8474 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 673/3570 Training loss: 1.8473 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 674/3570 Training loss: 1.8471 0.1400 sec/batch\n",
      "Epoch 2/10  Iteration 675/3570 Training loss: 1.8470 0.1490 sec/batch\n",
      "Epoch 2/10  Iteration 676/3570 Training loss: 1.8467 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 677/3570 Training loss: 1.8465 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 678/3570 Training loss: 1.8462 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 679/3570 Training loss: 1.8460 0.1379 sec/batch\n",
      "Epoch 2/10  Iteration 680/3570 Training loss: 1.8459 0.1374 sec/batch\n",
      "Epoch 2/10  Iteration 681/3570 Training loss: 1.8457 0.1454 sec/batch\n",
      "Epoch 2/10  Iteration 682/3570 Training loss: 1.8453 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 683/3570 Training loss: 1.8450 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 684/3570 Training loss: 1.8448 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 685/3570 Training loss: 1.8446 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 686/3570 Training loss: 1.8444 0.1365 sec/batch\n",
      "Epoch 2/10  Iteration 687/3570 Training loss: 1.8441 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 688/3570 Training loss: 1.8440 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 689/3570 Training loss: 1.8438 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 690/3570 Training loss: 1.8437 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 691/3570 Training loss: 1.8435 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 692/3570 Training loss: 1.8431 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 693/3570 Training loss: 1.8428 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 694/3570 Training loss: 1.8426 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 695/3570 Training loss: 1.8421 0.1340 sec/batch\n",
      "Epoch 2/10  Iteration 696/3570 Training loss: 1.8419 0.1390 sec/batch\n",
      "Epoch 2/10  Iteration 697/3570 Training loss: 1.8417 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 698/3570 Training loss: 1.8416 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 699/3570 Training loss: 1.8415 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 700/3570 Training loss: 1.8414 0.1380 sec/batch\n",
      "Epoch 2/10  Iteration 701/3570 Training loss: 1.8412 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 702/3570 Training loss: 1.8410 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 703/3570 Training loss: 1.8408 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 704/3570 Training loss: 1.8407 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 705/3570 Training loss: 1.8406 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 706/3570 Training loss: 1.8404 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 707/3570 Training loss: 1.8402 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 708/3570 Training loss: 1.8401 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 709/3570 Training loss: 1.8400 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 710/3570 Training loss: 1.8398 0.1370 sec/batch\n",
      "Epoch 2/10  Iteration 711/3570 Training loss: 1.8397 0.1330 sec/batch\n",
      "Epoch 2/10  Iteration 712/3570 Training loss: 1.8395 0.1360 sec/batch\n",
      "Epoch 2/10  Iteration 713/3570 Training loss: 1.8393 0.1350 sec/batch\n",
      "Epoch 2/10  Iteration 714/3570 Training loss: 1.8391 0.1410 sec/batch\n",
      "Epoch 3/10  Iteration 715/3570 Training loss: 1.8076 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 716/3570 Training loss: 1.7612 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 717/3570 Training loss: 1.7636 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 718/3570 Training loss: 1.7531 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 719/3570 Training loss: 1.7450 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 720/3570 Training loss: 1.7440 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 721/3570 Training loss: 1.7454 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 722/3570 Training loss: 1.7472 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 723/3570 Training loss: 1.7512 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 724/3570 Training loss: 1.7500 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 725/3570 Training loss: 1.7531 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 726/3570 Training loss: 1.7538 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 727/3570 Training loss: 1.7547 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 728/3570 Training loss: 1.7528 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 729/3570 Training loss: 1.7513 0.1320 sec/batch\n",
      "Epoch 3/10  Iteration 730/3570 Training loss: 1.7486 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 731/3570 Training loss: 1.7504 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 732/3570 Training loss: 1.7501 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 733/3570 Training loss: 1.7503 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 734/3570 Training loss: 1.7519 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 735/3570 Training loss: 1.7524 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 736/3570 Training loss: 1.7541 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 737/3570 Training loss: 1.7529 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 738/3570 Training loss: 1.7521 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 739/3570 Training loss: 1.7526 0.1460 sec/batch\n",
      "Epoch 3/10  Iteration 740/3570 Training loss: 1.7529 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 741/3570 Training loss: 1.7526 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 742/3570 Training loss: 1.7538 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 743/3570 Training loss: 1.7529 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 744/3570 Training loss: 1.7539 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 745/3570 Training loss: 1.7544 0.1320 sec/batch\n",
      "Epoch 3/10  Iteration 746/3570 Training loss: 1.7539 0.1369 sec/batch\n",
      "Epoch 3/10  Iteration 747/3570 Training loss: 1.7534 0.1345 sec/batch\n",
      "Epoch 3/10  Iteration 748/3570 Training loss: 1.7530 0.1377 sec/batch\n",
      "Epoch 3/10  Iteration 749/3570 Training loss: 1.7521 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 750/3570 Training loss: 1.7524 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 751/3570 Training loss: 1.7532 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 752/3570 Training loss: 1.7535 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 753/3570 Training loss: 1.7536 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 754/3570 Training loss: 1.7531 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 755/3570 Training loss: 1.7535 0.1430 sec/batch\n",
      "Epoch 3/10  Iteration 756/3570 Training loss: 1.7529 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 757/3570 Training loss: 1.7523 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 758/3570 Training loss: 1.7517 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 759/3570 Training loss: 1.7508 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 760/3570 Training loss: 1.7514 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 761/3570 Training loss: 1.7514 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 762/3570 Training loss: 1.7512 0.1410 sec/batch\n",
      "Epoch 3/10  Iteration 763/3570 Training loss: 1.7520 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 764/3570 Training loss: 1.7520 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 765/3570 Training loss: 1.7516 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 766/3570 Training loss: 1.7510 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 767/3570 Training loss: 1.7513 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 768/3570 Training loss: 1.7516 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 769/3570 Training loss: 1.7514 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 770/3570 Training loss: 1.7521 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 771/3570 Training loss: 1.7525 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 772/3570 Training loss: 1.7520 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 773/3570 Training loss: 1.7514 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 774/3570 Training loss: 1.7514 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 775/3570 Training loss: 1.7515 0.1334 sec/batch\n",
      "Epoch 3/10  Iteration 776/3570 Training loss: 1.7519 0.1358 sec/batch\n",
      "Epoch 3/10  Iteration 777/3570 Training loss: 1.7520 0.1339 sec/batch\n",
      "Epoch 3/10  Iteration 778/3570 Training loss: 1.7509 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 779/3570 Training loss: 1.7513 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 780/3570 Training loss: 1.7516 0.1355 sec/batch\n",
      "Epoch 3/10  Iteration 781/3570 Training loss: 1.7511 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 782/3570 Training loss: 1.7513 0.1411 sec/batch\n",
      "Epoch 3/10  Iteration 783/3570 Training loss: 1.7519 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 784/3570 Training loss: 1.7520 0.1362 sec/batch\n",
      "Epoch 3/10  Iteration 785/3570 Training loss: 1.7529 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 786/3570 Training loss: 1.7529 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 787/3570 Training loss: 1.7528 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 788/3570 Training loss: 1.7525 0.1379 sec/batch\n",
      "Epoch 3/10  Iteration 789/3570 Training loss: 1.7527 0.1353 sec/batch\n",
      "Epoch 3/10  Iteration 790/3570 Training loss: 1.7528 0.1357 sec/batch\n",
      "Epoch 3/10  Iteration 791/3570 Training loss: 1.7535 0.1413 sec/batch\n",
      "Epoch 3/10  Iteration 792/3570 Training loss: 1.7534 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 793/3570 Training loss: 1.7531 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 794/3570 Training loss: 1.7525 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 795/3570 Training loss: 1.7519 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 796/3570 Training loss: 1.7514 0.1343 sec/batch\n",
      "Epoch 3/10  Iteration 797/3570 Training loss: 1.7514 0.1336 sec/batch\n",
      "Epoch 3/10  Iteration 798/3570 Training loss: 1.7511 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 799/3570 Training loss: 1.7507 0.1334 sec/batch\n",
      "Epoch 3/10  Iteration 800/3570 Training loss: 1.7501 0.1382 sec/batch\n",
      "Validation loss: 1.56411 Saving checkpoint!\n",
      "Epoch 3/10  Iteration 801/3570 Training loss: 1.7522 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 802/3570 Training loss: 1.7519 0.1411 sec/batch\n",
      "Epoch 3/10  Iteration 803/3570 Training loss: 1.7520 0.1431 sec/batch\n",
      "Epoch 3/10  Iteration 804/3570 Training loss: 1.7521 0.1385 sec/batch\n",
      "Epoch 3/10  Iteration 805/3570 Training loss: 1.7518 0.1412 sec/batch\n",
      "Epoch 3/10  Iteration 806/3570 Training loss: 1.7512 0.1473 sec/batch\n",
      "Epoch 3/10  Iteration 807/3570 Training loss: 1.7510 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 808/3570 Training loss: 1.7508 0.1566 sec/batch\n",
      "Epoch 3/10  Iteration 809/3570 Training loss: 1.7505 0.2517 sec/batch\n",
      "Epoch 3/10  Iteration 810/3570 Training loss: 1.7505 0.1367 sec/batch\n",
      "Epoch 3/10  Iteration 811/3570 Training loss: 1.7505 0.1643 sec/batch\n",
      "Epoch 3/10  Iteration 812/3570 Training loss: 1.7505 0.1598 sec/batch\n",
      "Epoch 3/10  Iteration 813/3570 Training loss: 1.7506 0.1450 sec/batch\n",
      "Epoch 3/10  Iteration 814/3570 Training loss: 1.7508 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 815/3570 Training loss: 1.7512 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 816/3570 Training loss: 1.7508 0.1372 sec/batch\n",
      "Epoch 3/10  Iteration 817/3570 Training loss: 1.7512 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 818/3570 Training loss: 1.7507 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 819/3570 Training loss: 1.7503 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 820/3570 Training loss: 1.7500 0.1410 sec/batch\n",
      "Epoch 3/10  Iteration 821/3570 Training loss: 1.7495 0.1420 sec/batch\n",
      "Epoch 3/10  Iteration 822/3570 Training loss: 1.7490 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 823/3570 Training loss: 1.7486 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 824/3570 Training loss: 1.7484 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 825/3570 Training loss: 1.7482 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 826/3570 Training loss: 1.7482 0.1411 sec/batch\n",
      "Epoch 3/10  Iteration 827/3570 Training loss: 1.7481 0.1403 sec/batch\n",
      "Epoch 3/10  Iteration 828/3570 Training loss: 1.7481 0.1432 sec/batch\n",
      "Epoch 3/10  Iteration 829/3570 Training loss: 1.7485 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 830/3570 Training loss: 1.7484 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 831/3570 Training loss: 1.7482 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 832/3570 Training loss: 1.7484 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 833/3570 Training loss: 1.7485 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 834/3570 Training loss: 1.7483 0.1331 sec/batch\n",
      "Epoch 3/10  Iteration 835/3570 Training loss: 1.7483 0.1367 sec/batch\n",
      "Epoch 3/10  Iteration 836/3570 Training loss: 1.7478 0.1381 sec/batch\n",
      "Epoch 3/10  Iteration 837/3570 Training loss: 1.7478 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 838/3570 Training loss: 1.7476 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 839/3570 Training loss: 1.7474 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 840/3570 Training loss: 1.7473 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 841/3570 Training loss: 1.7471 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 842/3570 Training loss: 1.7470 0.1378 sec/batch\n",
      "Epoch 3/10  Iteration 843/3570 Training loss: 1.7469 0.1365 sec/batch\n",
      "Epoch 3/10  Iteration 844/3570 Training loss: 1.7466 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 845/3570 Training loss: 1.7465 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 846/3570 Training loss: 1.7463 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 847/3570 Training loss: 1.7462 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 848/3570 Training loss: 1.7461 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 849/3570 Training loss: 1.7459 0.1429 sec/batch\n",
      "Epoch 3/10  Iteration 850/3570 Training loss: 1.7457 0.1428 sec/batch\n",
      "Epoch 3/10  Iteration 851/3570 Training loss: 1.7455 0.1383 sec/batch\n",
      "Epoch 3/10  Iteration 852/3570 Training loss: 1.7453 0.1403 sec/batch\n",
      "Epoch 3/10  Iteration 853/3570 Training loss: 1.7452 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 854/3570 Training loss: 1.7451 0.1400 sec/batch\n",
      "Epoch 3/10  Iteration 855/3570 Training loss: 1.7450 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 856/3570 Training loss: 1.7450 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 857/3570 Training loss: 1.7447 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 858/3570 Training loss: 1.7443 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 859/3570 Training loss: 1.7443 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 860/3570 Training loss: 1.7440 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 861/3570 Training loss: 1.7437 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 862/3570 Training loss: 1.7433 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 863/3570 Training loss: 1.7429 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 864/3570 Training loss: 1.7425 0.1368 sec/batch\n",
      "Epoch 3/10  Iteration 865/3570 Training loss: 1.7423 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 866/3570 Training loss: 1.7421 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 867/3570 Training loss: 1.7418 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 868/3570 Training loss: 1.7417 0.1460 sec/batch\n",
      "Epoch 3/10  Iteration 869/3570 Training loss: 1.7417 0.1880 sec/batch\n",
      "Epoch 3/10  Iteration 870/3570 Training loss: 1.7418 0.1400 sec/batch\n",
      "Epoch 3/10  Iteration 871/3570 Training loss: 1.7415 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 872/3570 Training loss: 1.7413 0.1430 sec/batch\n",
      "Epoch 3/10  Iteration 873/3570 Training loss: 1.7415 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 874/3570 Training loss: 1.7414 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 875/3570 Training loss: 1.7411 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 876/3570 Training loss: 1.7410 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 877/3570 Training loss: 1.7411 0.1351 sec/batch\n",
      "Epoch 3/10  Iteration 878/3570 Training loss: 1.7409 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 879/3570 Training loss: 1.7410 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 880/3570 Training loss: 1.7410 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 881/3570 Training loss: 1.7406 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 882/3570 Training loss: 1.7405 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 883/3570 Training loss: 1.7403 0.1400 sec/batch\n",
      "Epoch 3/10  Iteration 884/3570 Training loss: 1.7402 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 885/3570 Training loss: 1.7400 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 886/3570 Training loss: 1.7397 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 887/3570 Training loss: 1.7397 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 888/3570 Training loss: 1.7396 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 889/3570 Training loss: 1.7395 0.1320 sec/batch\n",
      "Epoch 3/10  Iteration 890/3570 Training loss: 1.7389 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 891/3570 Training loss: 1.7385 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 892/3570 Training loss: 1.7381 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 893/3570 Training loss: 1.7379 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 894/3570 Training loss: 1.7377 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 895/3570 Training loss: 1.7376 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 896/3570 Training loss: 1.7375 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 897/3570 Training loss: 1.7374 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 898/3570 Training loss: 1.7371 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 899/3570 Training loss: 1.7370 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 900/3570 Training loss: 1.7367 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 901/3570 Training loss: 1.7365 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 902/3570 Training loss: 1.7361 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 903/3570 Training loss: 1.7361 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 904/3570 Training loss: 1.7359 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 905/3570 Training loss: 1.7357 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 906/3570 Training loss: 1.7358 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 907/3570 Training loss: 1.7357 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 908/3570 Training loss: 1.7355 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 909/3570 Training loss: 1.7357 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 910/3570 Training loss: 1.7355 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 911/3570 Training loss: 1.7355 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 912/3570 Training loss: 1.7355 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 913/3570 Training loss: 1.7355 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 914/3570 Training loss: 1.7356 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 915/3570 Training loss: 1.7357 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 916/3570 Training loss: 1.7357 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 917/3570 Training loss: 1.7353 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 918/3570 Training loss: 1.7354 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 919/3570 Training loss: 1.7351 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 920/3570 Training loss: 1.7350 0.1410 sec/batch\n",
      "Epoch 3/10  Iteration 921/3570 Training loss: 1.7348 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 922/3570 Training loss: 1.7346 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 923/3570 Training loss: 1.7344 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 924/3570 Training loss: 1.7340 0.1420 sec/batch\n",
      "Epoch 3/10  Iteration 925/3570 Training loss: 1.7338 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 926/3570 Training loss: 1.7338 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 927/3570 Training loss: 1.7335 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 928/3570 Training loss: 1.7333 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 929/3570 Training loss: 1.7332 0.1320 sec/batch\n",
      "Epoch 3/10  Iteration 930/3570 Training loss: 1.7330 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 931/3570 Training loss: 1.7330 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 932/3570 Training loss: 1.7330 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 933/3570 Training loss: 1.7327 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 934/3570 Training loss: 1.7325 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 935/3570 Training loss: 1.7325 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 936/3570 Training loss: 1.7323 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 937/3570 Training loss: 1.7324 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 938/3570 Training loss: 1.7323 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 939/3570 Training loss: 1.7323 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 940/3570 Training loss: 1.7324 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 941/3570 Training loss: 1.7322 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 942/3570 Training loss: 1.7321 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 943/3570 Training loss: 1.7318 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 944/3570 Training loss: 1.7317 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 945/3570 Training loss: 1.7315 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 946/3570 Training loss: 1.7313 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 947/3570 Training loss: 1.7311 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 948/3570 Training loss: 1.7310 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 949/3570 Training loss: 1.7310 0.1320 sec/batch\n",
      "Epoch 3/10  Iteration 950/3570 Training loss: 1.7309 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 951/3570 Training loss: 1.7308 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 952/3570 Training loss: 1.7305 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 953/3570 Training loss: 1.7304 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 954/3570 Training loss: 1.7300 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 955/3570 Training loss: 1.7300 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 956/3570 Training loss: 1.7302 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 957/3570 Training loss: 1.7301 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 958/3570 Training loss: 1.7302 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 959/3570 Training loss: 1.7302 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 960/3570 Training loss: 1.7302 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 961/3570 Training loss: 1.7301 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 962/3570 Training loss: 1.7301 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 963/3570 Training loss: 1.7301 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 964/3570 Training loss: 1.7300 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 965/3570 Training loss: 1.7299 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 966/3570 Training loss: 1.7299 0.1400 sec/batch\n",
      "Epoch 3/10  Iteration 967/3570 Training loss: 1.7297 0.1358 sec/batch\n",
      "Epoch 3/10  Iteration 968/3570 Training loss: 1.7296 0.1475 sec/batch\n",
      "Epoch 3/10  Iteration 969/3570 Training loss: 1.7298 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 970/3570 Training loss: 1.7296 0.1387 sec/batch\n",
      "Epoch 3/10  Iteration 971/3570 Training loss: 1.7295 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 972/3570 Training loss: 1.7295 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 973/3570 Training loss: 1.7294 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 974/3570 Training loss: 1.7293 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 975/3570 Training loss: 1.7291 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 976/3570 Training loss: 1.7290 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 977/3570 Training loss: 1.7289 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 978/3570 Training loss: 1.7289 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 979/3570 Training loss: 1.7288 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 980/3570 Training loss: 1.7287 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 981/3570 Training loss: 1.7287 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 982/3570 Training loss: 1.7285 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 983/3570 Training loss: 1.7287 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 984/3570 Training loss: 1.7288 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 985/3570 Training loss: 1.7287 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 986/3570 Training loss: 1.7287 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 987/3570 Training loss: 1.7286 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 988/3570 Training loss: 1.7286 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 989/3570 Training loss: 1.7286 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 990/3570 Training loss: 1.7284 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 991/3570 Training loss: 1.7281 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 992/3570 Training loss: 1.7281 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 993/3570 Training loss: 1.7278 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 994/3570 Training loss: 1.7278 0.1400 sec/batch\n",
      "Epoch 3/10  Iteration 995/3570 Training loss: 1.7276 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 996/3570 Training loss: 1.7275 0.1390 sec/batch\n",
      "Epoch 3/10  Iteration 997/3570 Training loss: 1.7274 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 998/3570 Training loss: 1.7272 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 999/3570 Training loss: 1.7268 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1000/3570 Training loss: 1.7267 0.1410 sec/batch\n",
      "Validation loss: 1.52092 Saving checkpoint!\n",
      "Epoch 3/10  Iteration 1001/3570 Training loss: 1.7270 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1002/3570 Training loss: 1.7270 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1003/3570 Training loss: 1.7270 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1004/3570 Training loss: 1.7267 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1005/3570 Training loss: 1.7264 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1006/3570 Training loss: 1.7263 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1007/3570 Training loss: 1.7262 0.1345 sec/batch\n",
      "Epoch 3/10  Iteration 1008/3570 Training loss: 1.7261 0.1369 sec/batch\n",
      "Epoch 3/10  Iteration 1009/3570 Training loss: 1.7261 0.1355 sec/batch\n",
      "Epoch 3/10  Iteration 1010/3570 Training loss: 1.7260 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1011/3570 Training loss: 1.7259 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1012/3570 Training loss: 1.7259 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1013/3570 Training loss: 1.7258 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1014/3570 Training loss: 1.7256 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1015/3570 Training loss: 1.7255 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1016/3570 Training loss: 1.7254 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1017/3570 Training loss: 1.7252 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1018/3570 Training loss: 1.7250 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1019/3570 Training loss: 1.7248 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 1020/3570 Training loss: 1.7246 0.1417 sec/batch\n",
      "Epoch 3/10  Iteration 1021/3570 Training loss: 1.7246 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1022/3570 Training loss: 1.7245 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1023/3570 Training loss: 1.7244 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1024/3570 Training loss: 1.7245 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1025/3570 Training loss: 1.7243 0.1320 sec/batch\n",
      "Epoch 3/10  Iteration 1026/3570 Training loss: 1.7242 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1027/3570 Training loss: 1.7242 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1028/3570 Training loss: 1.7242 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1029/3570 Training loss: 1.7242 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1030/3570 Training loss: 1.7242 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1031/3570 Training loss: 1.7241 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1032/3570 Training loss: 1.7240 0.1347 sec/batch\n",
      "Epoch 3/10  Iteration 1033/3570 Training loss: 1.7239 0.1338 sec/batch\n",
      "Epoch 3/10  Iteration 1034/3570 Training loss: 1.7238 0.1507 sec/batch\n",
      "Epoch 3/10  Iteration 1035/3570 Training loss: 1.7237 0.1362 sec/batch\n",
      "Epoch 3/10  Iteration 1036/3570 Training loss: 1.7236 0.1416 sec/batch\n",
      "Epoch 3/10  Iteration 1037/3570 Training loss: 1.7236 0.1463 sec/batch\n",
      "Epoch 3/10  Iteration 1038/3570 Training loss: 1.7234 0.1429 sec/batch\n",
      "Epoch 3/10  Iteration 1039/3570 Training loss: 1.7231 0.2380 sec/batch\n",
      "Epoch 3/10  Iteration 1040/3570 Training loss: 1.7230 0.1325 sec/batch\n",
      "Epoch 3/10  Iteration 1041/3570 Training loss: 1.7229 0.1440 sec/batch\n",
      "Epoch 3/10  Iteration 1042/3570 Training loss: 1.7229 0.1379 sec/batch\n",
      "Epoch 3/10  Iteration 1043/3570 Training loss: 1.7228 0.1400 sec/batch\n",
      "Epoch 3/10  Iteration 1044/3570 Training loss: 1.7228 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1045/3570 Training loss: 1.7227 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1046/3570 Training loss: 1.7228 0.1370 sec/batch\n",
      "Epoch 3/10  Iteration 1047/3570 Training loss: 1.7227 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1048/3570 Training loss: 1.7226 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1049/3570 Training loss: 1.7224 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1050/3570 Training loss: 1.7222 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1051/3570 Training loss: 1.7221 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1052/3570 Training loss: 1.7218 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1053/3570 Training loss: 1.7216 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1054/3570 Training loss: 1.7216 0.1360 sec/batch\n",
      "Epoch 3/10  Iteration 1055/3570 Training loss: 1.7216 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1056/3570 Training loss: 1.7216 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1057/3570 Training loss: 1.7215 0.1410 sec/batch\n",
      "Epoch 3/10  Iteration 1058/3570 Training loss: 1.7215 0.1410 sec/batch\n",
      "Epoch 3/10  Iteration 1059/3570 Training loss: 1.7213 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1060/3570 Training loss: 1.7213 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1061/3570 Training loss: 1.7214 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1062/3570 Training loss: 1.7213 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1063/3570 Training loss: 1.7212 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1064/3570 Training loss: 1.7213 0.1380 sec/batch\n",
      "Epoch 3/10  Iteration 1065/3570 Training loss: 1.7213 0.1310 sec/batch\n",
      "Epoch 3/10  Iteration 1066/3570 Training loss: 1.7213 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1067/3570 Training loss: 1.7213 0.1330 sec/batch\n",
      "Epoch 3/10  Iteration 1068/3570 Training loss: 1.7211 0.1340 sec/batch\n",
      "Epoch 3/10  Iteration 1069/3570 Training loss: 1.7211 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1070/3570 Training loss: 1.7210 0.1350 sec/batch\n",
      "Epoch 3/10  Iteration 1071/3570 Training loss: 1.7209 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1072/3570 Training loss: 1.7557 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1073/3570 Training loss: 1.7030 0.1427 sec/batch\n",
      "Epoch 4/10  Iteration 1074/3570 Training loss: 1.7013 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1075/3570 Training loss: 1.6841 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1076/3570 Training loss: 1.6734 0.1383 sec/batch\n",
      "Epoch 4/10  Iteration 1077/3570 Training loss: 1.6697 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1078/3570 Training loss: 1.6717 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1079/3570 Training loss: 1.6720 0.1320 sec/batch\n",
      "Epoch 4/10  Iteration 1080/3570 Training loss: 1.6761 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1081/3570 Training loss: 1.6773 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1082/3570 Training loss: 1.6798 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1083/3570 Training loss: 1.6797 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1084/3570 Training loss: 1.6793 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1085/3570 Training loss: 1.6783 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1086/3570 Training loss: 1.6762 0.1420 sec/batch\n",
      "Epoch 4/10  Iteration 1087/3570 Training loss: 1.6742 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1088/3570 Training loss: 1.6746 0.1407 sec/batch\n",
      "Epoch 4/10  Iteration 1089/3570 Training loss: 1.6739 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1090/3570 Training loss: 1.6741 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1091/3570 Training loss: 1.6761 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1092/3570 Training loss: 1.6765 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1093/3570 Training loss: 1.6782 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1094/3570 Training loss: 1.6774 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1095/3570 Training loss: 1.6761 0.1332 sec/batch\n",
      "Epoch 4/10  Iteration 1096/3570 Training loss: 1.6761 0.1372 sec/batch\n",
      "Epoch 4/10  Iteration 1097/3570 Training loss: 1.6764 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1098/3570 Training loss: 1.6767 0.1420 sec/batch\n",
      "Epoch 4/10  Iteration 1099/3570 Training loss: 1.6787 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1100/3570 Training loss: 1.6780 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1101/3570 Training loss: 1.6789 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1102/3570 Training loss: 1.6798 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1103/3570 Training loss: 1.6789 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1104/3570 Training loss: 1.6780 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1105/3570 Training loss: 1.6770 0.1363 sec/batch\n",
      "Epoch 4/10  Iteration 1106/3570 Training loss: 1.6760 0.1375 sec/batch\n",
      "Epoch 4/10  Iteration 1107/3570 Training loss: 1.6764 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1108/3570 Training loss: 1.6770 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1109/3570 Training loss: 1.6783 0.1377 sec/batch\n",
      "Epoch 4/10  Iteration 1110/3570 Training loss: 1.6794 0.1371 sec/batch\n",
      "Epoch 4/10  Iteration 1111/3570 Training loss: 1.6787 0.1375 sec/batch\n",
      "Epoch 4/10  Iteration 1112/3570 Training loss: 1.6793 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1113/3570 Training loss: 1.6790 0.1366 sec/batch\n",
      "Epoch 4/10  Iteration 1114/3570 Training loss: 1.6779 0.1399 sec/batch\n",
      "Epoch 4/10  Iteration 1115/3570 Training loss: 1.6774 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1116/3570 Training loss: 1.6766 0.1425 sec/batch\n",
      "Epoch 4/10  Iteration 1117/3570 Training loss: 1.6775 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1118/3570 Training loss: 1.6775 0.1394 sec/batch\n",
      "Epoch 4/10  Iteration 1119/3570 Training loss: 1.6767 0.1359 sec/batch\n",
      "Epoch 4/10  Iteration 1120/3570 Training loss: 1.6775 0.1371 sec/batch\n",
      "Epoch 4/10  Iteration 1121/3570 Training loss: 1.6774 0.1373 sec/batch\n",
      "Epoch 4/10  Iteration 1122/3570 Training loss: 1.6774 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1123/3570 Training loss: 1.6770 0.1348 sec/batch\n",
      "Epoch 4/10  Iteration 1124/3570 Training loss: 1.6771 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1125/3570 Training loss: 1.6776 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1126/3570 Training loss: 1.6777 0.1352 sec/batch\n",
      "Epoch 4/10  Iteration 1127/3570 Training loss: 1.6779 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1128/3570 Training loss: 1.6786 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1129/3570 Training loss: 1.6782 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1130/3570 Training loss: 1.6779 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1131/3570 Training loss: 1.6780 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1132/3570 Training loss: 1.6781 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1133/3570 Training loss: 1.6786 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1134/3570 Training loss: 1.6790 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1135/3570 Training loss: 1.6779 0.1341 sec/batch\n",
      "Epoch 4/10  Iteration 1136/3570 Training loss: 1.6787 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1137/3570 Training loss: 1.6787 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1138/3570 Training loss: 1.6783 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1139/3570 Training loss: 1.6784 0.1333 sec/batch\n",
      "Epoch 4/10  Iteration 1140/3570 Training loss: 1.6788 0.1359 sec/batch\n",
      "Epoch 4/10  Iteration 1141/3570 Training loss: 1.6793 0.1368 sec/batch\n",
      "Epoch 4/10  Iteration 1142/3570 Training loss: 1.6802 0.1338 sec/batch\n",
      "Epoch 4/10  Iteration 1143/3570 Training loss: 1.6801 0.1342 sec/batch\n",
      "Epoch 4/10  Iteration 1144/3570 Training loss: 1.6798 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1145/3570 Training loss: 1.6795 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1146/3570 Training loss: 1.6797 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1147/3570 Training loss: 1.6800 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1148/3570 Training loss: 1.6812 0.1338 sec/batch\n",
      "Epoch 4/10  Iteration 1149/3570 Training loss: 1.6813 0.1320 sec/batch\n",
      "Epoch 4/10  Iteration 1150/3570 Training loss: 1.6814 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1151/3570 Training loss: 1.6810 0.1332 sec/batch\n",
      "Epoch 4/10  Iteration 1152/3570 Training loss: 1.6806 0.1357 sec/batch\n",
      "Epoch 4/10  Iteration 1153/3570 Training loss: 1.6802 0.1359 sec/batch\n",
      "Epoch 4/10  Iteration 1154/3570 Training loss: 1.6805 0.1356 sec/batch\n",
      "Epoch 4/10  Iteration 1155/3570 Training loss: 1.6803 0.1397 sec/batch\n",
      "Epoch 4/10  Iteration 1156/3570 Training loss: 1.6801 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1157/3570 Training loss: 1.6797 0.1455 sec/batch\n",
      "Epoch 4/10  Iteration 1158/3570 Training loss: 1.6803 0.1464 sec/batch\n",
      "Epoch 4/10  Iteration 1159/3570 Training loss: 1.6801 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1160/3570 Training loss: 1.6801 0.1398 sec/batch\n",
      "Epoch 4/10  Iteration 1161/3570 Training loss: 1.6805 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1162/3570 Training loss: 1.6800 0.1347 sec/batch\n",
      "Epoch 4/10  Iteration 1163/3570 Training loss: 1.6794 0.1368 sec/batch\n",
      "Epoch 4/10  Iteration 1164/3570 Training loss: 1.6793 0.1348 sec/batch\n",
      "Epoch 4/10  Iteration 1165/3570 Training loss: 1.6793 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1166/3570 Training loss: 1.6793 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1167/3570 Training loss: 1.6792 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1168/3570 Training loss: 1.6792 0.1339 sec/batch\n",
      "Epoch 4/10  Iteration 1169/3570 Training loss: 1.6792 0.1345 sec/batch\n",
      "Epoch 4/10  Iteration 1170/3570 Training loss: 1.6793 0.1372 sec/batch\n",
      "Epoch 4/10  Iteration 1171/3570 Training loss: 1.6795 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1172/3570 Training loss: 1.6797 0.1357 sec/batch\n",
      "Epoch 4/10  Iteration 1173/3570 Training loss: 1.6795 0.1347 sec/batch\n",
      "Epoch 4/10  Iteration 1174/3570 Training loss: 1.6801 0.1352 sec/batch\n",
      "Epoch 4/10  Iteration 1175/3570 Training loss: 1.6797 0.1359 sec/batch\n",
      "Epoch 4/10  Iteration 1176/3570 Training loss: 1.6794 0.1387 sec/batch\n",
      "Epoch 4/10  Iteration 1177/3570 Training loss: 1.6792 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1178/3570 Training loss: 1.6789 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1179/3570 Training loss: 1.6784 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1180/3570 Training loss: 1.6781 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1181/3570 Training loss: 1.6780 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1182/3570 Training loss: 1.6777 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1183/3570 Training loss: 1.6778 0.1358 sec/batch\n",
      "Epoch 4/10  Iteration 1184/3570 Training loss: 1.6778 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1185/3570 Training loss: 1.6778 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1186/3570 Training loss: 1.6783 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1187/3570 Training loss: 1.6782 0.1447 sec/batch\n",
      "Epoch 4/10  Iteration 1188/3570 Training loss: 1.6779 0.1365 sec/batch\n",
      "Epoch 4/10  Iteration 1189/3570 Training loss: 1.6780 0.1407 sec/batch\n",
      "Epoch 4/10  Iteration 1190/3570 Training loss: 1.6784 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1191/3570 Training loss: 1.6785 0.1371 sec/batch\n",
      "Epoch 4/10  Iteration 1192/3570 Training loss: 1.6785 0.1356 sec/batch\n",
      "Epoch 4/10  Iteration 1193/3570 Training loss: 1.6783 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1194/3570 Training loss: 1.6785 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1195/3570 Training loss: 1.6782 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1196/3570 Training loss: 1.6781 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1197/3570 Training loss: 1.6780 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1198/3570 Training loss: 1.6777 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1199/3570 Training loss: 1.6777 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1200/3570 Training loss: 1.6776 0.1410 sec/batch\n",
      "Validation loss: 1.49154 Saving checkpoint!\n",
      "Epoch 4/10  Iteration 1201/3570 Training loss: 1.6781 0.1373 sec/batch\n",
      "Epoch 4/10  Iteration 1202/3570 Training loss: 1.6780 0.1384 sec/batch\n",
      "Epoch 4/10  Iteration 1203/3570 Training loss: 1.6779 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1204/3570 Training loss: 1.6779 0.1391 sec/batch\n",
      "Epoch 4/10  Iteration 1205/3570 Training loss: 1.6778 0.1354 sec/batch\n",
      "Epoch 4/10  Iteration 1206/3570 Training loss: 1.6775 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1207/3570 Training loss: 1.6773 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1208/3570 Training loss: 1.6773 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1209/3570 Training loss: 1.6771 0.1374 sec/batch\n",
      "Epoch 4/10  Iteration 1210/3570 Training loss: 1.6771 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1211/3570 Training loss: 1.6770 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1212/3570 Training loss: 1.6769 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1213/3570 Training loss: 1.6769 0.1365 sec/batch\n",
      "Epoch 4/10  Iteration 1214/3570 Training loss: 1.6766 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1215/3570 Training loss: 1.6762 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1216/3570 Training loss: 1.6761 0.1364 sec/batch\n",
      "Epoch 4/10  Iteration 1217/3570 Training loss: 1.6758 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1218/3570 Training loss: 1.6755 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1219/3570 Training loss: 1.6752 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1220/3570 Training loss: 1.6749 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1221/3570 Training loss: 1.6745 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1222/3570 Training loss: 1.6743 0.1420 sec/batch\n",
      "Epoch 4/10  Iteration 1223/3570 Training loss: 1.6740 0.1354 sec/batch\n",
      "Epoch 4/10  Iteration 1224/3570 Training loss: 1.6736 0.1354 sec/batch\n",
      "Epoch 4/10  Iteration 1225/3570 Training loss: 1.6734 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1226/3570 Training loss: 1.6736 0.1430 sec/batch\n",
      "Epoch 4/10  Iteration 1227/3570 Training loss: 1.6735 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1228/3570 Training loss: 1.6734 0.1375 sec/batch\n",
      "Epoch 4/10  Iteration 1229/3570 Training loss: 1.6734 0.1366 sec/batch\n",
      "Epoch 4/10  Iteration 1230/3570 Training loss: 1.6737 0.1422 sec/batch\n",
      "Epoch 4/10  Iteration 1231/3570 Training loss: 1.6737 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1232/3570 Training loss: 1.6734 0.1345 sec/batch\n",
      "Epoch 4/10  Iteration 1233/3570 Training loss: 1.6734 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1234/3570 Training loss: 1.6735 0.1354 sec/batch\n",
      "Epoch 4/10  Iteration 1235/3570 Training loss: 1.6733 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1236/3570 Training loss: 1.6734 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1237/3570 Training loss: 1.6735 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1238/3570 Training loss: 1.6731 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1239/3570 Training loss: 1.6731 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1240/3570 Training loss: 1.6729 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1241/3570 Training loss: 1.6728 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1242/3570 Training loss: 1.6726 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1243/3570 Training loss: 1.6725 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1244/3570 Training loss: 1.6725 0.1354 sec/batch\n",
      "Epoch 4/10  Iteration 1245/3570 Training loss: 1.6724 0.1354 sec/batch\n",
      "Epoch 4/10  Iteration 1246/3570 Training loss: 1.6724 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1247/3570 Training loss: 1.6718 0.1335 sec/batch\n",
      "Epoch 4/10  Iteration 1248/3570 Training loss: 1.6715 0.1404 sec/batch\n",
      "Epoch 4/10  Iteration 1249/3570 Training loss: 1.6711 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1250/3570 Training loss: 1.6709 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1251/3570 Training loss: 1.6708 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1252/3570 Training loss: 1.6706 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1253/3570 Training loss: 1.6705 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1254/3570 Training loss: 1.6706 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1255/3570 Training loss: 1.6704 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1256/3570 Training loss: 1.6702 0.1367 sec/batch\n",
      "Epoch 4/10  Iteration 1257/3570 Training loss: 1.6700 0.1424 sec/batch\n",
      "Epoch 4/10  Iteration 1258/3570 Training loss: 1.6698 0.1440 sec/batch\n",
      "Epoch 4/10  Iteration 1259/3570 Training loss: 1.6694 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1260/3570 Training loss: 1.6694 0.1358 sec/batch\n",
      "Epoch 4/10  Iteration 1261/3570 Training loss: 1.6692 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1262/3570 Training loss: 1.6690 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1263/3570 Training loss: 1.6692 0.1395 sec/batch\n",
      "Epoch 4/10  Iteration 1264/3570 Training loss: 1.6692 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1265/3570 Training loss: 1.6690 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1266/3570 Training loss: 1.6693 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1267/3570 Training loss: 1.6693 0.1450 sec/batch\n",
      "Epoch 4/10  Iteration 1268/3570 Training loss: 1.6693 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1269/3570 Training loss: 1.6694 0.1393 sec/batch\n",
      "Epoch 4/10  Iteration 1270/3570 Training loss: 1.6693 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1271/3570 Training loss: 1.6694 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1272/3570 Training loss: 1.6695 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1273/3570 Training loss: 1.6695 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1274/3570 Training loss: 1.6692 0.1430 sec/batch\n",
      "Epoch 4/10  Iteration 1275/3570 Training loss: 1.6693 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1276/3570 Training loss: 1.6691 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1277/3570 Training loss: 1.6691 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1278/3570 Training loss: 1.6689 0.1384 sec/batch\n",
      "Epoch 4/10  Iteration 1279/3570 Training loss: 1.6688 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1280/3570 Training loss: 1.6685 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1281/3570 Training loss: 1.6681 0.1460 sec/batch\n",
      "Epoch 4/10  Iteration 1282/3570 Training loss: 1.6681 0.1355 sec/batch\n",
      "Epoch 4/10  Iteration 1283/3570 Training loss: 1.6681 0.1347 sec/batch\n",
      "Epoch 4/10  Iteration 1284/3570 Training loss: 1.6679 0.1401 sec/batch\n",
      "Epoch 4/10  Iteration 1285/3570 Training loss: 1.6678 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1286/3570 Training loss: 1.6678 0.1389 sec/batch\n",
      "Epoch 4/10  Iteration 1287/3570 Training loss: 1.6677 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1288/3570 Training loss: 1.6677 0.1384 sec/batch\n",
      "Epoch 4/10  Iteration 1289/3570 Training loss: 1.6677 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1290/3570 Training loss: 1.6675 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1291/3570 Training loss: 1.6674 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1292/3570 Training loss: 1.6675 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1293/3570 Training loss: 1.6674 0.1358 sec/batch\n",
      "Epoch 4/10  Iteration 1294/3570 Training loss: 1.6675 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1295/3570 Training loss: 1.6675 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1296/3570 Training loss: 1.6675 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1297/3570 Training loss: 1.6676 0.1348 sec/batch\n",
      "Epoch 4/10  Iteration 1298/3570 Training loss: 1.6676 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1299/3570 Training loss: 1.6675 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1300/3570 Training loss: 1.6673 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1301/3570 Training loss: 1.6672 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1302/3570 Training loss: 1.6671 0.1415 sec/batch\n",
      "Epoch 4/10  Iteration 1303/3570 Training loss: 1.6670 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1304/3570 Training loss: 1.6668 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1305/3570 Training loss: 1.6668 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1306/3570 Training loss: 1.6669 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1307/3570 Training loss: 1.6669 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1308/3570 Training loss: 1.6668 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1309/3570 Training loss: 1.6665 0.1510 sec/batch\n",
      "Epoch 4/10  Iteration 1310/3570 Training loss: 1.6666 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1311/3570 Training loss: 1.6663 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1312/3570 Training loss: 1.6663 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1313/3570 Training loss: 1.6665 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1314/3570 Training loss: 1.6665 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1315/3570 Training loss: 1.6666 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1316/3570 Training loss: 1.6667 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1317/3570 Training loss: 1.6668 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1318/3570 Training loss: 1.6667 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1319/3570 Training loss: 1.6668 0.1393 sec/batch\n",
      "Epoch 4/10  Iteration 1320/3570 Training loss: 1.6669 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1321/3570 Training loss: 1.6669 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1322/3570 Training loss: 1.6668 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1323/3570 Training loss: 1.6669 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1324/3570 Training loss: 1.6667 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1325/3570 Training loss: 1.6667 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1326/3570 Training loss: 1.6668 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1327/3570 Training loss: 1.6667 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1328/3570 Training loss: 1.6667 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1329/3570 Training loss: 1.6667 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1330/3570 Training loss: 1.6666 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1331/3570 Training loss: 1.6667 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1332/3570 Training loss: 1.6666 0.1379 sec/batch\n",
      "Epoch 4/10  Iteration 1333/3570 Training loss: 1.6664 0.1362 sec/batch\n",
      "Epoch 4/10  Iteration 1334/3570 Training loss: 1.6665 0.1365 sec/batch\n",
      "Epoch 4/10  Iteration 1335/3570 Training loss: 1.6665 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1336/3570 Training loss: 1.6665 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1337/3570 Training loss: 1.6665 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1338/3570 Training loss: 1.6665 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1339/3570 Training loss: 1.6664 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1340/3570 Training loss: 1.6667 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1341/3570 Training loss: 1.6668 0.1364 sec/batch\n",
      "Epoch 4/10  Iteration 1342/3570 Training loss: 1.6668 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1343/3570 Training loss: 1.6668 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1344/3570 Training loss: 1.6668 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1345/3570 Training loss: 1.6668 0.1333 sec/batch\n",
      "Epoch 4/10  Iteration 1346/3570 Training loss: 1.6668 0.1358 sec/batch\n",
      "Epoch 4/10  Iteration 1347/3570 Training loss: 1.6668 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1348/3570 Training loss: 1.6664 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1349/3570 Training loss: 1.6663 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1350/3570 Training loss: 1.6660 0.1384 sec/batch\n",
      "Epoch 4/10  Iteration 1351/3570 Training loss: 1.6662 0.1345 sec/batch\n",
      "Epoch 4/10  Iteration 1352/3570 Training loss: 1.6660 0.1397 sec/batch\n",
      "Epoch 4/10  Iteration 1353/3570 Training loss: 1.6659 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1354/3570 Training loss: 1.6659 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1355/3570 Training loss: 1.6657 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1356/3570 Training loss: 1.6655 0.1348 sec/batch\n",
      "Epoch 4/10  Iteration 1357/3570 Training loss: 1.6654 0.1345 sec/batch\n",
      "Epoch 4/10  Iteration 1358/3570 Training loss: 1.6653 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1359/3570 Training loss: 1.6654 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1360/3570 Training loss: 1.6653 0.1407 sec/batch\n",
      "Epoch 4/10  Iteration 1361/3570 Training loss: 1.6650 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1362/3570 Training loss: 1.6647 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1363/3570 Training loss: 1.6646 0.1333 sec/batch\n",
      "Epoch 4/10  Iteration 1364/3570 Training loss: 1.6646 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1365/3570 Training loss: 1.6645 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1366/3570 Training loss: 1.6645 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1367/3570 Training loss: 1.6645 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1368/3570 Training loss: 1.6645 0.1379 sec/batch\n",
      "Epoch 4/10  Iteration 1369/3570 Training loss: 1.6644 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1370/3570 Training loss: 1.6644 0.1430 sec/batch\n",
      "Epoch 4/10  Iteration 1371/3570 Training loss: 1.6642 0.1460 sec/batch\n",
      "Epoch 4/10  Iteration 1372/3570 Training loss: 1.6642 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1373/3570 Training loss: 1.6641 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1374/3570 Training loss: 1.6639 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1375/3570 Training loss: 1.6638 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1376/3570 Training loss: 1.6637 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1377/3570 Training loss: 1.6635 0.1404 sec/batch\n",
      "Epoch 4/10  Iteration 1378/3570 Training loss: 1.6636 0.1357 sec/batch\n",
      "Epoch 4/10  Iteration 1379/3570 Training loss: 1.6635 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1380/3570 Training loss: 1.6635 0.1400 sec/batch\n",
      "Epoch 4/10  Iteration 1381/3570 Training loss: 1.6635 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1382/3570 Training loss: 1.6634 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1383/3570 Training loss: 1.6633 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1384/3570 Training loss: 1.6633 0.1440 sec/batch\n",
      "Epoch 4/10  Iteration 1385/3570 Training loss: 1.6634 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1386/3570 Training loss: 1.6634 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1387/3570 Training loss: 1.6635 0.1410 sec/batch\n",
      "Epoch 4/10  Iteration 1388/3570 Training loss: 1.6634 0.1450 sec/batch\n",
      "Epoch 4/10  Iteration 1389/3570 Training loss: 1.6634 0.1411 sec/batch\n",
      "Epoch 4/10  Iteration 1390/3570 Training loss: 1.6633 0.1403 sec/batch\n",
      "Epoch 4/10  Iteration 1391/3570 Training loss: 1.6633 0.1371 sec/batch\n",
      "Epoch 4/10  Iteration 1392/3570 Training loss: 1.6631 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1393/3570 Training loss: 1.6631 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1394/3570 Training loss: 1.6632 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1395/3570 Training loss: 1.6630 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1396/3570 Training loss: 1.6629 0.1390 sec/batch\n",
      "Epoch 4/10  Iteration 1397/3570 Training loss: 1.6628 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1398/3570 Training loss: 1.6627 0.1430 sec/batch\n",
      "Epoch 4/10  Iteration 1399/3570 Training loss: 1.6627 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1400/3570 Training loss: 1.6626 0.1340 sec/batch\n",
      "Validation loss: 1.46529 Saving checkpoint!\n",
      "Epoch 4/10  Iteration 1401/3570 Training loss: 1.6630 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1402/3570 Training loss: 1.6630 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1403/3570 Training loss: 1.6630 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1404/3570 Training loss: 1.6631 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1405/3570 Training loss: 1.6630 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1406/3570 Training loss: 1.6628 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1407/3570 Training loss: 1.6627 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1408/3570 Training loss: 1.6626 0.1352 sec/batch\n",
      "Epoch 4/10  Iteration 1409/3570 Training loss: 1.6623 0.1343 sec/batch\n",
      "Epoch 4/10  Iteration 1410/3570 Training loss: 1.6622 0.1387 sec/batch\n",
      "Epoch 4/10  Iteration 1411/3570 Training loss: 1.6622 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1412/3570 Training loss: 1.6622 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1413/3570 Training loss: 1.6623 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1414/3570 Training loss: 1.6623 0.1351 sec/batch\n",
      "Epoch 4/10  Iteration 1415/3570 Training loss: 1.6624 0.1362 sec/batch\n",
      "Epoch 4/10  Iteration 1416/3570 Training loss: 1.6623 0.1337 sec/batch\n",
      "Epoch 4/10  Iteration 1417/3570 Training loss: 1.6623 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1418/3570 Training loss: 1.6624 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1419/3570 Training loss: 1.6624 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1420/3570 Training loss: 1.6623 0.1360 sec/batch\n",
      "Epoch 4/10  Iteration 1421/3570 Training loss: 1.6624 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1422/3570 Training loss: 1.6624 0.1370 sec/batch\n",
      "Epoch 4/10  Iteration 1423/3570 Training loss: 1.6624 0.1330 sec/batch\n",
      "Epoch 4/10  Iteration 1424/3570 Training loss: 1.6624 0.1380 sec/batch\n",
      "Epoch 4/10  Iteration 1425/3570 Training loss: 1.6623 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1426/3570 Training loss: 1.6624 0.1350 sec/batch\n",
      "Epoch 4/10  Iteration 1427/3570 Training loss: 1.6623 0.1340 sec/batch\n",
      "Epoch 4/10  Iteration 1428/3570 Training loss: 1.6623 0.1450 sec/batch\n",
      "Epoch 5/10  Iteration 1429/3570 Training loss: 1.7121 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1430/3570 Training loss: 1.6592 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1431/3570 Training loss: 1.6569 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1432/3570 Training loss: 1.6393 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1433/3570 Training loss: 1.6284 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1434/3570 Training loss: 1.6271 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1435/3570 Training loss: 1.6266 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1436/3570 Training loss: 1.6255 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1437/3570 Training loss: 1.6287 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1438/3570 Training loss: 1.6295 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1439/3570 Training loss: 1.6339 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1440/3570 Training loss: 1.6333 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1441/3570 Training loss: 1.6345 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1442/3570 Training loss: 1.6326 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1443/3570 Training loss: 1.6305 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1444/3570 Training loss: 1.6290 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1445/3570 Training loss: 1.6295 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1446/3570 Training loss: 1.6301 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1447/3570 Training loss: 1.6302 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1448/3570 Training loss: 1.6320 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1449/3570 Training loss: 1.6311 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1450/3570 Training loss: 1.6324 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1451/3570 Training loss: 1.6313 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1452/3570 Training loss: 1.6306 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1453/3570 Training loss: 1.6320 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1454/3570 Training loss: 1.6327 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1455/3570 Training loss: 1.6335 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1456/3570 Training loss: 1.6351 0.1339 sec/batch\n",
      "Epoch 5/10  Iteration 1457/3570 Training loss: 1.6348 0.1396 sec/batch\n",
      "Epoch 5/10  Iteration 1458/3570 Training loss: 1.6357 0.1406 sec/batch\n",
      "Epoch 5/10  Iteration 1459/3570 Training loss: 1.6365 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1460/3570 Training loss: 1.6353 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1461/3570 Training loss: 1.6342 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1462/3570 Training loss: 1.6334 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1463/3570 Training loss: 1.6324 0.1382 sec/batch\n",
      "Epoch 5/10  Iteration 1464/3570 Training loss: 1.6330 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1465/3570 Training loss: 1.6340 0.1342 sec/batch\n",
      "Epoch 5/10  Iteration 1466/3570 Training loss: 1.6346 0.1546 sec/batch\n",
      "Epoch 5/10  Iteration 1467/3570 Training loss: 1.6353 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1468/3570 Training loss: 1.6353 0.1480 sec/batch\n",
      "Epoch 5/10  Iteration 1469/3570 Training loss: 1.6359 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1470/3570 Training loss: 1.6356 0.1352 sec/batch\n",
      "Epoch 5/10  Iteration 1471/3570 Training loss: 1.6352 0.1540 sec/batch\n",
      "Epoch 5/10  Iteration 1472/3570 Training loss: 1.6351 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1473/3570 Training loss: 1.6340 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1474/3570 Training loss: 1.6348 0.1420 sec/batch\n",
      "Epoch 5/10  Iteration 1475/3570 Training loss: 1.6348 0.1366 sec/batch\n",
      "Epoch 5/10  Iteration 1476/3570 Training loss: 1.6344 0.1361 sec/batch\n",
      "Epoch 5/10  Iteration 1477/3570 Training loss: 1.6354 0.1704 sec/batch\n",
      "Epoch 5/10  Iteration 1478/3570 Training loss: 1.6356 0.1691 sec/batch\n",
      "Epoch 5/10  Iteration 1479/3570 Training loss: 1.6356 0.1846 sec/batch\n",
      "Epoch 5/10  Iteration 1480/3570 Training loss: 1.6351 0.4205 sec/batch\n",
      "Epoch 5/10  Iteration 1481/3570 Training loss: 1.6353 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1482/3570 Training loss: 1.6360 0.1364 sec/batch\n",
      "Epoch 5/10  Iteration 1483/3570 Training loss: 1.6359 0.1361 sec/batch\n",
      "Epoch 5/10  Iteration 1484/3570 Training loss: 1.6362 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1485/3570 Training loss: 1.6364 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1486/3570 Training loss: 1.6360 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1487/3570 Training loss: 1.6354 0.1359 sec/batch\n",
      "Epoch 5/10  Iteration 1488/3570 Training loss: 1.6356 0.1363 sec/batch\n",
      "Epoch 5/10  Iteration 1489/3570 Training loss: 1.6359 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1490/3570 Training loss: 1.6364 0.1548 sec/batch\n",
      "Epoch 5/10  Iteration 1491/3570 Training loss: 1.6370 0.1510 sec/batch\n",
      "Epoch 5/10  Iteration 1492/3570 Training loss: 1.6360 0.1465 sec/batch\n",
      "Epoch 5/10  Iteration 1493/3570 Training loss: 1.6367 0.1361 sec/batch\n",
      "Epoch 5/10  Iteration 1494/3570 Training loss: 1.6368 0.1355 sec/batch\n",
      "Epoch 5/10  Iteration 1495/3570 Training loss: 1.6363 0.1353 sec/batch\n",
      "Epoch 5/10  Iteration 1496/3570 Training loss: 1.6364 0.1345 sec/batch\n",
      "Epoch 5/10  Iteration 1497/3570 Training loss: 1.6369 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1498/3570 Training loss: 1.6370 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1499/3570 Training loss: 1.6380 0.1375 sec/batch\n",
      "Epoch 5/10  Iteration 1500/3570 Training loss: 1.6380 0.1405 sec/batch\n",
      "Epoch 5/10  Iteration 1501/3570 Training loss: 1.6377 0.1430 sec/batch\n",
      "Epoch 5/10  Iteration 1502/3570 Training loss: 1.6375 0.1355 sec/batch\n",
      "Epoch 5/10  Iteration 1503/3570 Training loss: 1.6376 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1504/3570 Training loss: 1.6379 0.1355 sec/batch\n",
      "Epoch 5/10  Iteration 1505/3570 Training loss: 1.6390 0.1410 sec/batch\n",
      "Epoch 5/10  Iteration 1506/3570 Training loss: 1.6387 0.1345 sec/batch\n",
      "Epoch 5/10  Iteration 1507/3570 Training loss: 1.6386 0.1365 sec/batch\n",
      "Epoch 5/10  Iteration 1508/3570 Training loss: 1.6381 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1509/3570 Training loss: 1.6377 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1510/3570 Training loss: 1.6375 0.1345 sec/batch\n",
      "Epoch 5/10  Iteration 1511/3570 Training loss: 1.6379 0.1426 sec/batch\n",
      "Epoch 5/10  Iteration 1512/3570 Training loss: 1.6379 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1513/3570 Training loss: 1.6378 0.1438 sec/batch\n",
      "Epoch 5/10  Iteration 1514/3570 Training loss: 1.6374 0.1551 sec/batch\n",
      "Epoch 5/10  Iteration 1515/3570 Training loss: 1.6381 0.1425 sec/batch\n",
      "Epoch 5/10  Iteration 1516/3570 Training loss: 1.6381 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1517/3570 Training loss: 1.6381 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1518/3570 Training loss: 1.6385 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1519/3570 Training loss: 1.6385 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1520/3570 Training loss: 1.6379 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1521/3570 Training loss: 1.6378 0.1387 sec/batch\n",
      "Epoch 5/10  Iteration 1522/3570 Training loss: 1.6377 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1523/3570 Training loss: 1.6377 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1524/3570 Training loss: 1.6378 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1525/3570 Training loss: 1.6378 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1526/3570 Training loss: 1.6378 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1527/3570 Training loss: 1.6380 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1528/3570 Training loss: 1.6383 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1529/3570 Training loss: 1.6387 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1530/3570 Training loss: 1.6384 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1531/3570 Training loss: 1.6390 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1532/3570 Training loss: 1.6387 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1533/3570 Training loss: 1.6386 0.1341 sec/batch\n",
      "Epoch 5/10  Iteration 1534/3570 Training loss: 1.6384 0.1385 sec/batch\n",
      "Epoch 5/10  Iteration 1535/3570 Training loss: 1.6381 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1536/3570 Training loss: 1.6377 0.1336 sec/batch\n",
      "Epoch 5/10  Iteration 1537/3570 Training loss: 1.6373 0.1344 sec/batch\n",
      "Epoch 5/10  Iteration 1538/3570 Training loss: 1.6372 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1539/3570 Training loss: 1.6369 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1540/3570 Training loss: 1.6371 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1541/3570 Training loss: 1.6370 0.1420 sec/batch\n",
      "Epoch 5/10  Iteration 1542/3570 Training loss: 1.6370 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1543/3570 Training loss: 1.6373 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1544/3570 Training loss: 1.6371 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1545/3570 Training loss: 1.6368 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1546/3570 Training loss: 1.6370 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1547/3570 Training loss: 1.6375 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1548/3570 Training loss: 1.6375 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1549/3570 Training loss: 1.6375 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1550/3570 Training loss: 1.6372 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1551/3570 Training loss: 1.6373 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1552/3570 Training loss: 1.6369 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1553/3570 Training loss: 1.6370 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1554/3570 Training loss: 1.6369 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1555/3570 Training loss: 1.6366 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1556/3570 Training loss: 1.6367 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1557/3570 Training loss: 1.6367 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1558/3570 Training loss: 1.6364 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1559/3570 Training loss: 1.6362 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1560/3570 Training loss: 1.6360 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1561/3570 Training loss: 1.6361 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1562/3570 Training loss: 1.6361 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1563/3570 Training loss: 1.6359 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1564/3570 Training loss: 1.6358 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1565/3570 Training loss: 1.6358 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1566/3570 Training loss: 1.6357 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1567/3570 Training loss: 1.6356 0.1410 sec/batch\n",
      "Epoch 5/10  Iteration 1568/3570 Training loss: 1.6356 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1569/3570 Training loss: 1.6356 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1570/3570 Training loss: 1.6356 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1571/3570 Training loss: 1.6354 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1572/3570 Training loss: 1.6351 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1573/3570 Training loss: 1.6350 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1574/3570 Training loss: 1.6349 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1575/3570 Training loss: 1.6347 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1576/3570 Training loss: 1.6344 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1577/3570 Training loss: 1.6341 0.1420 sec/batch\n",
      "Epoch 5/10  Iteration 1578/3570 Training loss: 1.6338 0.1500 sec/batch\n",
      "Epoch 5/10  Iteration 1579/3570 Training loss: 1.6337 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1580/3570 Training loss: 1.6334 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1581/3570 Training loss: 1.6332 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1582/3570 Training loss: 1.6332 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1583/3570 Training loss: 1.6333 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1584/3570 Training loss: 1.6333 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1585/3570 Training loss: 1.6331 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1586/3570 Training loss: 1.6331 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1587/3570 Training loss: 1.6334 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1588/3570 Training loss: 1.6334 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1589/3570 Training loss: 1.6331 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1590/3570 Training loss: 1.6333 0.1420 sec/batch\n",
      "Epoch 5/10  Iteration 1591/3570 Training loss: 1.6335 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1592/3570 Training loss: 1.6333 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1593/3570 Training loss: 1.6334 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1594/3570 Training loss: 1.6336 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1595/3570 Training loss: 1.6333 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1596/3570 Training loss: 1.6332 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1597/3570 Training loss: 1.6330 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1598/3570 Training loss: 1.6330 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1599/3570 Training loss: 1.6328 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1600/3570 Training loss: 1.6327 0.1340 sec/batch\n",
      "Validation loss: 1.4507 Saving checkpoint!\n",
      "Epoch 5/10  Iteration 1601/3570 Training loss: 1.6338 0.1342 sec/batch\n",
      "Epoch 5/10  Iteration 1602/3570 Training loss: 1.6338 0.1420 sec/batch\n",
      "Epoch 5/10  Iteration 1603/3570 Training loss: 1.6339 0.1420 sec/batch\n",
      "Epoch 5/10  Iteration 1604/3570 Training loss: 1.6334 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1605/3570 Training loss: 1.6331 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1606/3570 Training loss: 1.6328 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1607/3570 Training loss: 1.6325 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1608/3570 Training loss: 1.6324 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1609/3570 Training loss: 1.6323 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1610/3570 Training loss: 1.6322 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1611/3570 Training loss: 1.6323 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1612/3570 Training loss: 1.6321 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1613/3570 Training loss: 1.6320 0.1413 sec/batch\n",
      "Epoch 5/10  Iteration 1614/3570 Training loss: 1.6319 0.1428 sec/batch\n",
      "Epoch 5/10  Iteration 1615/3570 Training loss: 1.6317 0.1410 sec/batch\n",
      "Epoch 5/10  Iteration 1616/3570 Training loss: 1.6313 0.1440 sec/batch\n",
      "Epoch 5/10  Iteration 1617/3570 Training loss: 1.6313 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1618/3570 Training loss: 1.6312 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1619/3570 Training loss: 1.6311 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1620/3570 Training loss: 1.6314 0.1404 sec/batch\n",
      "Epoch 5/10  Iteration 1621/3570 Training loss: 1.6314 0.1332 sec/batch\n",
      "Epoch 5/10  Iteration 1622/3570 Training loss: 1.6312 0.1361 sec/batch\n",
      "Epoch 5/10  Iteration 1623/3570 Training loss: 1.6315 0.1369 sec/batch\n",
      "Epoch 5/10  Iteration 1624/3570 Training loss: 1.6314 0.1356 sec/batch\n",
      "Epoch 5/10  Iteration 1625/3570 Training loss: 1.6314 0.1356 sec/batch\n",
      "Epoch 5/10  Iteration 1626/3570 Training loss: 1.6313 0.1416 sec/batch\n",
      "Epoch 5/10  Iteration 1627/3570 Training loss: 1.6313 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1628/3570 Training loss: 1.6314 0.1440 sec/batch\n",
      "Epoch 5/10  Iteration 1629/3570 Training loss: 1.6314 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1630/3570 Training loss: 1.6315 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1631/3570 Training loss: 1.6311 0.1352 sec/batch\n",
      "Epoch 5/10  Iteration 1632/3570 Training loss: 1.6312 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1633/3570 Training loss: 1.6309 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1634/3570 Training loss: 1.6309 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1635/3570 Training loss: 1.6308 0.1351 sec/batch\n",
      "Epoch 5/10  Iteration 1636/3570 Training loss: 1.6306 0.1378 sec/batch\n",
      "Epoch 5/10  Iteration 1637/3570 Training loss: 1.6304 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1638/3570 Training loss: 1.6301 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1639/3570 Training loss: 1.6302 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1640/3570 Training loss: 1.6302 0.1379 sec/batch\n",
      "Epoch 5/10  Iteration 1641/3570 Training loss: 1.6300 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1642/3570 Training loss: 1.6299 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1643/3570 Training loss: 1.6297 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1644/3570 Training loss: 1.6296 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1645/3570 Training loss: 1.6297 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1646/3570 Training loss: 1.6298 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1647/3570 Training loss: 1.6296 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1648/3570 Training loss: 1.6295 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1649/3570 Training loss: 1.6295 0.1357 sec/batch\n",
      "Epoch 5/10  Iteration 1650/3570 Training loss: 1.6295 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1651/3570 Training loss: 1.6295 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1652/3570 Training loss: 1.6296 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1653/3570 Training loss: 1.6296 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1654/3570 Training loss: 1.6298 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1655/3570 Training loss: 1.6297 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1656/3570 Training loss: 1.6296 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1657/3570 Training loss: 1.6295 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1658/3570 Training loss: 1.6294 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1659/3570 Training loss: 1.6293 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1660/3570 Training loss: 1.6292 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1661/3570 Training loss: 1.6290 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1662/3570 Training loss: 1.6289 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1663/3570 Training loss: 1.6290 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1664/3570 Training loss: 1.6290 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1665/3570 Training loss: 1.6290 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1666/3570 Training loss: 1.6288 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1667/3570 Training loss: 1.6289 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1668/3570 Training loss: 1.6287 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1669/3570 Training loss: 1.6286 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1670/3570 Training loss: 1.6288 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1671/3570 Training loss: 1.6289 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1672/3570 Training loss: 1.6290 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1673/3570 Training loss: 1.6291 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1674/3570 Training loss: 1.6292 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1675/3570 Training loss: 1.6291 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1676/3570 Training loss: 1.6292 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1677/3570 Training loss: 1.6293 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1678/3570 Training loss: 1.6294 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1679/3570 Training loss: 1.6293 0.1440 sec/batch\n",
      "Epoch 5/10  Iteration 1680/3570 Training loss: 1.6295 0.1470 sec/batch\n",
      "Epoch 5/10  Iteration 1681/3570 Training loss: 1.6292 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1682/3570 Training loss: 1.6291 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1683/3570 Training loss: 1.6293 0.1361 sec/batch\n",
      "Epoch 5/10  Iteration 1684/3570 Training loss: 1.6291 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1685/3570 Training loss: 1.6292 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1686/3570 Training loss: 1.6292 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1687/3570 Training loss: 1.6291 0.1450 sec/batch\n",
      "Epoch 5/10  Iteration 1688/3570 Training loss: 1.6291 0.1356 sec/batch\n",
      "Epoch 5/10  Iteration 1689/3570 Training loss: 1.6290 0.1395 sec/batch\n",
      "Epoch 5/10  Iteration 1690/3570 Training loss: 1.6289 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1691/3570 Training loss: 1.6289 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1692/3570 Training loss: 1.6290 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1693/3570 Training loss: 1.6290 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1694/3570 Training loss: 1.6291 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1695/3570 Training loss: 1.6291 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1696/3570 Training loss: 1.6290 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1697/3570 Training loss: 1.6292 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1698/3570 Training loss: 1.6294 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1699/3570 Training loss: 1.6293 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1700/3570 Training loss: 1.6294 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1701/3570 Training loss: 1.6294 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1702/3570 Training loss: 1.6294 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1703/3570 Training loss: 1.6295 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1704/3570 Training loss: 1.6294 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1705/3570 Training loss: 1.6292 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1706/3570 Training loss: 1.6292 0.1380 sec/batch\n",
      "Epoch 5/10  Iteration 1707/3570 Training loss: 1.6290 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1708/3570 Training loss: 1.6291 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1709/3570 Training loss: 1.6290 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1710/3570 Training loss: 1.6289 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1711/3570 Training loss: 1.6288 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1712/3570 Training loss: 1.6287 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1713/3570 Training loss: 1.6284 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1714/3570 Training loss: 1.6284 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1715/3570 Training loss: 1.6283 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1716/3570 Training loss: 1.6284 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1717/3570 Training loss: 1.6283 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1718/3570 Training loss: 1.6281 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1719/3570 Training loss: 1.6279 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1720/3570 Training loss: 1.6278 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1721/3570 Training loss: 1.6278 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1722/3570 Training loss: 1.6277 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1723/3570 Training loss: 1.6278 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1724/3570 Training loss: 1.6278 0.1410 sec/batch\n",
      "Epoch 5/10  Iteration 1725/3570 Training loss: 1.6278 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1726/3570 Training loss: 1.6278 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1727/3570 Training loss: 1.6277 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1728/3570 Training loss: 1.6276 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1729/3570 Training loss: 1.6276 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1730/3570 Training loss: 1.6275 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1731/3570 Training loss: 1.6273 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1732/3570 Training loss: 1.6271 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1733/3570 Training loss: 1.6271 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1734/3570 Training loss: 1.6269 0.1416 sec/batch\n",
      "Epoch 5/10  Iteration 1735/3570 Training loss: 1.6269 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1736/3570 Training loss: 1.6269 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1737/3570 Training loss: 1.6269 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1738/3570 Training loss: 1.6270 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1739/3570 Training loss: 1.6268 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1740/3570 Training loss: 1.6268 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1741/3570 Training loss: 1.6268 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1742/3570 Training loss: 1.6269 0.1390 sec/batch\n",
      "Epoch 5/10  Iteration 1743/3570 Training loss: 1.6269 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1744/3570 Training loss: 1.6270 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1745/3570 Training loss: 1.6270 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1746/3570 Training loss: 1.6270 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1747/3570 Training loss: 1.6270 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1748/3570 Training loss: 1.6269 0.1394 sec/batch\n",
      "Epoch 5/10  Iteration 1749/3570 Training loss: 1.6268 0.1352 sec/batch\n",
      "Epoch 5/10  Iteration 1750/3570 Training loss: 1.6269 0.1363 sec/batch\n",
      "Epoch 5/10  Iteration 1751/3570 Training loss: 1.6270 0.1355 sec/batch\n",
      "Epoch 5/10  Iteration 1752/3570 Training loss: 1.6269 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1753/3570 Training loss: 1.6267 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1754/3570 Training loss: 1.6267 0.1370 sec/batch\n",
      "Epoch 5/10  Iteration 1755/3570 Training loss: 1.6266 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1756/3570 Training loss: 1.6267 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1757/3570 Training loss: 1.6266 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1758/3570 Training loss: 1.6266 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1759/3570 Training loss: 1.6266 0.1345 sec/batch\n",
      "Epoch 5/10  Iteration 1760/3570 Training loss: 1.6266 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1761/3570 Training loss: 1.6266 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1762/3570 Training loss: 1.6265 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1763/3570 Training loss: 1.6263 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1764/3570 Training loss: 1.6262 0.1430 sec/batch\n",
      "Epoch 5/10  Iteration 1765/3570 Training loss: 1.6261 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1766/3570 Training loss: 1.6258 0.1360 sec/batch\n",
      "Epoch 5/10  Iteration 1767/3570 Training loss: 1.6258 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1768/3570 Training loss: 1.6258 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1769/3570 Training loss: 1.6258 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1770/3570 Training loss: 1.6258 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1771/3570 Training loss: 1.6258 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1772/3570 Training loss: 1.6259 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1773/3570 Training loss: 1.6258 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1774/3570 Training loss: 1.6259 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1775/3570 Training loss: 1.6260 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1776/3570 Training loss: 1.6260 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1777/3570 Training loss: 1.6259 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1778/3570 Training loss: 1.6259 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1779/3570 Training loss: 1.6260 0.1340 sec/batch\n",
      "Epoch 5/10  Iteration 1780/3570 Training loss: 1.6261 0.1400 sec/batch\n",
      "Epoch 5/10  Iteration 1781/3570 Training loss: 1.6261 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1782/3570 Training loss: 1.6260 0.1350 sec/batch\n",
      "Epoch 5/10  Iteration 1783/3570 Training loss: 1.6260 0.1330 sec/batch\n",
      "Epoch 5/10  Iteration 1784/3570 Training loss: 1.6260 0.1320 sec/batch\n",
      "Epoch 5/10  Iteration 1785/3570 Training loss: 1.6260 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1786/3570 Training loss: 1.7104 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 1787/3570 Training loss: 1.6393 0.1460 sec/batch\n",
      "Epoch 6/10  Iteration 1788/3570 Training loss: 1.6360 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 1789/3570 Training loss: 1.6162 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1790/3570 Training loss: 1.6053 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1791/3570 Training loss: 1.6021 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1792/3570 Training loss: 1.6009 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1793/3570 Training loss: 1.5997 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1794/3570 Training loss: 1.6053 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1795/3570 Training loss: 1.6059 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1796/3570 Training loss: 1.6099 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1797/3570 Training loss: 1.6105 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1798/3570 Training loss: 1.6094 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1799/3570 Training loss: 1.6061 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1800/3570 Training loss: 1.6035 0.1410 sec/batch\n",
      "Validation loss: 1.43687 Saving checkpoint!\n",
      "Epoch 6/10  Iteration 1801/3570 Training loss: 1.6117 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1802/3570 Training loss: 1.6123 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1803/3570 Training loss: 1.6118 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1804/3570 Training loss: 1.6109 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1805/3570 Training loss: 1.6121 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1806/3570 Training loss: 1.6118 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1807/3570 Training loss: 1.6127 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1808/3570 Training loss: 1.6117 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1809/3570 Training loss: 1.6106 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1810/3570 Training loss: 1.6112 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1811/3570 Training loss: 1.6119 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1812/3570 Training loss: 1.6119 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1813/3570 Training loss: 1.6137 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1814/3570 Training loss: 1.6133 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1815/3570 Training loss: 1.6141 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1816/3570 Training loss: 1.6152 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1817/3570 Training loss: 1.6140 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 1818/3570 Training loss: 1.6130 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1819/3570 Training loss: 1.6119 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1820/3570 Training loss: 1.6112 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1821/3570 Training loss: 1.6118 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1822/3570 Training loss: 1.6125 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1823/3570 Training loss: 1.6130 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1824/3570 Training loss: 1.6138 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1825/3570 Training loss: 1.6128 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1826/3570 Training loss: 1.6132 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1827/3570 Training loss: 1.6123 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1828/3570 Training loss: 1.6113 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1829/3570 Training loss: 1.6110 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1830/3570 Training loss: 1.6102 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1831/3570 Training loss: 1.6108 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1832/3570 Training loss: 1.6107 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1833/3570 Training loss: 1.6104 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1834/3570 Training loss: 1.6107 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1835/3570 Training loss: 1.6108 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1836/3570 Training loss: 1.6105 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1837/3570 Training loss: 1.6100 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1838/3570 Training loss: 1.6097 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1839/3570 Training loss: 1.6103 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1840/3570 Training loss: 1.6104 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1841/3570 Training loss: 1.6102 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1842/3570 Training loss: 1.6106 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1843/3570 Training loss: 1.6101 0.1341 sec/batch\n",
      "Epoch 6/10  Iteration 1844/3570 Training loss: 1.6098 0.1329 sec/batch\n",
      "Epoch 6/10  Iteration 1845/3570 Training loss: 1.6101 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 1846/3570 Training loss: 1.6104 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1847/3570 Training loss: 1.6108 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1848/3570 Training loss: 1.6112 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1849/3570 Training loss: 1.6099 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1850/3570 Training loss: 1.6105 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1851/3570 Training loss: 1.6105 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1852/3570 Training loss: 1.6097 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1853/3570 Training loss: 1.6101 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1854/3570 Training loss: 1.6106 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1855/3570 Training loss: 1.6108 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1856/3570 Training loss: 1.6118 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1857/3570 Training loss: 1.6118 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1858/3570 Training loss: 1.6113 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 1859/3570 Training loss: 1.6110 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1860/3570 Training loss: 1.6111 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1861/3570 Training loss: 1.6115 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1862/3570 Training loss: 1.6127 0.1408 sec/batch\n",
      "Epoch 6/10  Iteration 1863/3570 Training loss: 1.6126 0.1405 sec/batch\n",
      "Epoch 6/10  Iteration 1864/3570 Training loss: 1.6124 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1865/3570 Training loss: 1.6119 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1866/3570 Training loss: 1.6115 0.1345 sec/batch\n",
      "Epoch 6/10  Iteration 1867/3570 Training loss: 1.6112 0.1333 sec/batch\n",
      "Epoch 6/10  Iteration 1868/3570 Training loss: 1.6115 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1869/3570 Training loss: 1.6114 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1870/3570 Training loss: 1.6110 0.1391 sec/batch\n",
      "Epoch 6/10  Iteration 1871/3570 Training loss: 1.6108 0.1495 sec/batch\n",
      "Epoch 6/10  Iteration 1872/3570 Training loss: 1.6114 0.1334 sec/batch\n",
      "Epoch 6/10  Iteration 1873/3570 Training loss: 1.6112 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1874/3570 Training loss: 1.6114 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1875/3570 Training loss: 1.6116 0.1359 sec/batch\n",
      "Epoch 6/10  Iteration 1876/3570 Training loss: 1.6114 0.1358 sec/batch\n",
      "Epoch 6/10  Iteration 1877/3570 Training loss: 1.6106 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1878/3570 Training loss: 1.6107 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1879/3570 Training loss: 1.6106 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1880/3570 Training loss: 1.6106 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1881/3570 Training loss: 1.6107 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1882/3570 Training loss: 1.6108 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1883/3570 Training loss: 1.6110 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1884/3570 Training loss: 1.6111 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1885/3570 Training loss: 1.6113 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1886/3570 Training loss: 1.6118 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1887/3570 Training loss: 1.6115 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1888/3570 Training loss: 1.6123 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1889/3570 Training loss: 1.6119 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1890/3570 Training loss: 1.6115 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1891/3570 Training loss: 1.6115 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1892/3570 Training loss: 1.6110 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1893/3570 Training loss: 1.6106 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1894/3570 Training loss: 1.6100 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1895/3570 Training loss: 1.6101 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1896/3570 Training loss: 1.6098 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 1897/3570 Training loss: 1.6097 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1898/3570 Training loss: 1.6097 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1899/3570 Training loss: 1.6097 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1900/3570 Training loss: 1.6101 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1901/3570 Training loss: 1.6100 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1902/3570 Training loss: 1.6098 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1903/3570 Training loss: 1.6100 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1904/3570 Training loss: 1.6104 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1905/3570 Training loss: 1.6104 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1906/3570 Training loss: 1.6103 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1907/3570 Training loss: 1.6100 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1908/3570 Training loss: 1.6100 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1909/3570 Training loss: 1.6098 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1910/3570 Training loss: 1.6099 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1911/3570 Training loss: 1.6099 0.1335 sec/batch\n",
      "Epoch 6/10  Iteration 1912/3570 Training loss: 1.6097 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1913/3570 Training loss: 1.6098 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1914/3570 Training loss: 1.6097 0.1450 sec/batch\n",
      "Epoch 6/10  Iteration 1915/3570 Training loss: 1.6093 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1916/3570 Training loss: 1.6093 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1917/3570 Training loss: 1.6091 0.1333 sec/batch\n",
      "Epoch 6/10  Iteration 1918/3570 Training loss: 1.6091 0.1377 sec/batch\n",
      "Epoch 6/10  Iteration 1919/3570 Training loss: 1.6092 0.1450 sec/batch\n",
      "Epoch 6/10  Iteration 1920/3570 Training loss: 1.6090 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1921/3570 Training loss: 1.6089 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1922/3570 Training loss: 1.6089 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1923/3570 Training loss: 1.6089 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1924/3570 Training loss: 1.6087 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1925/3570 Training loss: 1.6087 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1926/3570 Training loss: 1.6086 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1927/3570 Training loss: 1.6088 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1928/3570 Training loss: 1.6085 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1929/3570 Training loss: 1.6083 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1930/3570 Training loss: 1.6082 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1931/3570 Training loss: 1.6080 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1932/3570 Training loss: 1.6078 0.1431 sec/batch\n",
      "Epoch 6/10  Iteration 1933/3570 Training loss: 1.6074 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1934/3570 Training loss: 1.6072 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1935/3570 Training loss: 1.6068 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1936/3570 Training loss: 1.6067 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1937/3570 Training loss: 1.6064 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1938/3570 Training loss: 1.6061 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1939/3570 Training loss: 1.6061 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1940/3570 Training loss: 1.6062 0.1421 sec/batch\n",
      "Epoch 6/10  Iteration 1941/3570 Training loss: 1.6062 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1942/3570 Training loss: 1.6061 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1943/3570 Training loss: 1.6061 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1944/3570 Training loss: 1.6064 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1945/3570 Training loss: 1.6065 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1946/3570 Training loss: 1.6062 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1947/3570 Training loss: 1.6063 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1948/3570 Training loss: 1.6066 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1949/3570 Training loss: 1.6065 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1950/3570 Training loss: 1.6066 0.1430 sec/batch\n",
      "Epoch 6/10  Iteration 1951/3570 Training loss: 1.6065 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1952/3570 Training loss: 1.6062 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1953/3570 Training loss: 1.6062 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1954/3570 Training loss: 1.6061 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 1955/3570 Training loss: 1.6061 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1956/3570 Training loss: 1.6059 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1957/3570 Training loss: 1.6056 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1958/3570 Training loss: 1.6057 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1959/3570 Training loss: 1.6057 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1960/3570 Training loss: 1.6056 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1961/3570 Training loss: 1.6051 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1962/3570 Training loss: 1.6048 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1963/3570 Training loss: 1.6045 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1964/3570 Training loss: 1.6042 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1965/3570 Training loss: 1.6041 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1966/3570 Training loss: 1.6039 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1967/3570 Training loss: 1.6038 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1968/3570 Training loss: 1.6038 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1969/3570 Training loss: 1.6036 0.1357 sec/batch\n",
      "Epoch 6/10  Iteration 1970/3570 Training loss: 1.6034 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1971/3570 Training loss: 1.6033 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1972/3570 Training loss: 1.6031 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1973/3570 Training loss: 1.6027 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1974/3570 Training loss: 1.6028 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 1975/3570 Training loss: 1.6028 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1976/3570 Training loss: 1.6026 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1977/3570 Training loss: 1.6029 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1978/3570 Training loss: 1.6031 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1979/3570 Training loss: 1.6029 0.1351 sec/batch\n",
      "Epoch 6/10  Iteration 1980/3570 Training loss: 1.6031 0.1369 sec/batch\n",
      "Epoch 6/10  Iteration 1981/3570 Training loss: 1.6030 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1982/3570 Training loss: 1.6030 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1983/3570 Training loss: 1.6031 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1984/3570 Training loss: 1.6032 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1985/3570 Training loss: 1.6033 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 1986/3570 Training loss: 1.6033 0.1440 sec/batch\n",
      "Epoch 6/10  Iteration 1987/3570 Training loss: 1.6035 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 1988/3570 Training loss: 1.6032 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1989/3570 Training loss: 1.6034 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 1990/3570 Training loss: 1.6032 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1991/3570 Training loss: 1.6033 0.1331 sec/batch\n",
      "Epoch 6/10  Iteration 1992/3570 Training loss: 1.6031 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1993/3570 Training loss: 1.6029 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 1994/3570 Training loss: 1.6028 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 1995/3570 Training loss: 1.6025 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 1996/3570 Training loss: 1.6025 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 1997/3570 Training loss: 1.6025 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 1998/3570 Training loss: 1.6024 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 1999/3570 Training loss: 1.6023 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2000/3570 Training loss: 1.6022 0.1340 sec/batch\n",
      "Validation loss: 1.42242 Saving checkpoint!\n",
      "Epoch 6/10  Iteration 2001/3570 Training loss: 1.6029 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2002/3570 Training loss: 1.6030 0.1368 sec/batch\n",
      "Epoch 6/10  Iteration 2003/3570 Training loss: 1.6031 0.1342 sec/batch\n",
      "Epoch 6/10  Iteration 2004/3570 Training loss: 1.6030 0.1469 sec/batch\n",
      "Epoch 6/10  Iteration 2005/3570 Training loss: 1.6029 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2006/3570 Training loss: 1.6030 0.1394 sec/batch\n",
      "Epoch 6/10  Iteration 2007/3570 Training loss: 1.6030 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2008/3570 Training loss: 1.6031 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 2009/3570 Training loss: 1.6032 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 2010/3570 Training loss: 1.6032 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2011/3570 Training loss: 1.6034 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2012/3570 Training loss: 1.6033 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 2013/3570 Training loss: 1.6033 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2014/3570 Training loss: 1.6033 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2015/3570 Training loss: 1.6031 0.1457 sec/batch\n",
      "Epoch 6/10  Iteration 2016/3570 Training loss: 1.6030 0.1408 sec/batch\n",
      "Epoch 6/10  Iteration 2017/3570 Training loss: 1.6029 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2018/3570 Training loss: 1.6028 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2019/3570 Training loss: 1.6028 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2020/3570 Training loss: 1.6028 0.1430 sec/batch\n",
      "Epoch 6/10  Iteration 2021/3570 Training loss: 1.6029 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2022/3570 Training loss: 1.6030 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2023/3570 Training loss: 1.6027 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2024/3570 Training loss: 1.6027 0.1420 sec/batch\n",
      "Epoch 6/10  Iteration 2025/3570 Training loss: 1.6025 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2026/3570 Training loss: 1.6025 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2027/3570 Training loss: 1.6028 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2028/3570 Training loss: 1.6028 0.1490 sec/batch\n",
      "Epoch 6/10  Iteration 2029/3570 Training loss: 1.6028 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2030/3570 Training loss: 1.6029 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2031/3570 Training loss: 1.6030 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2032/3570 Training loss: 1.6030 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2033/3570 Training loss: 1.6031 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2034/3570 Training loss: 1.6032 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2035/3570 Training loss: 1.6032 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2036/3570 Training loss: 1.6032 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2037/3570 Training loss: 1.6034 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2038/3570 Training loss: 1.6033 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2039/3570 Training loss: 1.6033 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2040/3570 Training loss: 1.6035 0.1367 sec/batch\n",
      "Epoch 6/10  Iteration 2041/3570 Training loss: 1.6034 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2042/3570 Training loss: 1.6034 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 2043/3570 Training loss: 1.6033 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2044/3570 Training loss: 1.6033 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2045/3570 Training loss: 1.6032 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2046/3570 Training loss: 1.6031 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 2047/3570 Training loss: 1.6030 0.1520 sec/batch\n",
      "Epoch 6/10  Iteration 2048/3570 Training loss: 1.6031 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2049/3570 Training loss: 1.6032 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2050/3570 Training loss: 1.6032 0.1358 sec/batch\n",
      "Epoch 6/10  Iteration 2051/3570 Training loss: 1.6032 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2052/3570 Training loss: 1.6032 0.1354 sec/batch\n",
      "Epoch 6/10  Iteration 2053/3570 Training loss: 1.6031 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2054/3570 Training loss: 1.6034 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 2055/3570 Training loss: 1.6035 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 2056/3570 Training loss: 1.6035 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2057/3570 Training loss: 1.6036 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2058/3570 Training loss: 1.6036 0.1450 sec/batch\n",
      "Epoch 6/10  Iteration 2059/3570 Training loss: 1.6037 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2060/3570 Training loss: 1.6038 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2061/3570 Training loss: 1.6037 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 2062/3570 Training loss: 1.6035 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2063/3570 Training loss: 1.6035 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2064/3570 Training loss: 1.6033 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 2065/3570 Training loss: 1.6034 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2066/3570 Training loss: 1.6032 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2067/3570 Training loss: 1.6032 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2068/3570 Training loss: 1.6032 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 2069/3570 Training loss: 1.6031 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2070/3570 Training loss: 1.6028 0.1434 sec/batch\n",
      "Epoch 6/10  Iteration 2071/3570 Training loss: 1.6027 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 2072/3570 Training loss: 1.6028 0.1592 sec/batch\n",
      "Epoch 6/10  Iteration 2073/3570 Training loss: 1.6028 0.1481 sec/batch\n",
      "Epoch 6/10  Iteration 2074/3570 Training loss: 1.6028 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2075/3570 Training loss: 1.6026 0.1410 sec/batch\n",
      "Epoch 6/10  Iteration 2076/3570 Training loss: 1.6024 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2077/3570 Training loss: 1.6023 0.1320 sec/batch\n",
      "Epoch 6/10  Iteration 2078/3570 Training loss: 1.6023 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2079/3570 Training loss: 1.6022 0.1384 sec/batch\n",
      "Epoch 6/10  Iteration 2080/3570 Training loss: 1.6022 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2081/3570 Training loss: 1.6023 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2082/3570 Training loss: 1.6022 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2083/3570 Training loss: 1.6022 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2084/3570 Training loss: 1.6022 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2085/3570 Training loss: 1.6021 0.1348 sec/batch\n",
      "Epoch 6/10  Iteration 2086/3570 Training loss: 1.6020 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2087/3570 Training loss: 1.6019 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2088/3570 Training loss: 1.6018 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2089/3570 Training loss: 1.6017 0.1400 sec/batch\n",
      "Epoch 6/10  Iteration 2090/3570 Training loss: 1.6016 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2091/3570 Training loss: 1.6014 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2092/3570 Training loss: 1.6013 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2093/3570 Training loss: 1.6013 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2094/3570 Training loss: 1.6013 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2095/3570 Training loss: 1.6014 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2096/3570 Training loss: 1.6012 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2097/3570 Training loss: 1.6013 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2098/3570 Training loss: 1.6013 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2099/3570 Training loss: 1.6014 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2100/3570 Training loss: 1.6014 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2101/3570 Training loss: 1.6014 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2102/3570 Training loss: 1.6014 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2103/3570 Training loss: 1.6014 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2104/3570 Training loss: 1.6013 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2105/3570 Training loss: 1.6013 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2106/3570 Training loss: 1.6012 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2107/3570 Training loss: 1.6012 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2108/3570 Training loss: 1.6013 0.1368 sec/batch\n",
      "Epoch 6/10  Iteration 2109/3570 Training loss: 1.6012 0.1395 sec/batch\n",
      "Epoch 6/10  Iteration 2110/3570 Training loss: 1.6011 0.1420 sec/batch\n",
      "Epoch 6/10  Iteration 2111/3570 Training loss: 1.6010 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2112/3570 Training loss: 1.6010 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2113/3570 Training loss: 1.6010 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2114/3570 Training loss: 1.6009 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2115/3570 Training loss: 1.6009 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2116/3570 Training loss: 1.6010 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2117/3570 Training loss: 1.6010 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2118/3570 Training loss: 1.6011 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2119/3570 Training loss: 1.6010 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2120/3570 Training loss: 1.6009 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2121/3570 Training loss: 1.6007 0.1590 sec/batch\n",
      "Epoch 6/10  Iteration 2122/3570 Training loss: 1.6007 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2123/3570 Training loss: 1.6005 0.1360 sec/batch\n",
      "Epoch 6/10  Iteration 2124/3570 Training loss: 1.6004 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2125/3570 Training loss: 1.6004 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2126/3570 Training loss: 1.6004 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2127/3570 Training loss: 1.6005 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2128/3570 Training loss: 1.6005 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2129/3570 Training loss: 1.6006 0.1365 sec/batch\n",
      "Epoch 6/10  Iteration 2130/3570 Training loss: 1.6005 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2131/3570 Training loss: 1.6005 0.1390 sec/batch\n",
      "Epoch 6/10  Iteration 2132/3570 Training loss: 1.6006 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2133/3570 Training loss: 1.6006 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2134/3570 Training loss: 1.6006 0.1330 sec/batch\n",
      "Epoch 6/10  Iteration 2135/3570 Training loss: 1.6007 0.1380 sec/batch\n",
      "Epoch 6/10  Iteration 2136/3570 Training loss: 1.6008 0.1340 sec/batch\n",
      "Epoch 6/10  Iteration 2137/3570 Training loss: 1.6009 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2138/3570 Training loss: 1.6009 0.1370 sec/batch\n",
      "Epoch 6/10  Iteration 2139/3570 Training loss: 1.6009 0.1372 sec/batch\n",
      "Epoch 6/10  Iteration 2140/3570 Training loss: 1.6009 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2141/3570 Training loss: 1.6009 0.1350 sec/batch\n",
      "Epoch 6/10  Iteration 2142/3570 Training loss: 1.6009 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2143/3570 Training loss: 1.6642 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2144/3570 Training loss: 1.6131 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2145/3570 Training loss: 1.6086 0.1563 sec/batch\n",
      "Epoch 7/10  Iteration 2146/3570 Training loss: 1.5922 0.1390 sec/batch\n",
      "Epoch 7/10  Iteration 2147/3570 Training loss: 1.5815 0.1440 sec/batch\n",
      "Epoch 7/10  Iteration 2148/3570 Training loss: 1.5779 0.1449 sec/batch\n",
      "Epoch 7/10  Iteration 2149/3570 Training loss: 1.5775 0.1436 sec/batch\n",
      "Epoch 7/10  Iteration 2150/3570 Training loss: 1.5783 0.1394 sec/batch\n",
      "Epoch 7/10  Iteration 2151/3570 Training loss: 1.5812 0.1383 sec/batch\n",
      "Epoch 7/10  Iteration 2152/3570 Training loss: 1.5807 0.1339 sec/batch\n",
      "Epoch 7/10  Iteration 2153/3570 Training loss: 1.5840 0.1374 sec/batch\n",
      "Epoch 7/10  Iteration 2154/3570 Training loss: 1.5865 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2155/3570 Training loss: 1.5878 0.1347 sec/batch\n",
      "Epoch 7/10  Iteration 2156/3570 Training loss: 1.5861 0.1455 sec/batch\n",
      "Epoch 7/10  Iteration 2157/3570 Training loss: 1.5835 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2158/3570 Training loss: 1.5821 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2159/3570 Training loss: 1.5827 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2160/3570 Training loss: 1.5814 0.1354 sec/batch\n",
      "Epoch 7/10  Iteration 2161/3570 Training loss: 1.5814 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2162/3570 Training loss: 1.5823 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2163/3570 Training loss: 1.5824 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2164/3570 Training loss: 1.5840 0.1349 sec/batch\n",
      "Epoch 7/10  Iteration 2165/3570 Training loss: 1.5834 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2166/3570 Training loss: 1.5823 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2167/3570 Training loss: 1.5830 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2168/3570 Training loss: 1.5840 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2169/3570 Training loss: 1.5841 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2170/3570 Training loss: 1.5855 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2171/3570 Training loss: 1.5853 0.1440 sec/batch\n",
      "Epoch 7/10  Iteration 2172/3570 Training loss: 1.5863 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2173/3570 Training loss: 1.5878 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2174/3570 Training loss: 1.5863 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2175/3570 Training loss: 1.5853 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2176/3570 Training loss: 1.5844 0.1364 sec/batch\n",
      "Epoch 7/10  Iteration 2177/3570 Training loss: 1.5842 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2178/3570 Training loss: 1.5850 0.1450 sec/batch\n",
      "Epoch 7/10  Iteration 2179/3570 Training loss: 1.5857 0.1409 sec/batch\n",
      "Epoch 7/10  Iteration 2180/3570 Training loss: 1.5866 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2181/3570 Training loss: 1.5870 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2182/3570 Training loss: 1.5867 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2183/3570 Training loss: 1.5874 0.1354 sec/batch\n",
      "Epoch 7/10  Iteration 2184/3570 Training loss: 1.5869 0.1352 sec/batch\n",
      "Epoch 7/10  Iteration 2185/3570 Training loss: 1.5858 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2186/3570 Training loss: 1.5856 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2187/3570 Training loss: 1.5847 0.1351 sec/batch\n",
      "Epoch 7/10  Iteration 2188/3570 Training loss: 1.5853 0.1372 sec/batch\n",
      "Epoch 7/10  Iteration 2189/3570 Training loss: 1.5853 0.1353 sec/batch\n",
      "Epoch 7/10  Iteration 2190/3570 Training loss: 1.5850 0.1355 sec/batch\n",
      "Epoch 7/10  Iteration 2191/3570 Training loss: 1.5855 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2192/3570 Training loss: 1.5853 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2193/3570 Training loss: 1.5852 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2194/3570 Training loss: 1.5849 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2195/3570 Training loss: 1.5850 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2196/3570 Training loss: 1.5855 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2197/3570 Training loss: 1.5854 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2198/3570 Training loss: 1.5854 0.1390 sec/batch\n",
      "Epoch 7/10  Iteration 2199/3570 Training loss: 1.5858 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2200/3570 Training loss: 1.5854 0.1345 sec/batch\n",
      "Validation loss: 1.41316 Saving checkpoint!\n",
      "Epoch 7/10  Iteration 2201/3570 Training loss: 1.5876 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2202/3570 Training loss: 1.5881 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2203/3570 Training loss: 1.5887 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2204/3570 Training loss: 1.5891 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2205/3570 Training loss: 1.5898 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2206/3570 Training loss: 1.5889 0.1410 sec/batch\n",
      "Epoch 7/10  Iteration 2207/3570 Training loss: 1.5895 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2208/3570 Training loss: 1.5898 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2209/3570 Training loss: 1.5890 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2210/3570 Training loss: 1.5891 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2211/3570 Training loss: 1.5898 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2212/3570 Training loss: 1.5900 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2213/3570 Training loss: 1.5910 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2214/3570 Training loss: 1.5910 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2215/3570 Training loss: 1.5909 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2216/3570 Training loss: 1.5909 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2217/3570 Training loss: 1.5910 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2218/3570 Training loss: 1.5914 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2219/3570 Training loss: 1.5926 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2220/3570 Training loss: 1.5925 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2221/3570 Training loss: 1.5923 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2222/3570 Training loss: 1.5918 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2223/3570 Training loss: 1.5914 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2224/3570 Training loss: 1.5913 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2225/3570 Training loss: 1.5917 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2226/3570 Training loss: 1.5916 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2227/3570 Training loss: 1.5914 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2228/3570 Training loss: 1.5910 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2229/3570 Training loss: 1.5918 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2230/3570 Training loss: 1.5918 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2231/3570 Training loss: 1.5919 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2232/3570 Training loss: 1.5923 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2233/3570 Training loss: 1.5922 0.1480 sec/batch\n",
      "Epoch 7/10  Iteration 2234/3570 Training loss: 1.5915 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2235/3570 Training loss: 1.5913 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2236/3570 Training loss: 1.5914 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2237/3570 Training loss: 1.5914 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2238/3570 Training loss: 1.5915 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2239/3570 Training loss: 1.5917 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2240/3570 Training loss: 1.5919 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2241/3570 Training loss: 1.5919 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2242/3570 Training loss: 1.5923 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2243/3570 Training loss: 1.5927 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2244/3570 Training loss: 1.5924 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2245/3570 Training loss: 1.5930 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2246/3570 Training loss: 1.5928 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2247/3570 Training loss: 1.5924 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2248/3570 Training loss: 1.5923 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2249/3570 Training loss: 1.5917 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2250/3570 Training loss: 1.5913 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2251/3570 Training loss: 1.5910 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2252/3570 Training loss: 1.5910 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2253/3570 Training loss: 1.5905 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2254/3570 Training loss: 1.5907 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2255/3570 Training loss: 1.5907 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2256/3570 Training loss: 1.5908 0.1410 sec/batch\n",
      "Epoch 7/10  Iteration 2257/3570 Training loss: 1.5911 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2258/3570 Training loss: 1.5910 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2259/3570 Training loss: 1.5909 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2260/3570 Training loss: 1.5910 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2261/3570 Training loss: 1.5913 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2262/3570 Training loss: 1.5913 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2263/3570 Training loss: 1.5914 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2264/3570 Training loss: 1.5911 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2265/3570 Training loss: 1.5913 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2266/3570 Training loss: 1.5910 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2267/3570 Training loss: 1.5910 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2268/3570 Training loss: 1.5908 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2269/3570 Training loss: 1.5906 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2270/3570 Training loss: 1.5907 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2271/3570 Training loss: 1.5906 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2272/3570 Training loss: 1.5902 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2273/3570 Training loss: 1.5901 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2274/3570 Training loss: 1.5901 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2275/3570 Training loss: 1.5901 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2276/3570 Training loss: 1.5900 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2277/3570 Training loss: 1.5898 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2278/3570 Training loss: 1.5897 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2279/3570 Training loss: 1.5897 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2280/3570 Training loss: 1.5896 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2281/3570 Training loss: 1.5895 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2282/3570 Training loss: 1.5896 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2283/3570 Training loss: 1.5895 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2284/3570 Training loss: 1.5896 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2285/3570 Training loss: 1.5894 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2286/3570 Training loss: 1.5891 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2287/3570 Training loss: 1.5890 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2288/3570 Training loss: 1.5888 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2289/3570 Training loss: 1.5886 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2290/3570 Training loss: 1.5883 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2291/3570 Training loss: 1.5880 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2292/3570 Training loss: 1.5878 0.1460 sec/batch\n",
      "Epoch 7/10  Iteration 2293/3570 Training loss: 1.5876 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2294/3570 Training loss: 1.5873 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2295/3570 Training loss: 1.5870 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2296/3570 Training loss: 1.5870 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2297/3570 Training loss: 1.5871 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2298/3570 Training loss: 1.5871 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2299/3570 Training loss: 1.5870 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2300/3570 Training loss: 1.5869 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2301/3570 Training loss: 1.5871 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2302/3570 Training loss: 1.5872 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2303/3570 Training loss: 1.5868 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2304/3570 Training loss: 1.5870 0.1410 sec/batch\n",
      "Epoch 7/10  Iteration 2305/3570 Training loss: 1.5872 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2306/3570 Training loss: 1.5870 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2307/3570 Training loss: 1.5871 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2308/3570 Training loss: 1.5871 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2309/3570 Training loss: 1.5869 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2310/3570 Training loss: 1.5868 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2311/3570 Training loss: 1.5865 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2312/3570 Training loss: 1.5866 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2313/3570 Training loss: 1.5865 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2314/3570 Training loss: 1.5862 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2315/3570 Training loss: 1.5864 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2316/3570 Training loss: 1.5864 0.1470 sec/batch\n",
      "Epoch 7/10  Iteration 2317/3570 Training loss: 1.5864 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2318/3570 Training loss: 1.5858 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2319/3570 Training loss: 1.5856 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2320/3570 Training loss: 1.5852 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2321/3570 Training loss: 1.5850 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2322/3570 Training loss: 1.5849 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2323/3570 Training loss: 1.5848 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2324/3570 Training loss: 1.5848 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2325/3570 Training loss: 1.5849 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2326/3570 Training loss: 1.5847 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2327/3570 Training loss: 1.5846 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2328/3570 Training loss: 1.5845 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2329/3570 Training loss: 1.5844 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2330/3570 Training loss: 1.5840 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2331/3570 Training loss: 1.5840 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2332/3570 Training loss: 1.5839 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2333/3570 Training loss: 1.5837 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2334/3570 Training loss: 1.5839 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2335/3570 Training loss: 1.5840 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2336/3570 Training loss: 1.5838 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2337/3570 Training loss: 1.5840 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2338/3570 Training loss: 1.5840 0.1420 sec/batch\n",
      "Epoch 7/10  Iteration 2339/3570 Training loss: 1.5840 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2340/3570 Training loss: 1.5841 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2341/3570 Training loss: 1.5841 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2342/3570 Training loss: 1.5842 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2343/3570 Training loss: 1.5843 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2344/3570 Training loss: 1.5844 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2345/3570 Training loss: 1.5840 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2346/3570 Training loss: 1.5841 0.1327 sec/batch\n",
      "Epoch 7/10  Iteration 2347/3570 Training loss: 1.5839 0.1335 sec/batch\n",
      "Epoch 7/10  Iteration 2348/3570 Training loss: 1.5839 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2349/3570 Training loss: 1.5839 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2350/3570 Training loss: 1.5837 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2351/3570 Training loss: 1.5835 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2352/3570 Training loss: 1.5831 0.1410 sec/batch\n",
      "Epoch 7/10  Iteration 2353/3570 Training loss: 1.5832 0.1374 sec/batch\n",
      "Epoch 7/10  Iteration 2354/3570 Training loss: 1.5832 0.1398 sec/batch\n",
      "Epoch 7/10  Iteration 2355/3570 Training loss: 1.5831 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2356/3570 Training loss: 1.5830 0.1611 sec/batch\n",
      "Epoch 7/10  Iteration 2357/3570 Training loss: 1.5829 0.2260 sec/batch\n",
      "Epoch 7/10  Iteration 2358/3570 Training loss: 1.5829 0.1390 sec/batch\n",
      "Epoch 7/10  Iteration 2359/3570 Training loss: 1.5829 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2360/3570 Training loss: 1.5831 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2361/3570 Training loss: 1.5829 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2362/3570 Training loss: 1.5828 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2363/3570 Training loss: 1.5828 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2364/3570 Training loss: 1.5828 0.1405 sec/batch\n",
      "Epoch 7/10  Iteration 2365/3570 Training loss: 1.5830 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2366/3570 Training loss: 1.5830 0.1512 sec/batch\n",
      "Epoch 7/10  Iteration 2367/3570 Training loss: 1.5830 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2368/3570 Training loss: 1.5831 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2369/3570 Training loss: 1.5830 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2370/3570 Training loss: 1.5830 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2371/3570 Training loss: 1.5829 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2372/3570 Training loss: 1.5828 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2373/3570 Training loss: 1.5827 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2374/3570 Training loss: 1.5827 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2375/3570 Training loss: 1.5825 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2376/3570 Training loss: 1.5825 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2377/3570 Training loss: 1.5826 0.1371 sec/batch\n",
      "Epoch 7/10  Iteration 2378/3570 Training loss: 1.5826 0.1680 sec/batch\n",
      "Epoch 7/10  Iteration 2379/3570 Training loss: 1.5826 0.1410 sec/batch\n",
      "Epoch 7/10  Iteration 2380/3570 Training loss: 1.5825 0.1465 sec/batch\n",
      "Epoch 7/10  Iteration 2381/3570 Training loss: 1.5825 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2382/3570 Training loss: 1.5823 0.1515 sec/batch\n",
      "Epoch 7/10  Iteration 2383/3570 Training loss: 1.5823 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2384/3570 Training loss: 1.5825 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2385/3570 Training loss: 1.5825 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2386/3570 Training loss: 1.5826 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2387/3570 Training loss: 1.5828 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2388/3570 Training loss: 1.5829 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2389/3570 Training loss: 1.5828 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2390/3570 Training loss: 1.5830 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2391/3570 Training loss: 1.5831 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2392/3570 Training loss: 1.5831 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2393/3570 Training loss: 1.5830 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2394/3570 Training loss: 1.5832 0.1417 sec/batch\n",
      "Epoch 7/10  Iteration 2395/3570 Training loss: 1.5830 0.1490 sec/batch\n",
      "Epoch 7/10  Iteration 2396/3570 Training loss: 1.5831 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2397/3570 Training loss: 1.5833 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2398/3570 Training loss: 1.5832 0.1337 sec/batch\n",
      "Epoch 7/10  Iteration 2399/3570 Training loss: 1.5833 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2400/3570 Training loss: 1.5833 0.1350 sec/batch\n",
      "Validation loss: 1.40812 Saving checkpoint!\n",
      "Epoch 7/10  Iteration 2401/3570 Training loss: 1.5839 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2402/3570 Training loss: 1.5839 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2403/3570 Training loss: 1.5839 0.1325 sec/batch\n",
      "Epoch 7/10  Iteration 2404/3570 Training loss: 1.5838 0.1515 sec/batch\n",
      "Epoch 7/10  Iteration 2405/3570 Training loss: 1.5839 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2406/3570 Training loss: 1.5840 0.1425 sec/batch\n",
      "Epoch 7/10  Iteration 2407/3570 Training loss: 1.5840 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2408/3570 Training loss: 1.5841 0.1383 sec/batch\n",
      "Epoch 7/10  Iteration 2409/3570 Training loss: 1.5842 0.1349 sec/batch\n",
      "Epoch 7/10  Iteration 2410/3570 Training loss: 1.5841 0.1470 sec/batch\n",
      "Epoch 7/10  Iteration 2411/3570 Training loss: 1.5844 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2412/3570 Training loss: 1.5846 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2413/3570 Training loss: 1.5845 0.1426 sec/batch\n",
      "Epoch 7/10  Iteration 2414/3570 Training loss: 1.5846 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2415/3570 Training loss: 1.5846 0.1410 sec/batch\n",
      "Epoch 7/10  Iteration 2416/3570 Training loss: 1.5847 0.1539 sec/batch\n",
      "Epoch 7/10  Iteration 2417/3570 Training loss: 1.5848 0.1345 sec/batch\n",
      "Epoch 7/10  Iteration 2418/3570 Training loss: 1.5848 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2419/3570 Training loss: 1.5846 0.1349 sec/batch\n",
      "Epoch 7/10  Iteration 2420/3570 Training loss: 1.5845 0.1346 sec/batch\n",
      "Epoch 7/10  Iteration 2421/3570 Training loss: 1.5844 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2422/3570 Training loss: 1.5844 0.1394 sec/batch\n",
      "Epoch 7/10  Iteration 2423/3570 Training loss: 1.5843 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2424/3570 Training loss: 1.5843 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2425/3570 Training loss: 1.5842 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2426/3570 Training loss: 1.5842 0.1498 sec/batch\n",
      "Epoch 7/10  Iteration 2427/3570 Training loss: 1.5839 0.1621 sec/batch\n",
      "Epoch 7/10  Iteration 2428/3570 Training loss: 1.5838 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2429/3570 Training loss: 1.5838 0.1383 sec/batch\n",
      "Epoch 7/10  Iteration 2430/3570 Training loss: 1.5838 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2431/3570 Training loss: 1.5838 0.1355 sec/batch\n",
      "Epoch 7/10  Iteration 2432/3570 Training loss: 1.5835 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2433/3570 Training loss: 1.5834 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2434/3570 Training loss: 1.5832 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2435/3570 Training loss: 1.5833 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2436/3570 Training loss: 1.5832 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2437/3570 Training loss: 1.5833 0.1332 sec/batch\n",
      "Epoch 7/10  Iteration 2438/3570 Training loss: 1.5834 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2439/3570 Training loss: 1.5834 0.1352 sec/batch\n",
      "Epoch 7/10  Iteration 2440/3570 Training loss: 1.5834 0.1342 sec/batch\n",
      "Epoch 7/10  Iteration 2441/3570 Training loss: 1.5834 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2442/3570 Training loss: 1.5834 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2443/3570 Training loss: 1.5833 0.1345 sec/batch\n",
      "Epoch 7/10  Iteration 2444/3570 Training loss: 1.5832 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2445/3570 Training loss: 1.5830 0.1400 sec/batch\n",
      "Epoch 7/10  Iteration 2446/3570 Training loss: 1.5828 0.1367 sec/batch\n",
      "Epoch 7/10  Iteration 2447/3570 Training loss: 1.5827 0.1506 sec/batch\n",
      "Epoch 7/10  Iteration 2448/3570 Training loss: 1.5826 0.1329 sec/batch\n",
      "Epoch 7/10  Iteration 2449/3570 Training loss: 1.5826 0.1368 sec/batch\n",
      "Epoch 7/10  Iteration 2450/3570 Training loss: 1.5825 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2451/3570 Training loss: 1.5826 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2452/3570 Training loss: 1.5827 0.1362 sec/batch\n",
      "Epoch 7/10  Iteration 2453/3570 Training loss: 1.5825 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2454/3570 Training loss: 1.5826 0.1320 sec/batch\n",
      "Epoch 7/10  Iteration 2455/3570 Training loss: 1.5826 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2456/3570 Training loss: 1.5827 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2457/3570 Training loss: 1.5828 0.1494 sec/batch\n",
      "Epoch 7/10  Iteration 2458/3570 Training loss: 1.5829 0.1460 sec/batch\n",
      "Epoch 7/10  Iteration 2459/3570 Training loss: 1.5829 0.1339 sec/batch\n",
      "Epoch 7/10  Iteration 2460/3570 Training loss: 1.5828 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2461/3570 Training loss: 1.5828 0.1351 sec/batch\n",
      "Epoch 7/10  Iteration 2462/3570 Training loss: 1.5828 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2463/3570 Training loss: 1.5827 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2464/3570 Training loss: 1.5827 0.1336 sec/batch\n",
      "Epoch 7/10  Iteration 2465/3570 Training loss: 1.5828 0.1337 sec/batch\n",
      "Epoch 7/10  Iteration 2466/3570 Training loss: 1.5828 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2467/3570 Training loss: 1.5827 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2468/3570 Training loss: 1.5826 0.1442 sec/batch\n",
      "Epoch 7/10  Iteration 2469/3570 Training loss: 1.5826 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2470/3570 Training loss: 1.5826 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2471/3570 Training loss: 1.5826 0.1415 sec/batch\n",
      "Epoch 7/10  Iteration 2472/3570 Training loss: 1.5825 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2473/3570 Training loss: 1.5826 0.1346 sec/batch\n",
      "Epoch 7/10  Iteration 2474/3570 Training loss: 1.5827 0.1490 sec/batch\n",
      "Epoch 7/10  Iteration 2475/3570 Training loss: 1.5827 0.1460 sec/batch\n",
      "Epoch 7/10  Iteration 2476/3570 Training loss: 1.5827 0.1536 sec/batch\n",
      "Epoch 7/10  Iteration 2477/3570 Training loss: 1.5826 0.1329 sec/batch\n",
      "Epoch 7/10  Iteration 2478/3570 Training loss: 1.5824 0.1353 sec/batch\n",
      "Epoch 7/10  Iteration 2479/3570 Training loss: 1.5824 0.1343 sec/batch\n",
      "Epoch 7/10  Iteration 2480/3570 Training loss: 1.5822 0.1411 sec/batch\n",
      "Epoch 7/10  Iteration 2481/3570 Training loss: 1.5821 0.1369 sec/batch\n",
      "Epoch 7/10  Iteration 2482/3570 Training loss: 1.5821 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2483/3570 Training loss: 1.5822 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2484/3570 Training loss: 1.5823 0.1390 sec/batch\n",
      "Epoch 7/10  Iteration 2485/3570 Training loss: 1.5823 0.1340 sec/batch\n",
      "Epoch 7/10  Iteration 2486/3570 Training loss: 1.5824 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2487/3570 Training loss: 1.5823 0.1380 sec/batch\n",
      "Epoch 7/10  Iteration 2488/3570 Training loss: 1.5824 0.1388 sec/batch\n",
      "Epoch 7/10  Iteration 2489/3570 Training loss: 1.5825 0.1359 sec/batch\n",
      "Epoch 7/10  Iteration 2490/3570 Training loss: 1.5825 0.1355 sec/batch\n",
      "Epoch 7/10  Iteration 2491/3570 Training loss: 1.5824 0.1368 sec/batch\n",
      "Epoch 7/10  Iteration 2492/3570 Training loss: 1.5826 0.1360 sec/batch\n",
      "Epoch 7/10  Iteration 2493/3570 Training loss: 1.5826 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2494/3570 Training loss: 1.5827 0.1330 sec/batch\n",
      "Epoch 7/10  Iteration 2495/3570 Training loss: 1.5827 0.1370 sec/batch\n",
      "Epoch 7/10  Iteration 2496/3570 Training loss: 1.5827 0.1347 sec/batch\n",
      "Epoch 7/10  Iteration 2497/3570 Training loss: 1.5828 0.1350 sec/batch\n",
      "Epoch 7/10  Iteration 2498/3570 Training loss: 1.5827 0.1493 sec/batch\n",
      "Epoch 7/10  Iteration 2499/3570 Training loss: 1.5827 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2500/3570 Training loss: 1.6503 0.1328 sec/batch\n",
      "Epoch 8/10  Iteration 2501/3570 Training loss: 1.5975 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2502/3570 Training loss: 1.5903 0.1420 sec/batch\n",
      "Epoch 8/10  Iteration 2503/3570 Training loss: 1.5729 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2504/3570 Training loss: 1.5639 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2505/3570 Training loss: 1.5612 0.1383 sec/batch\n",
      "Epoch 8/10  Iteration 2506/3570 Training loss: 1.5615 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2507/3570 Training loss: 1.5615 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2508/3570 Training loss: 1.5629 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2509/3570 Training loss: 1.5632 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2510/3570 Training loss: 1.5666 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2511/3570 Training loss: 1.5674 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2512/3570 Training loss: 1.5671 0.1351 sec/batch\n",
      "Epoch 8/10  Iteration 2513/3570 Training loss: 1.5643 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2514/3570 Training loss: 1.5638 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2515/3570 Training loss: 1.5625 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2516/3570 Training loss: 1.5632 0.1393 sec/batch\n",
      "Epoch 8/10  Iteration 2517/3570 Training loss: 1.5633 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2518/3570 Training loss: 1.5627 0.1394 sec/batch\n",
      "Epoch 8/10  Iteration 2519/3570 Training loss: 1.5648 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2520/3570 Training loss: 1.5648 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2521/3570 Training loss: 1.5669 0.1369 sec/batch\n",
      "Epoch 8/10  Iteration 2522/3570 Training loss: 1.5660 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2523/3570 Training loss: 1.5649 0.1400 sec/batch\n",
      "Epoch 8/10  Iteration 2524/3570 Training loss: 1.5665 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2525/3570 Training loss: 1.5671 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2526/3570 Training loss: 1.5673 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2527/3570 Training loss: 1.5689 0.1344 sec/batch\n",
      "Epoch 8/10  Iteration 2528/3570 Training loss: 1.5691 0.1372 sec/batch\n",
      "Epoch 8/10  Iteration 2529/3570 Training loss: 1.5700 0.1368 sec/batch\n",
      "Epoch 8/10  Iteration 2530/3570 Training loss: 1.5708 0.1418 sec/batch\n",
      "Epoch 8/10  Iteration 2531/3570 Training loss: 1.5696 0.1369 sec/batch\n",
      "Epoch 8/10  Iteration 2532/3570 Training loss: 1.5691 0.1369 sec/batch\n",
      "Epoch 8/10  Iteration 2533/3570 Training loss: 1.5682 0.1378 sec/batch\n",
      "Epoch 8/10  Iteration 2534/3570 Training loss: 1.5678 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2535/3570 Training loss: 1.5682 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2536/3570 Training loss: 1.5690 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2537/3570 Training loss: 1.5698 0.1337 sec/batch\n",
      "Epoch 8/10  Iteration 2538/3570 Training loss: 1.5707 0.1344 sec/batch\n",
      "Epoch 8/10  Iteration 2539/3570 Training loss: 1.5702 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2540/3570 Training loss: 1.5707 0.1354 sec/batch\n",
      "Epoch 8/10  Iteration 2541/3570 Training loss: 1.5704 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2542/3570 Training loss: 1.5696 0.1335 sec/batch\n",
      "Epoch 8/10  Iteration 2543/3570 Training loss: 1.5693 0.1354 sec/batch\n",
      "Epoch 8/10  Iteration 2544/3570 Training loss: 1.5682 0.1351 sec/batch\n",
      "Epoch 8/10  Iteration 2545/3570 Training loss: 1.5689 0.1343 sec/batch\n",
      "Epoch 8/10  Iteration 2546/3570 Training loss: 1.5692 0.1341 sec/batch\n",
      "Epoch 8/10  Iteration 2547/3570 Training loss: 1.5687 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2548/3570 Training loss: 1.5693 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2549/3570 Training loss: 1.5695 0.1420 sec/batch\n",
      "Epoch 8/10  Iteration 2550/3570 Training loss: 1.5697 0.1357 sec/batch\n",
      "Epoch 8/10  Iteration 2551/3570 Training loss: 1.5693 0.1349 sec/batch\n",
      "Epoch 8/10  Iteration 2552/3570 Training loss: 1.5693 0.1328 sec/batch\n",
      "Epoch 8/10  Iteration 2553/3570 Training loss: 1.5698 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2554/3570 Training loss: 1.5696 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2555/3570 Training loss: 1.5696 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2556/3570 Training loss: 1.5696 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2557/3570 Training loss: 1.5693 0.1329 sec/batch\n",
      "Epoch 8/10  Iteration 2558/3570 Training loss: 1.5692 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2559/3570 Training loss: 1.5695 0.1362 sec/batch\n",
      "Epoch 8/10  Iteration 2560/3570 Training loss: 1.5697 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2561/3570 Training loss: 1.5702 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2562/3570 Training loss: 1.5707 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2563/3570 Training loss: 1.5695 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2564/3570 Training loss: 1.5700 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2565/3570 Training loss: 1.5703 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2566/3570 Training loss: 1.5697 0.1379 sec/batch\n",
      "Epoch 8/10  Iteration 2567/3570 Training loss: 1.5699 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2568/3570 Training loss: 1.5707 0.1351 sec/batch\n",
      "Epoch 8/10  Iteration 2569/3570 Training loss: 1.5709 0.1531 sec/batch\n",
      "Epoch 8/10  Iteration 2570/3570 Training loss: 1.5720 0.1492 sec/batch\n",
      "Epoch 8/10  Iteration 2571/3570 Training loss: 1.5718 0.1408 sec/batch\n",
      "Epoch 8/10  Iteration 2572/3570 Training loss: 1.5716 0.1373 sec/batch\n",
      "Epoch 8/10  Iteration 2573/3570 Training loss: 1.5715 0.1331 sec/batch\n",
      "Epoch 8/10  Iteration 2574/3570 Training loss: 1.5717 0.1409 sec/batch\n",
      "Epoch 8/10  Iteration 2575/3570 Training loss: 1.5720 0.1334 sec/batch\n",
      "Epoch 8/10  Iteration 2576/3570 Training loss: 1.5730 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2577/3570 Training loss: 1.5731 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2578/3570 Training loss: 1.5729 0.1395 sec/batch\n",
      "Epoch 8/10  Iteration 2579/3570 Training loss: 1.5725 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2580/3570 Training loss: 1.5719 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2581/3570 Training loss: 1.5717 0.1458 sec/batch\n",
      "Epoch 8/10  Iteration 2582/3570 Training loss: 1.5721 0.1343 sec/batch\n",
      "Epoch 8/10  Iteration 2583/3570 Training loss: 1.5719 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2584/3570 Training loss: 1.5718 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2585/3570 Training loss: 1.5716 0.1349 sec/batch\n",
      "Epoch 8/10  Iteration 2586/3570 Training loss: 1.5724 0.1365 sec/batch\n",
      "Epoch 8/10  Iteration 2587/3570 Training loss: 1.5723 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2588/3570 Training loss: 1.5723 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2589/3570 Training loss: 1.5727 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2590/3570 Training loss: 1.5726 0.1420 sec/batch\n",
      "Epoch 8/10  Iteration 2591/3570 Training loss: 1.5720 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2592/3570 Training loss: 1.5720 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2593/3570 Training loss: 1.5719 0.1474 sec/batch\n",
      "Epoch 8/10  Iteration 2594/3570 Training loss: 1.5722 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2595/3570 Training loss: 1.5722 0.1328 sec/batch\n",
      "Epoch 8/10  Iteration 2596/3570 Training loss: 1.5725 0.1382 sec/batch\n",
      "Epoch 8/10  Iteration 2597/3570 Training loss: 1.5726 0.1332 sec/batch\n",
      "Epoch 8/10  Iteration 2598/3570 Training loss: 1.5728 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2599/3570 Training loss: 1.5732 0.1347 sec/batch\n",
      "Epoch 8/10  Iteration 2600/3570 Training loss: 1.5736 0.1370 sec/batch\n",
      "Validation loss: 1.4012 Saving checkpoint!\n",
      "Epoch 8/10  Iteration 2601/3570 Training loss: 1.5753 0.1358 sec/batch\n",
      "Epoch 8/10  Iteration 2602/3570 Training loss: 1.5763 0.1351 sec/batch\n",
      "Epoch 8/10  Iteration 2603/3570 Training loss: 1.5762 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2604/3570 Training loss: 1.5759 0.1513 sec/batch\n",
      "Epoch 8/10  Iteration 2605/3570 Training loss: 1.5758 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2606/3570 Training loss: 1.5755 0.1334 sec/batch\n",
      "Epoch 8/10  Iteration 2607/3570 Training loss: 1.5752 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2608/3570 Training loss: 1.5749 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2609/3570 Training loss: 1.5748 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2610/3570 Training loss: 1.5744 0.1341 sec/batch\n",
      "Epoch 8/10  Iteration 2611/3570 Training loss: 1.5745 0.1348 sec/batch\n",
      "Epoch 8/10  Iteration 2612/3570 Training loss: 1.5745 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2613/3570 Training loss: 1.5747 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2614/3570 Training loss: 1.5750 0.1343 sec/batch\n",
      "Epoch 8/10  Iteration 2615/3570 Training loss: 1.5749 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2616/3570 Training loss: 1.5748 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2617/3570 Training loss: 1.5750 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2618/3570 Training loss: 1.5753 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2619/3570 Training loss: 1.5754 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2620/3570 Training loss: 1.5754 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2621/3570 Training loss: 1.5751 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2622/3570 Training loss: 1.5752 0.1470 sec/batch\n",
      "Epoch 8/10  Iteration 2623/3570 Training loss: 1.5749 0.1344 sec/batch\n",
      "Epoch 8/10  Iteration 2624/3570 Training loss: 1.5750 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2625/3570 Training loss: 1.5749 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2626/3570 Training loss: 1.5747 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2627/3570 Training loss: 1.5748 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2628/3570 Training loss: 1.5747 0.1343 sec/batch\n",
      "Epoch 8/10  Iteration 2629/3570 Training loss: 1.5744 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2630/3570 Training loss: 1.5743 0.1384 sec/batch\n",
      "Epoch 8/10  Iteration 2631/3570 Training loss: 1.5742 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2632/3570 Training loss: 1.5743 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2633/3570 Training loss: 1.5742 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2634/3570 Training loss: 1.5741 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2635/3570 Training loss: 1.5740 0.1320 sec/batch\n",
      "Epoch 8/10  Iteration 2636/3570 Training loss: 1.5739 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2637/3570 Training loss: 1.5737 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2638/3570 Training loss: 1.5737 0.1346 sec/batch\n",
      "Epoch 8/10  Iteration 2639/3570 Training loss: 1.5737 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2640/3570 Training loss: 1.5737 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2641/3570 Training loss: 1.5736 0.1460 sec/batch\n",
      "Epoch 8/10  Iteration 2642/3570 Training loss: 1.5733 0.1362 sec/batch\n",
      "Epoch 8/10  Iteration 2643/3570 Training loss: 1.5731 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2644/3570 Training loss: 1.5731 0.1379 sec/batch\n",
      "Epoch 8/10  Iteration 2645/3570 Training loss: 1.5730 0.1379 sec/batch\n",
      "Epoch 8/10  Iteration 2646/3570 Training loss: 1.5727 0.1353 sec/batch\n",
      "Epoch 8/10  Iteration 2647/3570 Training loss: 1.5725 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2648/3570 Training loss: 1.5722 0.1391 sec/batch\n",
      "Epoch 8/10  Iteration 2649/3570 Training loss: 1.5719 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2650/3570 Training loss: 1.5719 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2651/3570 Training loss: 1.5716 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2652/3570 Training loss: 1.5714 0.1410 sec/batch\n",
      "Epoch 8/10  Iteration 2653/3570 Training loss: 1.5714 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2654/3570 Training loss: 1.5715 0.1400 sec/batch\n",
      "Epoch 8/10  Iteration 2655/3570 Training loss: 1.5714 0.1450 sec/batch\n",
      "Epoch 8/10  Iteration 2656/3570 Training loss: 1.5713 0.1543 sec/batch\n",
      "Epoch 8/10  Iteration 2657/3570 Training loss: 1.5713 0.2240 sec/batch\n",
      "Epoch 8/10  Iteration 2658/3570 Training loss: 1.5716 0.1394 sec/batch\n",
      "Epoch 8/10  Iteration 2659/3570 Training loss: 1.5716 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2660/3570 Training loss: 1.5713 0.1410 sec/batch\n",
      "Epoch 8/10  Iteration 2661/3570 Training loss: 1.5715 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2662/3570 Training loss: 1.5716 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2663/3570 Training loss: 1.5715 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2664/3570 Training loss: 1.5716 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2665/3570 Training loss: 1.5717 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2666/3570 Training loss: 1.5715 0.1365 sec/batch\n",
      "Epoch 8/10  Iteration 2667/3570 Training loss: 1.5714 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2668/3570 Training loss: 1.5713 0.1542 sec/batch\n",
      "Epoch 8/10  Iteration 2669/3570 Training loss: 1.5713 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2670/3570 Training loss: 1.5712 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2671/3570 Training loss: 1.5710 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2672/3570 Training loss: 1.5712 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2673/3570 Training loss: 1.5711 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2674/3570 Training loss: 1.5711 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2675/3570 Training loss: 1.5705 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2676/3570 Training loss: 1.5702 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2677/3570 Training loss: 1.5700 0.1410 sec/batch\n",
      "Epoch 8/10  Iteration 2678/3570 Training loss: 1.5697 0.1351 sec/batch\n",
      "Epoch 8/10  Iteration 2679/3570 Training loss: 1.5696 0.1359 sec/batch\n",
      "Epoch 8/10  Iteration 2680/3570 Training loss: 1.5694 0.1385 sec/batch\n",
      "Epoch 8/10  Iteration 2681/3570 Training loss: 1.5694 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2682/3570 Training loss: 1.5694 0.1377 sec/batch\n",
      "Epoch 8/10  Iteration 2683/3570 Training loss: 1.5692 0.1490 sec/batch\n",
      "Epoch 8/10  Iteration 2684/3570 Training loss: 1.5690 0.1387 sec/batch\n",
      "Epoch 8/10  Iteration 2685/3570 Training loss: 1.5689 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2686/3570 Training loss: 1.5688 0.1352 sec/batch\n",
      "Epoch 8/10  Iteration 2687/3570 Training loss: 1.5683 0.1450 sec/batch\n",
      "Epoch 8/10  Iteration 2688/3570 Training loss: 1.5683 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2689/3570 Training loss: 1.5682 0.1400 sec/batch\n",
      "Epoch 8/10  Iteration 2690/3570 Training loss: 1.5680 0.1480 sec/batch\n",
      "Epoch 8/10  Iteration 2691/3570 Training loss: 1.5682 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2692/3570 Training loss: 1.5682 0.1364 sec/batch\n",
      "Epoch 8/10  Iteration 2693/3570 Training loss: 1.5681 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2694/3570 Training loss: 1.5684 0.1441 sec/batch\n",
      "Epoch 8/10  Iteration 2695/3570 Training loss: 1.5683 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2696/3570 Training loss: 1.5683 0.1417 sec/batch\n",
      "Epoch 8/10  Iteration 2697/3570 Training loss: 1.5685 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2698/3570 Training loss: 1.5685 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2699/3570 Training loss: 1.5686 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2700/3570 Training loss: 1.5687 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2701/3570 Training loss: 1.5688 0.1414 sec/batch\n",
      "Epoch 8/10  Iteration 2702/3570 Training loss: 1.5684 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2703/3570 Training loss: 1.5685 0.2257 sec/batch\n",
      "Epoch 8/10  Iteration 2704/3570 Training loss: 1.5683 0.1450 sec/batch\n",
      "Epoch 8/10  Iteration 2705/3570 Training loss: 1.5683 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2706/3570 Training loss: 1.5681 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2707/3570 Training loss: 1.5680 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2708/3570 Training loss: 1.5678 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2709/3570 Training loss: 1.5675 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2710/3570 Training loss: 1.5676 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2711/3570 Training loss: 1.5676 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2712/3570 Training loss: 1.5674 0.1410 sec/batch\n",
      "Epoch 8/10  Iteration 2713/3570 Training loss: 1.5673 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2714/3570 Training loss: 1.5672 0.1397 sec/batch\n",
      "Epoch 8/10  Iteration 2715/3570 Training loss: 1.5671 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2716/3570 Training loss: 1.5672 0.1450 sec/batch\n",
      "Epoch 8/10  Iteration 2717/3570 Training loss: 1.5673 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2718/3570 Training loss: 1.5671 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2719/3570 Training loss: 1.5671 0.1400 sec/batch\n",
      "Epoch 8/10  Iteration 2720/3570 Training loss: 1.5672 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2721/3570 Training loss: 1.5671 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2722/3570 Training loss: 1.5672 0.1451 sec/batch\n",
      "Epoch 8/10  Iteration 2723/3570 Training loss: 1.5674 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2724/3570 Training loss: 1.5674 0.1366 sec/batch\n",
      "Epoch 8/10  Iteration 2725/3570 Training loss: 1.5676 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2726/3570 Training loss: 1.5675 0.1413 sec/batch\n",
      "Epoch 8/10  Iteration 2727/3570 Training loss: 1.5675 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2728/3570 Training loss: 1.5674 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2729/3570 Training loss: 1.5673 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2730/3570 Training loss: 1.5672 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2731/3570 Training loss: 1.5673 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2732/3570 Training loss: 1.5671 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2733/3570 Training loss: 1.5671 0.1380 sec/batch\n",
      "Epoch 8/10  Iteration 2734/3570 Training loss: 1.5673 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2735/3570 Training loss: 1.5673 0.1373 sec/batch\n",
      "Epoch 8/10  Iteration 2736/3570 Training loss: 1.5673 0.1347 sec/batch\n",
      "Epoch 8/10  Iteration 2737/3570 Training loss: 1.5671 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2738/3570 Training loss: 1.5672 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2739/3570 Training loss: 1.5670 0.1400 sec/batch\n",
      "Epoch 8/10  Iteration 2740/3570 Training loss: 1.5671 0.1463 sec/batch\n",
      "Epoch 8/10  Iteration 2741/3570 Training loss: 1.5674 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2742/3570 Training loss: 1.5674 0.1366 sec/batch\n",
      "Epoch 8/10  Iteration 2743/3570 Training loss: 1.5675 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2744/3570 Training loss: 1.5677 0.1430 sec/batch\n",
      "Epoch 8/10  Iteration 2745/3570 Training loss: 1.5678 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2746/3570 Training loss: 1.5678 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2747/3570 Training loss: 1.5679 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2748/3570 Training loss: 1.5680 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2749/3570 Training loss: 1.5680 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2750/3570 Training loss: 1.5679 0.1420 sec/batch\n",
      "Epoch 8/10  Iteration 2751/3570 Training loss: 1.5681 0.1400 sec/batch\n",
      "Epoch 8/10  Iteration 2752/3570 Training loss: 1.5680 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2753/3570 Training loss: 1.5679 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2754/3570 Training loss: 1.5682 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2755/3570 Training loss: 1.5681 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2756/3570 Training loss: 1.5681 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2757/3570 Training loss: 1.5682 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2758/3570 Training loss: 1.5682 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2759/3570 Training loss: 1.5681 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2760/3570 Training loss: 1.5681 0.1401 sec/batch\n",
      "Epoch 8/10  Iteration 2761/3570 Training loss: 1.5680 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2762/3570 Training loss: 1.5680 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2763/3570 Training loss: 1.5681 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2764/3570 Training loss: 1.5680 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2765/3570 Training loss: 1.5681 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2766/3570 Training loss: 1.5681 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2767/3570 Training loss: 1.5680 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2768/3570 Training loss: 1.5682 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2769/3570 Training loss: 1.5685 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2770/3570 Training loss: 1.5685 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2771/3570 Training loss: 1.5685 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2772/3570 Training loss: 1.5686 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2773/3570 Training loss: 1.5687 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2774/3570 Training loss: 1.5688 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2775/3570 Training loss: 1.5687 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2776/3570 Training loss: 1.5685 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2777/3570 Training loss: 1.5685 0.1347 sec/batch\n",
      "Epoch 8/10  Iteration 2778/3570 Training loss: 1.5683 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2779/3570 Training loss: 1.5685 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2780/3570 Training loss: 1.5684 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2781/3570 Training loss: 1.5684 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2782/3570 Training loss: 1.5683 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2783/3570 Training loss: 1.5683 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2784/3570 Training loss: 1.5680 0.1351 sec/batch\n",
      "Epoch 8/10  Iteration 2785/3570 Training loss: 1.5680 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2786/3570 Training loss: 1.5680 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2787/3570 Training loss: 1.5680 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2788/3570 Training loss: 1.5680 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2789/3570 Training loss: 1.5678 0.1370 sec/batch\n",
      "Epoch 8/10  Iteration 2790/3570 Training loss: 1.5676 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2791/3570 Training loss: 1.5675 0.1352 sec/batch\n",
      "Epoch 8/10  Iteration 2792/3570 Training loss: 1.5675 0.1343 sec/batch\n",
      "Epoch 8/10  Iteration 2793/3570 Training loss: 1.5675 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2794/3570 Training loss: 1.5676 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2795/3570 Training loss: 1.5676 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2796/3570 Training loss: 1.5675 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2797/3570 Training loss: 1.5676 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2798/3570 Training loss: 1.5675 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2799/3570 Training loss: 1.5674 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2800/3570 Training loss: 1.5674 0.1330 sec/batch\n",
      "Validation loss: 1.39009 Saving checkpoint!\n",
      "Epoch 8/10  Iteration 2801/3570 Training loss: 1.5679 0.1428 sec/batch\n",
      "Epoch 8/10  Iteration 2802/3570 Training loss: 1.5677 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2803/3570 Training loss: 1.5676 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2804/3570 Training loss: 1.5676 0.1341 sec/batch\n",
      "Epoch 8/10  Iteration 2805/3570 Training loss: 1.5675 0.1337 sec/batch\n",
      "Epoch 8/10  Iteration 2806/3570 Training loss: 1.5675 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2807/3570 Training loss: 1.5675 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2808/3570 Training loss: 1.5675 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2809/3570 Training loss: 1.5676 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2810/3570 Training loss: 1.5675 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2811/3570 Training loss: 1.5675 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2812/3570 Training loss: 1.5675 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2813/3570 Training loss: 1.5676 0.1320 sec/batch\n",
      "Epoch 8/10  Iteration 2814/3570 Training loss: 1.5677 0.1320 sec/batch\n",
      "Epoch 8/10  Iteration 2815/3570 Training loss: 1.5678 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2816/3570 Training loss: 1.5678 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2817/3570 Training loss: 1.5678 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2818/3570 Training loss: 1.5678 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2819/3570 Training loss: 1.5678 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2820/3570 Training loss: 1.5678 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2821/3570 Training loss: 1.5678 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2822/3570 Training loss: 1.5679 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2823/3570 Training loss: 1.5678 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2824/3570 Training loss: 1.5677 0.1337 sec/batch\n",
      "Epoch 8/10  Iteration 2825/3570 Training loss: 1.5676 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2826/3570 Training loss: 1.5675 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2827/3570 Training loss: 1.5676 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2828/3570 Training loss: 1.5676 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2829/3570 Training loss: 1.5676 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2830/3570 Training loss: 1.5676 0.1339 sec/batch\n",
      "Epoch 8/10  Iteration 2831/3570 Training loss: 1.5677 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2832/3570 Training loss: 1.5677 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2833/3570 Training loss: 1.5677 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2834/3570 Training loss: 1.5676 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2835/3570 Training loss: 1.5675 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2836/3570 Training loss: 1.5675 0.1345 sec/batch\n",
      "Epoch 8/10  Iteration 2837/3570 Training loss: 1.5672 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2838/3570 Training loss: 1.5672 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2839/3570 Training loss: 1.5672 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2840/3570 Training loss: 1.5673 0.1327 sec/batch\n",
      "Epoch 8/10  Iteration 2841/3570 Training loss: 1.5673 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2842/3570 Training loss: 1.5674 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2843/3570 Training loss: 1.5674 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2844/3570 Training loss: 1.5674 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2845/3570 Training loss: 1.5674 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2846/3570 Training loss: 1.5675 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2847/3570 Training loss: 1.5675 0.1340 sec/batch\n",
      "Epoch 8/10  Iteration 2848/3570 Training loss: 1.5674 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2849/3570 Training loss: 1.5675 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2850/3570 Training loss: 1.5676 0.1330 sec/batch\n",
      "Epoch 8/10  Iteration 2851/3570 Training loss: 1.5676 0.1434 sec/batch\n",
      "Epoch 8/10  Iteration 2852/3570 Training loss: 1.5677 0.1490 sec/batch\n",
      "Epoch 8/10  Iteration 2853/3570 Training loss: 1.5676 0.1390 sec/batch\n",
      "Epoch 8/10  Iteration 2854/3570 Training loss: 1.5677 0.1360 sec/batch\n",
      "Epoch 8/10  Iteration 2855/3570 Training loss: 1.5677 0.1350 sec/batch\n",
      "Epoch 8/10  Iteration 2856/3570 Training loss: 1.5677 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2857/3570 Training loss: 1.6318 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2858/3570 Training loss: 1.5728 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2859/3570 Training loss: 1.5725 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2860/3570 Training loss: 1.5561 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2861/3570 Training loss: 1.5455 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2862/3570 Training loss: 1.5436 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2863/3570 Training loss: 1.5431 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 2864/3570 Training loss: 1.5424 0.1392 sec/batch\n",
      "Epoch 9/10  Iteration 2865/3570 Training loss: 1.5433 0.1430 sec/batch\n",
      "Epoch 9/10  Iteration 2866/3570 Training loss: 1.5438 0.1373 sec/batch\n",
      "Epoch 9/10  Iteration 2867/3570 Training loss: 1.5471 0.1331 sec/batch\n",
      "Epoch 9/10  Iteration 2868/3570 Training loss: 1.5481 0.1320 sec/batch\n",
      "Epoch 9/10  Iteration 2869/3570 Training loss: 1.5479 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2870/3570 Training loss: 1.5469 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2871/3570 Training loss: 1.5444 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2872/3570 Training loss: 1.5429 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2873/3570 Training loss: 1.5430 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 2874/3570 Training loss: 1.5429 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2875/3570 Training loss: 1.5431 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2876/3570 Training loss: 1.5458 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2877/3570 Training loss: 1.5462 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2878/3570 Training loss: 1.5480 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2879/3570 Training loss: 1.5471 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2880/3570 Training loss: 1.5469 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2881/3570 Training loss: 1.5475 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2882/3570 Training loss: 1.5484 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 2883/3570 Training loss: 1.5487 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2884/3570 Training loss: 1.5507 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2885/3570 Training loss: 1.5505 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2886/3570 Training loss: 1.5516 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2887/3570 Training loss: 1.5526 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2888/3570 Training loss: 1.5517 0.1346 sec/batch\n",
      "Epoch 9/10  Iteration 2889/3570 Training loss: 1.5508 0.1343 sec/batch\n",
      "Epoch 9/10  Iteration 2890/3570 Training loss: 1.5504 0.1355 sec/batch\n",
      "Epoch 9/10  Iteration 2891/3570 Training loss: 1.5502 0.1396 sec/batch\n",
      "Epoch 9/10  Iteration 2892/3570 Training loss: 1.5509 0.1391 sec/batch\n",
      "Epoch 9/10  Iteration 2893/3570 Training loss: 1.5516 0.1625 sec/batch\n",
      "Epoch 9/10  Iteration 2894/3570 Training loss: 1.5525 0.1443 sec/batch\n",
      "Epoch 9/10  Iteration 2895/3570 Training loss: 1.5532 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2896/3570 Training loss: 1.5531 0.1359 sec/batch\n",
      "Epoch 9/10  Iteration 2897/3570 Training loss: 1.5540 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2898/3570 Training loss: 1.5537 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2899/3570 Training loss: 1.5528 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2900/3570 Training loss: 1.5528 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2901/3570 Training loss: 1.5520 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 2902/3570 Training loss: 1.5530 0.1367 sec/batch\n",
      "Epoch 9/10  Iteration 2903/3570 Training loss: 1.5532 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2904/3570 Training loss: 1.5527 0.1329 sec/batch\n",
      "Epoch 9/10  Iteration 2905/3570 Training loss: 1.5535 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2906/3570 Training loss: 1.5537 0.1470 sec/batch\n",
      "Epoch 9/10  Iteration 2907/3570 Training loss: 1.5537 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2908/3570 Training loss: 1.5534 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2909/3570 Training loss: 1.5536 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 2910/3570 Training loss: 1.5544 0.1334 sec/batch\n",
      "Epoch 9/10  Iteration 2911/3570 Training loss: 1.5543 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 2912/3570 Training loss: 1.5545 0.1374 sec/batch\n",
      "Epoch 9/10  Iteration 2913/3570 Training loss: 1.5546 0.1480 sec/batch\n",
      "Epoch 9/10  Iteration 2914/3570 Training loss: 1.5542 0.1402 sec/batch\n",
      "Epoch 9/10  Iteration 2915/3570 Training loss: 1.5541 0.1498 sec/batch\n",
      "Epoch 9/10  Iteration 2916/3570 Training loss: 1.5543 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2917/3570 Training loss: 1.5546 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2918/3570 Training loss: 1.5554 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 2919/3570 Training loss: 1.5559 0.1491 sec/batch\n",
      "Epoch 9/10  Iteration 2920/3570 Training loss: 1.5547 0.1379 sec/batch\n",
      "Epoch 9/10  Iteration 2921/3570 Training loss: 1.5554 0.1419 sec/batch\n",
      "Epoch 9/10  Iteration 2922/3570 Training loss: 1.5557 0.1411 sec/batch\n",
      "Epoch 9/10  Iteration 2923/3570 Training loss: 1.5553 0.1462 sec/batch\n",
      "Epoch 9/10  Iteration 2924/3570 Training loss: 1.5555 0.1357 sec/batch\n",
      "Epoch 9/10  Iteration 2925/3570 Training loss: 1.5561 0.1349 sec/batch\n",
      "Epoch 9/10  Iteration 2926/3570 Training loss: 1.5563 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2927/3570 Training loss: 1.5574 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2928/3570 Training loss: 1.5576 0.1450 sec/batch\n",
      "Epoch 9/10  Iteration 2929/3570 Training loss: 1.5573 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 2930/3570 Training loss: 1.5572 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2931/3570 Training loss: 1.5576 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2932/3570 Training loss: 1.5579 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 2933/3570 Training loss: 1.5591 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2934/3570 Training loss: 1.5592 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 2935/3570 Training loss: 1.5591 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2936/3570 Training loss: 1.5587 0.1440 sec/batch\n",
      "Epoch 9/10  Iteration 2937/3570 Training loss: 1.5582 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2938/3570 Training loss: 1.5584 0.1460 sec/batch\n",
      "Epoch 9/10  Iteration 2939/3570 Training loss: 1.5585 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2940/3570 Training loss: 1.5583 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2941/3570 Training loss: 1.5582 0.1440 sec/batch\n",
      "Epoch 9/10  Iteration 2942/3570 Training loss: 1.5579 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2943/3570 Training loss: 1.5589 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2944/3570 Training loss: 1.5590 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2945/3570 Training loss: 1.5592 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2946/3570 Training loss: 1.5597 0.1540 sec/batch\n",
      "Epoch 9/10  Iteration 2947/3570 Training loss: 1.5596 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2948/3570 Training loss: 1.5590 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2949/3570 Training loss: 1.5589 0.1480 sec/batch\n",
      "Epoch 9/10  Iteration 2950/3570 Training loss: 1.5589 0.1490 sec/batch\n",
      "Epoch 9/10  Iteration 2951/3570 Training loss: 1.5590 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2952/3570 Training loss: 1.5591 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2953/3570 Training loss: 1.5592 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2954/3570 Training loss: 1.5593 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2955/3570 Training loss: 1.5594 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 2956/3570 Training loss: 1.5598 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2957/3570 Training loss: 1.5602 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2958/3570 Training loss: 1.5601 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2959/3570 Training loss: 1.5607 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2960/3570 Training loss: 1.5605 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 2961/3570 Training loss: 1.5603 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2962/3570 Training loss: 1.5603 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2963/3570 Training loss: 1.5599 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2964/3570 Training loss: 1.5595 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2965/3570 Training loss: 1.5593 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 2966/3570 Training loss: 1.5593 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2967/3570 Training loss: 1.5589 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 2968/3570 Training loss: 1.5590 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2969/3570 Training loss: 1.5591 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2970/3570 Training loss: 1.5592 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2971/3570 Training loss: 1.5597 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 2972/3570 Training loss: 1.5595 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2973/3570 Training loss: 1.5592 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2974/3570 Training loss: 1.5595 0.1342 sec/batch\n",
      "Epoch 9/10  Iteration 2975/3570 Training loss: 1.5598 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 2976/3570 Training loss: 1.5599 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2977/3570 Training loss: 1.5599 0.1369 sec/batch\n",
      "Epoch 9/10  Iteration 2978/3570 Training loss: 1.5597 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2979/3570 Training loss: 1.5598 0.1356 sec/batch\n",
      "Epoch 9/10  Iteration 2980/3570 Training loss: 1.5596 0.1365 sec/batch\n",
      "Epoch 9/10  Iteration 2981/3570 Training loss: 1.5595 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2982/3570 Training loss: 1.5595 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2983/3570 Training loss: 1.5593 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 2984/3570 Training loss: 1.5595 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 2985/3570 Training loss: 1.5595 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 2986/3570 Training loss: 1.5593 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2987/3570 Training loss: 1.5592 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 2988/3570 Training loss: 1.5591 0.1530 sec/batch\n",
      "Epoch 9/10  Iteration 2989/3570 Training loss: 1.5591 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 2990/3570 Training loss: 1.5591 0.1353 sec/batch\n",
      "Epoch 9/10  Iteration 2991/3570 Training loss: 1.5590 0.1345 sec/batch\n",
      "Epoch 9/10  Iteration 2992/3570 Training loss: 1.5590 0.1387 sec/batch\n",
      "Epoch 9/10  Iteration 2993/3570 Training loss: 1.5591 0.2923 sec/batch\n",
      "Epoch 9/10  Iteration 2994/3570 Training loss: 1.5590 0.1444 sec/batch\n",
      "Epoch 9/10  Iteration 2995/3570 Training loss: 1.5591 0.1362 sec/batch\n",
      "Epoch 9/10  Iteration 2996/3570 Training loss: 1.5591 0.1336 sec/batch\n",
      "Epoch 9/10  Iteration 2997/3570 Training loss: 1.5591 0.1310 sec/batch\n",
      "Epoch 9/10  Iteration 2998/3570 Training loss: 1.5592 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 2999/3570 Training loss: 1.5590 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3000/3570 Training loss: 1.5588 0.1360 sec/batch\n",
      "Validation loss: 1.38718 Saving checkpoint!\n",
      "Epoch 9/10  Iteration 3001/3570 Training loss: 1.5600 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3002/3570 Training loss: 1.5601 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3003/3570 Training loss: 1.5599 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3004/3570 Training loss: 1.5596 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3005/3570 Training loss: 1.5593 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3006/3570 Training loss: 1.5591 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3007/3570 Training loss: 1.5591 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3008/3570 Training loss: 1.5587 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3009/3570 Training loss: 1.5585 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3010/3570 Training loss: 1.5585 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3011/3570 Training loss: 1.5586 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3012/3570 Training loss: 1.5586 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3013/3570 Training loss: 1.5584 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 3014/3570 Training loss: 1.5584 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3015/3570 Training loss: 1.5586 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 3016/3570 Training loss: 1.5587 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3017/3570 Training loss: 1.5585 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3018/3570 Training loss: 1.5587 0.1362 sec/batch\n",
      "Epoch 9/10  Iteration 3019/3570 Training loss: 1.5588 0.1345 sec/batch\n",
      "Epoch 9/10  Iteration 3020/3570 Training loss: 1.5587 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 3021/3570 Training loss: 1.5587 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3022/3570 Training loss: 1.5588 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3023/3570 Training loss: 1.5587 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3024/3570 Training loss: 1.5585 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3025/3570 Training loss: 1.5583 0.1375 sec/batch\n",
      "Epoch 9/10  Iteration 3026/3570 Training loss: 1.5583 0.1411 sec/batch\n",
      "Epoch 9/10  Iteration 3027/3570 Training loss: 1.5582 0.1355 sec/batch\n",
      "Epoch 9/10  Iteration 3028/3570 Training loss: 1.5580 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3029/3570 Training loss: 1.5580 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3030/3570 Training loss: 1.5580 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3031/3570 Training loss: 1.5581 0.1371 sec/batch\n",
      "Epoch 9/10  Iteration 3032/3570 Training loss: 1.5576 0.1346 sec/batch\n",
      "Epoch 9/10  Iteration 3033/3570 Training loss: 1.5573 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3034/3570 Training loss: 1.5568 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3035/3570 Training loss: 1.5565 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 3036/3570 Training loss: 1.5565 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 3037/3570 Training loss: 1.5563 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3038/3570 Training loss: 1.5564 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3039/3570 Training loss: 1.5564 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3040/3570 Training loss: 1.5563 0.1343 sec/batch\n",
      "Epoch 9/10  Iteration 3041/3570 Training loss: 1.5561 0.1384 sec/batch\n",
      "Epoch 9/10  Iteration 3042/3570 Training loss: 1.5560 0.1369 sec/batch\n",
      "Epoch 9/10  Iteration 3043/3570 Training loss: 1.5559 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3044/3570 Training loss: 1.5555 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3045/3570 Training loss: 1.5556 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3046/3570 Training loss: 1.5554 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3047/3570 Training loss: 1.5553 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3048/3570 Training loss: 1.5555 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3049/3570 Training loss: 1.5555 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3050/3570 Training loss: 1.5554 0.1347 sec/batch\n",
      "Epoch 9/10  Iteration 3051/3570 Training loss: 1.5556 0.1334 sec/batch\n",
      "Epoch 9/10  Iteration 3052/3570 Training loss: 1.5556 0.1349 sec/batch\n",
      "Epoch 9/10  Iteration 3053/3570 Training loss: 1.5556 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3054/3570 Training loss: 1.5557 0.1460 sec/batch\n",
      "Epoch 9/10  Iteration 3055/3570 Training loss: 1.5557 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3056/3570 Training loss: 1.5557 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3057/3570 Training loss: 1.5559 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3058/3570 Training loss: 1.5560 0.1480 sec/batch\n",
      "Epoch 9/10  Iteration 3059/3570 Training loss: 1.5557 0.1337 sec/batch\n",
      "Epoch 9/10  Iteration 3060/3570 Training loss: 1.5558 0.1354 sec/batch\n",
      "Epoch 9/10  Iteration 3061/3570 Training loss: 1.5557 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3062/3570 Training loss: 1.5557 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3063/3570 Training loss: 1.5556 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3064/3570 Training loss: 1.5554 0.1440 sec/batch\n",
      "Epoch 9/10  Iteration 3065/3570 Training loss: 1.5553 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3066/3570 Training loss: 1.5549 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 3067/3570 Training loss: 1.5550 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3068/3570 Training loss: 1.5550 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 3069/3570 Training loss: 1.5549 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3070/3570 Training loss: 1.5548 0.1470 sec/batch\n",
      "Epoch 9/10  Iteration 3071/3570 Training loss: 1.5547 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3072/3570 Training loss: 1.5546 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3073/3570 Training loss: 1.5547 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3074/3570 Training loss: 1.5548 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3075/3570 Training loss: 1.5546 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3076/3570 Training loss: 1.5546 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3077/3570 Training loss: 1.5546 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3078/3570 Training loss: 1.5547 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3079/3570 Training loss: 1.5548 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3080/3570 Training loss: 1.5550 0.1590 sec/batch\n",
      "Epoch 9/10  Iteration 3081/3570 Training loss: 1.5550 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3082/3570 Training loss: 1.5551 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3083/3570 Training loss: 1.5550 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3084/3570 Training loss: 1.5550 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3085/3570 Training loss: 1.5550 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3086/3570 Training loss: 1.5549 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3087/3570 Training loss: 1.5548 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3088/3570 Training loss: 1.5547 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3089/3570 Training loss: 1.5546 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 3090/3570 Training loss: 1.5546 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3091/3570 Training loss: 1.5547 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3092/3570 Training loss: 1.5548 0.1680 sec/batch\n",
      "Epoch 9/10  Iteration 3093/3570 Training loss: 1.5548 0.2380 sec/batch\n",
      "Epoch 9/10  Iteration 3094/3570 Training loss: 1.5546 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3095/3570 Training loss: 1.5547 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 3096/3570 Training loss: 1.5545 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3097/3570 Training loss: 1.5545 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3098/3570 Training loss: 1.5548 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3099/3570 Training loss: 1.5548 0.1368 sec/batch\n",
      "Epoch 9/10  Iteration 3100/3570 Training loss: 1.5548 0.1414 sec/batch\n",
      "Epoch 9/10  Iteration 3101/3570 Training loss: 1.5550 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3102/3570 Training loss: 1.5551 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3103/3570 Training loss: 1.5551 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3104/3570 Training loss: 1.5552 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3105/3570 Training loss: 1.5553 0.1349 sec/batch\n",
      "Epoch 9/10  Iteration 3106/3570 Training loss: 1.5553 0.1347 sec/batch\n",
      "Epoch 9/10  Iteration 3107/3570 Training loss: 1.5553 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3108/3570 Training loss: 1.5555 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3109/3570 Training loss: 1.5554 0.1385 sec/batch\n",
      "Epoch 9/10  Iteration 3110/3570 Training loss: 1.5554 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3111/3570 Training loss: 1.5557 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3112/3570 Training loss: 1.5557 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3113/3570 Training loss: 1.5558 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3114/3570 Training loss: 1.5558 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3115/3570 Training loss: 1.5559 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3116/3570 Training loss: 1.5559 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3117/3570 Training loss: 1.5558 0.1406 sec/batch\n",
      "Epoch 9/10  Iteration 3118/3570 Training loss: 1.5557 0.1355 sec/batch\n",
      "Epoch 9/10  Iteration 3119/3570 Training loss: 1.5559 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3120/3570 Training loss: 1.5560 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3121/3570 Training loss: 1.5560 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3122/3570 Training loss: 1.5561 0.1380 sec/batch\n",
      "Epoch 9/10  Iteration 3123/3570 Training loss: 1.5563 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3124/3570 Training loss: 1.5561 0.1347 sec/batch\n",
      "Epoch 9/10  Iteration 3125/3570 Training loss: 1.5564 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3126/3570 Training loss: 1.5566 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3127/3570 Training loss: 1.5566 0.1430 sec/batch\n",
      "Epoch 9/10  Iteration 3128/3570 Training loss: 1.5567 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3129/3570 Training loss: 1.5568 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3130/3570 Training loss: 1.5569 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3131/3570 Training loss: 1.5570 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3132/3570 Training loss: 1.5570 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3133/3570 Training loss: 1.5568 0.1320 sec/batch\n",
      "Epoch 9/10  Iteration 3134/3570 Training loss: 1.5568 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3135/3570 Training loss: 1.5566 0.1320 sec/batch\n",
      "Epoch 9/10  Iteration 3136/3570 Training loss: 1.5568 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3137/3570 Training loss: 1.5567 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3138/3570 Training loss: 1.5566 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3139/3570 Training loss: 1.5565 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 3140/3570 Training loss: 1.5565 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3141/3570 Training loss: 1.5562 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3142/3570 Training loss: 1.5561 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3143/3570 Training loss: 1.5561 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3144/3570 Training loss: 1.5561 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3145/3570 Training loss: 1.5561 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3146/3570 Training loss: 1.5560 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3147/3570 Training loss: 1.5558 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3148/3570 Training loss: 1.5558 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3149/3570 Training loss: 1.5558 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3150/3570 Training loss: 1.5558 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3151/3570 Training loss: 1.5559 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3152/3570 Training loss: 1.5560 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3153/3570 Training loss: 1.5559 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3154/3570 Training loss: 1.5560 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3155/3570 Training loss: 1.5560 0.1365 sec/batch\n",
      "Epoch 9/10  Iteration 3156/3570 Training loss: 1.5559 0.1337 sec/batch\n",
      "Epoch 9/10  Iteration 3157/3570 Training loss: 1.5557 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3158/3570 Training loss: 1.5557 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3159/3570 Training loss: 1.5555 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3160/3570 Training loss: 1.5554 0.1470 sec/batch\n",
      "Epoch 9/10  Iteration 3161/3570 Training loss: 1.5554 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3162/3570 Training loss: 1.5553 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3163/3570 Training loss: 1.5552 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 3164/3570 Training loss: 1.5552 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3165/3570 Training loss: 1.5552 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3166/3570 Training loss: 1.5553 0.1390 sec/batch\n",
      "Epoch 9/10  Iteration 3167/3570 Training loss: 1.5552 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 3168/3570 Training loss: 1.5551 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3169/3570 Training loss: 1.5552 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3170/3570 Training loss: 1.5553 0.1511 sec/batch\n",
      "Epoch 9/10  Iteration 3171/3570 Training loss: 1.5554 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3172/3570 Training loss: 1.5556 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3173/3570 Training loss: 1.5556 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3174/3570 Training loss: 1.5556 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3175/3570 Training loss: 1.5555 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3176/3570 Training loss: 1.5556 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3177/3570 Training loss: 1.5555 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3178/3570 Training loss: 1.5554 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3179/3570 Training loss: 1.5555 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3180/3570 Training loss: 1.5555 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3181/3570 Training loss: 1.5553 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3182/3570 Training loss: 1.5553 0.1410 sec/batch\n",
      "Epoch 9/10  Iteration 3183/3570 Training loss: 1.5553 0.1347 sec/batch\n",
      "Epoch 9/10  Iteration 3184/3570 Training loss: 1.5553 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3185/3570 Training loss: 1.5553 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3186/3570 Training loss: 1.5552 0.1320 sec/batch\n",
      "Epoch 9/10  Iteration 3187/3570 Training loss: 1.5553 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3188/3570 Training loss: 1.5554 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3189/3570 Training loss: 1.5554 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3190/3570 Training loss: 1.5554 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3191/3570 Training loss: 1.5553 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3192/3570 Training loss: 1.5552 0.1320 sec/batch\n",
      "Epoch 9/10  Iteration 3193/3570 Training loss: 1.5552 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3194/3570 Training loss: 1.5550 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3195/3570 Training loss: 1.5549 0.1330 sec/batch\n",
      "Epoch 9/10  Iteration 3196/3570 Training loss: 1.5550 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3197/3570 Training loss: 1.5550 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3198/3570 Training loss: 1.5551 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3199/3570 Training loss: 1.5551 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3200/3570 Training loss: 1.5552 0.1320 sec/batch\n",
      "Validation loss: 1.38039 Saving checkpoint!\n",
      "Epoch 9/10  Iteration 3201/3570 Training loss: 1.5556 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3202/3570 Training loss: 1.5558 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3203/3570 Training loss: 1.5559 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3204/3570 Training loss: 1.5559 0.1360 sec/batch\n",
      "Epoch 9/10  Iteration 3205/3570 Training loss: 1.5559 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3206/3570 Training loss: 1.5560 0.1400 sec/batch\n",
      "Epoch 9/10  Iteration 3207/3570 Training loss: 1.5561 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3208/3570 Training loss: 1.5561 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3209/3570 Training loss: 1.5562 0.1340 sec/batch\n",
      "Epoch 9/10  Iteration 3210/3570 Training loss: 1.5562 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3211/3570 Training loss: 1.5563 0.1370 sec/batch\n",
      "Epoch 9/10  Iteration 3212/3570 Training loss: 1.5563 0.1350 sec/batch\n",
      "Epoch 9/10  Iteration 3213/3570 Training loss: 1.5563 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3214/3570 Training loss: 1.6278 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3215/3570 Training loss: 1.5686 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3216/3570 Training loss: 1.5668 0.1351 sec/batch\n",
      "Epoch 10/10  Iteration 3217/3570 Training loss: 1.5487 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3218/3570 Training loss: 1.5405 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3219/3570 Training loss: 1.5371 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3220/3570 Training loss: 1.5362 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3221/3570 Training loss: 1.5380 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3222/3570 Training loss: 1.5401 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3223/3570 Training loss: 1.5420 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3224/3570 Training loss: 1.5447 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3225/3570 Training loss: 1.5450 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3226/3570 Training loss: 1.5450 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3227/3570 Training loss: 1.5425 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3228/3570 Training loss: 1.5405 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3229/3570 Training loss: 1.5384 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3230/3570 Training loss: 1.5378 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3231/3570 Training loss: 1.5373 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3232/3570 Training loss: 1.5370 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3233/3570 Training loss: 1.5393 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3234/3570 Training loss: 1.5404 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3235/3570 Training loss: 1.5421 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3236/3570 Training loss: 1.5407 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3237/3570 Training loss: 1.5404 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3238/3570 Training loss: 1.5409 0.1410 sec/batch\n",
      "Epoch 10/10  Iteration 3239/3570 Training loss: 1.5415 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3240/3570 Training loss: 1.5422 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3241/3570 Training loss: 1.5432 0.1392 sec/batch\n",
      "Epoch 10/10  Iteration 3242/3570 Training loss: 1.5434 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3243/3570 Training loss: 1.5441 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3244/3570 Training loss: 1.5455 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3245/3570 Training loss: 1.5440 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3246/3570 Training loss: 1.5433 0.1342 sec/batch\n",
      "Epoch 10/10  Iteration 3247/3570 Training loss: 1.5424 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3248/3570 Training loss: 1.5421 0.1354 sec/batch\n",
      "Epoch 10/10  Iteration 3249/3570 Training loss: 1.5432 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3250/3570 Training loss: 1.5443 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3251/3570 Training loss: 1.5457 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3252/3570 Training loss: 1.5470 0.1410 sec/batch\n",
      "Epoch 10/10  Iteration 3253/3570 Training loss: 1.5467 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3254/3570 Training loss: 1.5477 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3255/3570 Training loss: 1.5470 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3256/3570 Training loss: 1.5461 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3257/3570 Training loss: 1.5459 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3258/3570 Training loss: 1.5451 0.1342 sec/batch\n",
      "Epoch 10/10  Iteration 3259/3570 Training loss: 1.5461 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3260/3570 Training loss: 1.5462 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3261/3570 Training loss: 1.5457 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3262/3570 Training loss: 1.5463 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3263/3570 Training loss: 1.5463 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3264/3570 Training loss: 1.5465 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3265/3570 Training loss: 1.5462 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3266/3570 Training loss: 1.5460 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3267/3570 Training loss: 1.5466 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3268/3570 Training loss: 1.5466 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3269/3570 Training loss: 1.5468 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3270/3570 Training loss: 1.5469 0.1354 sec/batch\n",
      "Epoch 10/10  Iteration 3271/3570 Training loss: 1.5464 0.1320 sec/batch\n",
      "Epoch 10/10  Iteration 3272/3570 Training loss: 1.5462 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3273/3570 Training loss: 1.5466 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3274/3570 Training loss: 1.5469 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3275/3570 Training loss: 1.5474 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3276/3570 Training loss: 1.5480 0.1351 sec/batch\n",
      "Epoch 10/10  Iteration 3277/3570 Training loss: 1.5472 0.1375 sec/batch\n",
      "Epoch 10/10  Iteration 3278/3570 Training loss: 1.5478 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3279/3570 Training loss: 1.5480 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3280/3570 Training loss: 1.5476 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3281/3570 Training loss: 1.5477 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3282/3570 Training loss: 1.5486 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3283/3570 Training loss: 1.5488 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3284/3570 Training loss: 1.5496 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3285/3570 Training loss: 1.5495 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3286/3570 Training loss: 1.5492 0.1613 sec/batch\n",
      "Epoch 10/10  Iteration 3287/3570 Training loss: 1.5490 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3288/3570 Training loss: 1.5491 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3289/3570 Training loss: 1.5496 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3290/3570 Training loss: 1.5508 0.1372 sec/batch\n",
      "Epoch 10/10  Iteration 3291/3570 Training loss: 1.5509 0.1342 sec/batch\n",
      "Epoch 10/10  Iteration 3292/3570 Training loss: 1.5509 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3293/3570 Training loss: 1.5505 0.1361 sec/batch\n",
      "Epoch 10/10  Iteration 3294/3570 Training loss: 1.5499 0.1749 sec/batch\n",
      "Epoch 10/10  Iteration 3295/3570 Training loss: 1.5498 0.1534 sec/batch\n",
      "Epoch 10/10  Iteration 3296/3570 Training loss: 1.5502 0.1363 sec/batch\n",
      "Epoch 10/10  Iteration 3297/3570 Training loss: 1.5503 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3298/3570 Training loss: 1.5503 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3299/3570 Training loss: 1.5500 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3300/3570 Training loss: 1.5506 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3301/3570 Training loss: 1.5505 0.1529 sec/batch\n",
      "Epoch 10/10  Iteration 3302/3570 Training loss: 1.5506 0.3952 sec/batch\n",
      "Epoch 10/10  Iteration 3303/3570 Training loss: 1.5511 0.1382 sec/batch\n",
      "Epoch 10/10  Iteration 3304/3570 Training loss: 1.5509 0.1407 sec/batch\n",
      "Epoch 10/10  Iteration 3305/3570 Training loss: 1.5501 0.1374 sec/batch\n",
      "Epoch 10/10  Iteration 3306/3570 Training loss: 1.5501 0.1392 sec/batch\n",
      "Epoch 10/10  Iteration 3307/3570 Training loss: 1.5500 0.1358 sec/batch\n",
      "Epoch 10/10  Iteration 3308/3570 Training loss: 1.5500 0.2064 sec/batch\n",
      "Epoch 10/10  Iteration 3309/3570 Training loss: 1.5501 0.1466 sec/batch\n",
      "Epoch 10/10  Iteration 3310/3570 Training loss: 1.5503 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3311/3570 Training loss: 1.5502 0.1339 sec/batch\n",
      "Epoch 10/10  Iteration 3312/3570 Training loss: 1.5506 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3313/3570 Training loss: 1.5509 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3314/3570 Training loss: 1.5514 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3315/3570 Training loss: 1.5511 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3316/3570 Training loss: 1.5518 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3317/3570 Training loss: 1.5514 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3318/3570 Training loss: 1.5513 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3319/3570 Training loss: 1.5511 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3320/3570 Training loss: 1.5507 0.1460 sec/batch\n",
      "Epoch 10/10  Iteration 3321/3570 Training loss: 1.5504 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3322/3570 Training loss: 1.5501 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3323/3570 Training loss: 1.5500 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3324/3570 Training loss: 1.5496 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3325/3570 Training loss: 1.5499 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3326/3570 Training loss: 1.5499 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3327/3570 Training loss: 1.5499 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3328/3570 Training loss: 1.5504 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3329/3570 Training loss: 1.5503 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3330/3570 Training loss: 1.5502 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3331/3570 Training loss: 1.5504 0.1320 sec/batch\n",
      "Epoch 10/10  Iteration 3332/3570 Training loss: 1.5507 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3333/3570 Training loss: 1.5508 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3334/3570 Training loss: 1.5508 0.1800 sec/batch\n",
      "Epoch 10/10  Iteration 3335/3570 Training loss: 1.5506 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3336/3570 Training loss: 1.5505 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3337/3570 Training loss: 1.5502 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3338/3570 Training loss: 1.5502 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3339/3570 Training loss: 1.5502 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3340/3570 Training loss: 1.5501 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3341/3570 Training loss: 1.5502 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3342/3570 Training loss: 1.5503 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3343/3570 Training loss: 1.5501 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3344/3570 Training loss: 1.5499 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3345/3570 Training loss: 1.5501 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3346/3570 Training loss: 1.5502 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3347/3570 Training loss: 1.5501 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3348/3570 Training loss: 1.5499 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3349/3570 Training loss: 1.5499 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3350/3570 Training loss: 1.5499 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3351/3570 Training loss: 1.5499 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3352/3570 Training loss: 1.5499 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3353/3570 Training loss: 1.5499 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3354/3570 Training loss: 1.5500 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3355/3570 Training loss: 1.5501 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3356/3570 Training loss: 1.5499 0.1351 sec/batch\n",
      "Epoch 10/10  Iteration 3357/3570 Training loss: 1.5497 0.1348 sec/batch\n",
      "Epoch 10/10  Iteration 3358/3570 Training loss: 1.5498 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3359/3570 Training loss: 1.5495 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3360/3570 Training loss: 1.5492 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3361/3570 Training loss: 1.5489 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3362/3570 Training loss: 1.5485 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3363/3570 Training loss: 1.5482 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3364/3570 Training loss: 1.5481 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3365/3570 Training loss: 1.5478 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3366/3570 Training loss: 1.5475 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3367/3570 Training loss: 1.5477 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3368/3570 Training loss: 1.5478 0.1660 sec/batch\n",
      "Epoch 10/10  Iteration 3369/3570 Training loss: 1.5478 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3370/3570 Training loss: 1.5476 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3371/3570 Training loss: 1.5477 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3372/3570 Training loss: 1.5480 0.1410 sec/batch\n",
      "Epoch 10/10  Iteration 3373/3570 Training loss: 1.5479 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3374/3570 Training loss: 1.5477 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3375/3570 Training loss: 1.5478 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3376/3570 Training loss: 1.5480 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3377/3570 Training loss: 1.5479 0.1320 sec/batch\n",
      "Epoch 10/10  Iteration 3378/3570 Training loss: 1.5480 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3379/3570 Training loss: 1.5481 0.1335 sec/batch\n",
      "Epoch 10/10  Iteration 3380/3570 Training loss: 1.5480 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3381/3570 Training loss: 1.5481 0.1470 sec/batch\n",
      "Epoch 10/10  Iteration 3382/3570 Training loss: 1.5479 0.1481 sec/batch\n",
      "Epoch 10/10  Iteration 3383/3570 Training loss: 1.5479 0.1499 sec/batch\n",
      "Epoch 10/10  Iteration 3384/3570 Training loss: 1.5477 0.1430 sec/batch\n",
      "Epoch 10/10  Iteration 3385/3570 Training loss: 1.5475 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3386/3570 Training loss: 1.5476 0.1470 sec/batch\n",
      "Epoch 10/10  Iteration 3387/3570 Training loss: 1.5475 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3388/3570 Training loss: 1.5476 0.1622 sec/batch\n",
      "Epoch 10/10  Iteration 3389/3570 Training loss: 1.5471 0.1575 sec/batch\n",
      "Epoch 10/10  Iteration 3390/3570 Training loss: 1.5468 0.1818 sec/batch\n",
      "Epoch 10/10  Iteration 3391/3570 Training loss: 1.5465 0.1421 sec/batch\n",
      "Epoch 10/10  Iteration 3392/3570 Training loss: 1.5463 0.1644 sec/batch\n",
      "Epoch 10/10  Iteration 3393/3570 Training loss: 1.5461 0.1509 sec/batch\n",
      "Epoch 10/10  Iteration 3394/3570 Training loss: 1.5461 0.1431 sec/batch\n",
      "Epoch 10/10  Iteration 3395/3570 Training loss: 1.5460 0.1447 sec/batch\n",
      "Epoch 10/10  Iteration 3396/3570 Training loss: 1.5461 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3397/3570 Training loss: 1.5459 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3398/3570 Training loss: 1.5457 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3399/3570 Training loss: 1.5455 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3400/3570 Training loss: 1.5454 0.1390 sec/batch\n",
      "Validation loss: 1.37596 Saving checkpoint!\n",
      "Epoch 10/10  Iteration 3401/3570 Training loss: 1.5460 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3402/3570 Training loss: 1.5462 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3403/3570 Training loss: 1.5461 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3404/3570 Training loss: 1.5460 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3405/3570 Training loss: 1.5462 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3406/3570 Training loss: 1.5462 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3407/3570 Training loss: 1.5460 0.1379 sec/batch\n",
      "Epoch 10/10  Iteration 3408/3570 Training loss: 1.5463 0.1374 sec/batch\n",
      "Epoch 10/10  Iteration 3409/3570 Training loss: 1.5463 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3410/3570 Training loss: 1.5463 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3411/3570 Training loss: 1.5465 0.1420 sec/batch\n",
      "Epoch 10/10  Iteration 3412/3570 Training loss: 1.5464 0.1338 sec/batch\n",
      "Epoch 10/10  Iteration 3413/3570 Training loss: 1.5465 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3414/3570 Training loss: 1.5466 0.1346 sec/batch\n",
      "Epoch 10/10  Iteration 3415/3570 Training loss: 1.5466 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3416/3570 Training loss: 1.5463 0.1377 sec/batch\n",
      "Epoch 10/10  Iteration 3417/3570 Training loss: 1.5465 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3418/3570 Training loss: 1.5463 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3419/3570 Training loss: 1.5462 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3420/3570 Training loss: 1.5461 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3421/3570 Training loss: 1.5459 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3422/3570 Training loss: 1.5458 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3423/3570 Training loss: 1.5455 0.1373 sec/batch\n",
      "Epoch 10/10  Iteration 3424/3570 Training loss: 1.5455 0.1367 sec/batch\n",
      "Epoch 10/10  Iteration 3425/3570 Training loss: 1.5455 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3426/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3427/3570 Training loss: 1.5453 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3428/3570 Training loss: 1.5452 0.1342 sec/batch\n",
      "Epoch 10/10  Iteration 3429/3570 Training loss: 1.5451 0.1355 sec/batch\n",
      "Epoch 10/10  Iteration 3430/3570 Training loss: 1.5451 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3431/3570 Training loss: 1.5453 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3432/3570 Training loss: 1.5450 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3433/3570 Training loss: 1.5450 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3434/3570 Training loss: 1.5451 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3435/3570 Training loss: 1.5451 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3436/3570 Training loss: 1.5452 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3437/3570 Training loss: 1.5453 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3438/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3439/3570 Training loss: 1.5455 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3440/3570 Training loss: 1.5454 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3441/3570 Training loss: 1.5454 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3442/3570 Training loss: 1.5453 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3443/3570 Training loss: 1.5452 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3444/3570 Training loss: 1.5451 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3445/3570 Training loss: 1.5451 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3446/3570 Training loss: 1.5449 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3447/3570 Training loss: 1.5450 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3448/3570 Training loss: 1.5452 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3449/3570 Training loss: 1.5452 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3450/3570 Training loss: 1.5452 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3451/3570 Training loss: 1.5449 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3452/3570 Training loss: 1.5449 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3453/3570 Training loss: 1.5447 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3454/3570 Training loss: 1.5448 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3455/3570 Training loss: 1.5451 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3456/3570 Training loss: 1.5451 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3457/3570 Training loss: 1.5451 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3458/3570 Training loss: 1.5452 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3459/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3460/3570 Training loss: 1.5453 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3461/3570 Training loss: 1.5454 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3462/3570 Training loss: 1.5455 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3463/3570 Training loss: 1.5455 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3464/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3465/3570 Training loss: 1.5456 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3466/3570 Training loss: 1.5455 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3467/3570 Training loss: 1.5455 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3468/3570 Training loss: 1.5457 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3469/3570 Training loss: 1.5456 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3470/3570 Training loss: 1.5457 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3471/3570 Training loss: 1.5458 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3472/3570 Training loss: 1.5458 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3473/3570 Training loss: 1.5458 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3474/3570 Training loss: 1.5457 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3475/3570 Training loss: 1.5456 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3476/3570 Training loss: 1.5456 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3477/3570 Training loss: 1.5458 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3478/3570 Training loss: 1.5457 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3479/3570 Training loss: 1.5458 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3480/3570 Training loss: 1.5458 0.1420 sec/batch\n",
      "Epoch 10/10  Iteration 3481/3570 Training loss: 1.5457 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3482/3570 Training loss: 1.5460 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3483/3570 Training loss: 1.5461 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3484/3570 Training loss: 1.5461 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3485/3570 Training loss: 1.5462 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3486/3570 Training loss: 1.5463 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3487/3570 Training loss: 1.5464 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3488/3570 Training loss: 1.5465 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3489/3570 Training loss: 1.5465 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3490/3570 Training loss: 1.5462 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3491/3570 Training loss: 1.5462 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3492/3570 Training loss: 1.5460 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3493/3570 Training loss: 1.5462 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3494/3570 Training loss: 1.5461 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3495/3570 Training loss: 1.5461 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3496/3570 Training loss: 1.5461 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3497/3570 Training loss: 1.5461 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3498/3570 Training loss: 1.5458 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3499/3570 Training loss: 1.5457 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3500/3570 Training loss: 1.5458 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3501/3570 Training loss: 1.5458 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3502/3570 Training loss: 1.5457 0.1361 sec/batch\n",
      "Epoch 10/10  Iteration 3503/3570 Training loss: 1.5455 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3504/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3505/3570 Training loss: 1.5453 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3506/3570 Training loss: 1.5454 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3507/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3508/3570 Training loss: 1.5454 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3509/3570 Training loss: 1.5455 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3510/3570 Training loss: 1.5455 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3511/3570 Training loss: 1.5455 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3512/3570 Training loss: 1.5455 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3513/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3514/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3515/3570 Training loss: 1.5453 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3516/3570 Training loss: 1.5452 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3517/3570 Training loss: 1.5451 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3518/3570 Training loss: 1.5450 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3519/3570 Training loss: 1.5449 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3520/3570 Training loss: 1.5449 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3521/3570 Training loss: 1.5449 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3522/3570 Training loss: 1.5450 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3523/3570 Training loss: 1.5451 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3524/3570 Training loss: 1.5449 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3525/3570 Training loss: 1.5450 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3526/3570 Training loss: 1.5450 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3527/3570 Training loss: 1.5452 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3528/3570 Training loss: 1.5453 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3529/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3530/3570 Training loss: 1.5454 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3531/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3532/3570 Training loss: 1.5454 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3533/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3534/3570 Training loss: 1.5453 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3535/3570 Training loss: 1.5453 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3536/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3537/3570 Training loss: 1.5453 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3538/3570 Training loss: 1.5452 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3539/3570 Training loss: 1.5452 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3540/3570 Training loss: 1.5451 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3541/3570 Training loss: 1.5452 0.1370 sec/batch\n",
      "Epoch 10/10  Iteration 3542/3570 Training loss: 1.5452 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3543/3570 Training loss: 1.5452 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3544/3570 Training loss: 1.5452 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3545/3570 Training loss: 1.5453 0.1380 sec/batch\n",
      "Epoch 10/10  Iteration 3546/3570 Training loss: 1.5454 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3547/3570 Training loss: 1.5453 0.1410 sec/batch\n",
      "Epoch 10/10  Iteration 3548/3570 Training loss: 1.5452 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3549/3570 Training loss: 1.5451 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3550/3570 Training loss: 1.5451 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3551/3570 Training loss: 1.5449 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3552/3570 Training loss: 1.5449 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3553/3570 Training loss: 1.5449 0.1364 sec/batch\n",
      "Epoch 10/10  Iteration 3554/3570 Training loss: 1.5450 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3555/3570 Training loss: 1.5451 0.1360 sec/batch\n",
      "Epoch 10/10  Iteration 3556/3570 Training loss: 1.5451 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3557/3570 Training loss: 1.5452 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3558/3570 Training loss: 1.5452 0.1400 sec/batch\n",
      "Epoch 10/10  Iteration 3559/3570 Training loss: 1.5453 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3560/3570 Training loss: 1.5454 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3561/3570 Training loss: 1.5454 0.1390 sec/batch\n",
      "Epoch 10/10  Iteration 3562/3570 Training loss: 1.5454 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3563/3570 Training loss: 1.5455 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3564/3570 Training loss: 1.5456 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3565/3570 Training loss: 1.5457 0.1340 sec/batch\n",
      "Epoch 10/10  Iteration 3566/3570 Training loss: 1.5458 0.1334 sec/batch\n",
      "Epoch 10/10  Iteration 3567/3570 Training loss: 1.5458 0.1350 sec/batch\n",
      "Epoch 10/10  Iteration 3568/3570 Training loss: 1.5458 0.1330 sec/batch\n",
      "Epoch 10/10  Iteration 3569/3570 Training loss: 1.5458 0.1365 sec/batch\n",
      "Epoch 10/10  Iteration 3570/3570 Training loss: 1.5458 0.1370 sec/batch\n",
      "Validation loss: 1.37167 Saving checkpoint!\n"
=======
      "Epoch 1/20  Iteration 1/35720 Training loss: 4.4195 2.4569 sec/batch\n",
      "Epoch 1/20  Iteration 2/35720 Training loss: 4.3782 1.9192 sec/batch\n",
      "Epoch 1/20  Iteration 3/35720 Training loss: 4.2096 2.2325 sec/batch\n",
      "Epoch 1/20  Iteration 4/35720 Training loss: 4.6109 2.8204 sec/batch\n",
      "Epoch 1/20  Iteration 5/35720 Training loss: 4.5110 3.2098 sec/batch\n",
      "Epoch 1/20  Iteration 6/35720 Training loss: 4.4169 5.2340 sec/batch\n",
      "Epoch 1/20  Iteration 7/35720 Training loss: 4.3328 3.5796 sec/batch\n",
      "Epoch 1/20  Iteration 8/35720 Training loss: 4.2441 3.1673 sec/batch\n",
      "Epoch 1/20  Iteration 9/35720 Training loss: 4.1678 3.4450 sec/batch\n",
      "Epoch 1/20  Iteration 10/35720 Training loss: 4.0968 3.1539 sec/batch\n",
      "Epoch 1/20  Iteration 11/35720 Training loss: 4.0362 2.9651 sec/batch\n",
      "Epoch 1/20  Iteration 12/35720 Training loss: 3.9853 2.4784 sec/batch\n",
      "Epoch 1/20  Iteration 13/35720 Training loss: 3.9383 2.9154 sec/batch\n",
      "Epoch 1/20  Iteration 14/35720 Training loss: 3.8940 3.3329 sec/batch\n",
      "Epoch 1/20  Iteration 15/35720 Training loss: 3.8559 2.1046 sec/batch\n",
      "Epoch 1/20  Iteration 16/35720 Training loss: 3.8242 2.5068 sec/batch\n",
      "Epoch 1/20  Iteration 17/35720 Training loss: 3.7962 2.6359 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-780c3a10820d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     34\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 35\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mat/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mat/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mat/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/mat/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mat/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 1404ea24a4aaef3c319d8e24b244506f8bdfa10e
     ]
    }
   ],
   "source": [
    "#epochs = 20\n",
    "epochs = 10\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
=======
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4178...  0.1997 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3329...  0.1566 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8995...  0.1491 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 5.5604...  0.1464 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 3.9864...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.9316...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7652...  0.1483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.6055...  0.1458 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.4638...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4104...  0.1450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3789...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.4101...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3676...  0.1455 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3520...  0.1456 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3251...  0.1456 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3071...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.2944...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3086...  0.1456 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2904...  0.1449 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2569...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2755...  0.1473 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2541...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2444...  0.1473 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2370...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2317...  0.1457 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2293...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2449...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2143...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2177...  0.1455 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2169...  0.1459 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2354...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2030...  0.1457 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1910...  0.1453 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2049...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1849...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2021...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1663...  0.1487 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1718...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1665...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1721...  0.1457 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1674...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1716...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1607...  0.1487 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1600...  0.1479 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1484...  0.1484 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1698...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1696...  0.1459 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1753...  0.1453 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1728...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1726...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1542...  0.1473 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1503...  0.1478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1583...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1474...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1539...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1377...  0.1453 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1445...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1474...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1342...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1482...  0.1456 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1496...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1667...  0.1491 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1671...  0.1456 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1206...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1321...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1548...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1449...  0.1471 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.1037...  0.1487 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1202...  0.1471 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1429...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1259...  0.1459 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1506...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.1279...  0.1484 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 3.1300...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 3.1365...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 3.1415...  0.1494 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.1407...  0.1483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.1287...  0.1475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.1208...  0.1478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1043...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1117...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1287...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1310...  0.1458 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1137...  0.1489 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.1069...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1105...  0.1483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.1017...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1082...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1222...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1221...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1241...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.1091...  0.1483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1100...  0.1454 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.1038...  0.1449 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.1042...  0.1493 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1027...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1120...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.1009...  0.1483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.0982...  0.1473 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0939...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.1020...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.0966...  0.1465 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.0910...  0.1486 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.0869...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.0824...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.0828...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.0561...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.0640...  0.1476 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.0704...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0348...  0.1475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0521...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0492...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.1013...  0.1489 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0985...  0.1464 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0517...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0478...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0598...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0755...  0.1472 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0640...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.0424...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.0764...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0519...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0430...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0479...  0.1502 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.0220...  0.1479 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 3.0027...  0.1478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 3.0222...  0.1471 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 3.0282...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 3.0063...  0.1490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 3.0022...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 3.0031...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 2.9730...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 2.9831...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 2.9651...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.9330...  0.1467 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.9422...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.9498...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.9367...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.9526...  0.1460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.9326...  0.1489 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.9392...  0.1494 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.8942...  0.1479 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.9138...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.8872...  0.1482 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.8930...  0.1481 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.8873...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.8791...  0.1458 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.8893...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.8296...  0.1458 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.8458...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.8644...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.8688...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8250...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.8351...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.7969...  0.1482 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.7906...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.7751...  0.1472 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.7880...  0.1471 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.7380...  0.1473 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.7710...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.7494...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.7093...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.6965...  0.1481 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.6994...  0.1476 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.6990...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.6865...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.6751...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.6715...  0.1470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.6659...  0.1495 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.6242...  0.1478 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.6668...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.6969...  0.1475 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.6974...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.6692...  0.1479 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.6715...  0.1477 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.8228...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.6629...  0.1474 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.6093...  0.1465 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.5935...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.5895...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.5993...  0.1461 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.5892...  0.1457 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.5797...  0.1466 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.6051...  0.1472 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.6215...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.5779...  0.1464 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.5535...  0.1462 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.5344...  0.1480 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.5359...  0.1482 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.5342...  0.1529 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.5477...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.5057...  0.1469 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.5321...  0.1464 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.5142...  0.1463 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.5118...  0.1476 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.5068...  0.1483 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.5044...  0.1468 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.5003...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.5762...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4816...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.4804...  0.1532 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.4922...  0.1515 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.4915...  0.1497 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.4868...  0.1496 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.4942...  0.1475 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.4897...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.4990...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.4754...  0.1474 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.4678...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.4808...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.4629...  0.1459 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.5062...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.4719...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.4695...  0.1464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.4652...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.4945...  0.1481 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.4647...  0.1476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4447...  0.1475 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4435...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.4838...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.4544...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4413...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4307...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4408...  0.1464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.4302...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4330...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4417...  0.1476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4344...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4430...  0.1474 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.4160...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.3995...  0.1463 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4220...  0.1460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.3963...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.4158...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.3991...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.3759...  0.1483 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.3921...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.3972...  0.1476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.3939...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.3806...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.3811...  0.1492 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.3770...  0.1464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.3765...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3408...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.4097...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.3760...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.3794...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.3928...  0.1473 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3578...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3850...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.3627...  0.1463 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3608...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3523...  0.1463 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.3748...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3624...  0.1491 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3500...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3531...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.3727...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3493...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3616...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.3717...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3409...  0.1475 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.3355...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3594...  0.1481 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3432...  0.1485 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.3059...  0.1489 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.3199...  0.1481 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3349...  0.1474 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3475...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3349...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3348...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.3071...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.3228...  0.1460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3627...  0.1461 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.3187...  0.1460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.3276...  0.1481 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.3047...  0.1485 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.3062...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.2910...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.3192...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.2870...  0.1460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.2755...  0.1464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2539...  0.1483 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.2847...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.2853...  0.1461 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.2858...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2773...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.3016...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.2675...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.2928...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2626...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.2606...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2570...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.2586...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2850...  0.1484 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2647...  0.1482 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2473...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.2467...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.2771...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2623...  0.1458 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.2385...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.2456...  0.1506 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2445...  0.1464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2462...  0.1463 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.2492...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.2775...  0.1488 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.2664...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2324...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.2441...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2561...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2341...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2189...  0.1474 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2179...  0.1482 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.1948...  0.1474 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2308...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2302...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2449...  0.1484 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2428...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2463...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.2158...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.2160...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2486...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.2213...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.1927...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2302...  0.1473 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.2362...  0.1481 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.2198...  0.1462 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.2106...  0.1493 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.1959...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.1918...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.2301...  0.1474 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.2224...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.2042...  0.1462 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.2123...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.2004...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.2052...  0.1491 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2392...  0.1482 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.1902...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.2131...  0.1494 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.1936...  0.1482 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.1977...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.1831...  0.1481 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.1864...  0.1486 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.2168...  0.1482 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.1973...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.2094...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.1811...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.1705...  0.1499 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.1971...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.2239...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.1995...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.1997...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.1656...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.1727...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1611...  0.1475 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1613...  0.1476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.1416...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.2156...  0.1488 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.1704...  0.1464 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1549...  0.1476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1655...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.1640...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1714...  0.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1625...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.1639...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.1769...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1562...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.1474...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1445...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1702...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.1666...  0.1498 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.1589...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1722...  0.1488 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1763...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.1377...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.1593...  0.1477 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.1229...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.1105...  0.1466 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.1331...  0.1468 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.1511...  0.1471 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.1437...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1655...  0.1476 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.1462...  0.1479 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.1352...  0.1483 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1356...  0.1465 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.1135...  0.1478 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.1199...  0.1480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1290...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1417...  0.1501 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.1003...  0.1472 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1321...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.1266...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.0990...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.1232...  0.1469 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.1153...  0.1467 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.0960...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.1949...  0.1472 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.0923...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.0769...  0.1462 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0929...  0.1472 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.1005...  0.1518 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.0726...  0.1499 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.0991...  0.1505 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.0952...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1287...  0.1484 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.0938...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.0799...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.0800...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.1055...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1335...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.0908...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.0808...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.0789...  0.1489 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1235...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.0959...  0.1475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.0841...  0.1466 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.0713...  0.1468 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1413...  0.1493 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.0856...  0.1496 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0784...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.0839...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0541...  0.1465 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0567...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.0877...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.1118...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.0829...  0.1484 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.0685...  0.1472 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0460...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0692...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.1007...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0443...  0.1466 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0657...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0586...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0171...  0.1462 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.0285...  0.1487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0305...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.0350...  0.1496 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0514...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0339...  0.1487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0312...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0538...  0.1462 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 1.9874...  0.1488 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0569...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0218...  0.1475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.0379...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.0805...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.0173...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.0869...  0.1486 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0334...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0326...  0.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0221...  0.1467 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0444...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0408...  0.1469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0283...  0.1469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0211...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0666...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.0324...  0.1497 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0736...  0.1461 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0644...  0.1472 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0308...  0.1466 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0157...  0.1469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0642...  0.1513 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0352...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 1.9999...  0.1500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.0092...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0161...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0601...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0308...  0.1467 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0298...  0.1469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 2.0016...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.0138...  0.1498 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0410...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.0149...  0.1469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.0167...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 1.9786...  0.1460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 1.9936...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 1.9717...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0201...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 1.9687...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 1.9953...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9668...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 1.9923...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 1.9915...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 1.9846...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9640...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.0094...  0.1467 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 1.9693...  0.1462 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 1.9983...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9606...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 1.9656...  0.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9733...  0.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 1.9816...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 1.9781...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9608...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9637...  0.1475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9308...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 1.9890...  0.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 1.9874...  0.1470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9670...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9761...  0.1470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9570...  0.1493 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 1.9738...  0.1473 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9737...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 1.9906...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 1.9865...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 1.9788...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9649...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9651...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9668...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9612...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9527...  0.1486 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9262...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9580...  0.1470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9567...  0.1488 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 1.9659...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9651...  0.1503 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 1.9696...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9395...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9435...  0.1469 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 1.9838...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9536...  0.1486 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.9126...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9740...  0.1468 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9603...  0.1472 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9483...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9630...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9268...  0.1468 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9328...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9538...  0.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9494...  0.1464 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9440...  0.1494 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9563...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9633...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9568...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 1.9788...  0.1467 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9490...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9820...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.9310...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9552...  0.1472 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9327...  0.1467 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9240...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9468...  0.1468 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9593...  0.1510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9666...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9459...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9273...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9314...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9724...  0.1466 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9410...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9536...  0.1486 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9252...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9243...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9293...  0.1468 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9332...  0.1483 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.9035...  0.1492 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9698...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9563...  0.1503 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9299...  0.1470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9488...  0.1471 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9325...  0.1490 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9195...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9179...  0.1473 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9326...  0.1463 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 1.9763...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9205...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9155...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.9100...  0.1499 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.9058...  0.1465 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9424...  0.1481 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9264...  0.1492 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9198...  0.1482 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.9186...  0.1491 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.8994...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9325...  0.1479 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.8895...  0.1477 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.8801...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.8912...  0.1487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.9110...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.8981...  0.1499 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9337...  0.1475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.9136...  0.1476 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.9001...  0.1475 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.9054...  0.1474 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.8894...  0.1487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.8941...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.8931...  0.1484 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.9144...  0.1485 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.8764...  0.1499 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.9055...  0.1487 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.8790...  0.1480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8684...  0.1488 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.9002...  0.1489 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.8967...  0.1478 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.8792...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 1.9743...  0.1493 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.8896...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8837...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.8908...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.8812...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8537...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.8817...  0.1533 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8822...  0.1516 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9265...  0.1501 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.8877...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8672...  0.1470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8664...  0.1465 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.8811...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9223...  0.1469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8733...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8713...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.8748...  0.1491 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9166...  0.1469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.8814...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.8868...  0.1486 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8606...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9209...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8669...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8706...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8705...  0.1495 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8383...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8368...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.8756...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.9096...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.8773...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8686...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8463...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8775...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.8859...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8460...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8527...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8519...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8239...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8222...  0.1471 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8243...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8318...  0.1493 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8696...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8295...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8222...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8629...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.8067...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8517...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.8397...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8408...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.8875...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8140...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.8956...  0.1489 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8388...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8534...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8379...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8500...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8610...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8231...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8197...  0.1488 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.8762...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8418...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.8716...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8766...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8610...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8290...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8621...  0.1471 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8565...  0.1471 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8162...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8185...  0.1486 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8276...  0.1500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8647...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8498...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8521...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8185...  0.1499 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8250...  0.1492 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8539...  0.1471 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8382...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8356...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.7962...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8159...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.7938...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8446...  0.1493 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.7890...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8186...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.7909...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.8021...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.8090...  0.1472 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.7930...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.7771...  0.1508 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8285...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.7977...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.7990...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.7907...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.7878...  0.1463 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.7900...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8187...  0.1510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.7929...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.7618...  0.1514 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.7861...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7423...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8155...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.7905...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.7789...  0.1472 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.8029...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.7922...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7954...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.7881...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.8110...  0.1499 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8109...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8253...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.8110...  0.1471 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.7860...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.8007...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.7990...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.7808...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7623...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.8037...  0.1489 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.7779...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.7950...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.7885...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.8035...  0.1469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7583...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7588...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.8163...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.7843...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7439...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8087...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.8012...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.7754...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.7735...  0.1472 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7553...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7612...  0.1486 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.7982...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.7953...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.7914...  0.1469 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.7900...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.8080...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.7928...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8123...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.7763...  0.1488 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8230...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7785...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.7806...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.7857...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7599...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.7882...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.7915...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.8210...  0.1481 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.7903...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.7640...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7640...  0.1471 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.7963...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.7864...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.7806...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.7802...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7710...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.7849...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.7692...  0.1472 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7451...  0.1480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.7969...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.8062...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7713...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.7861...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.7769...  0.1472 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7666...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7673...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.7732...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8315...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7603...  0.1496 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7591...  0.1486 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7641...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7484...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.7837...  0.1491 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7647...  0.1477 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7792...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7668...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7445...  0.1484 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.7723...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7407...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7289...  0.1489 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7423...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7476...  0.1479 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7525...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.7745...  0.1478 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7545...  0.1473 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7356...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7650...  0.1485 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7380...  0.1487 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7503...  0.1490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7545...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7510...  0.1494 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7276...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7536...  0.1474 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7147...  0.1482 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7181...  0.1483 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7484...  0.1475 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7388...  0.1476 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7299...  0.1482 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8349...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7431...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7264...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7350...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7302...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.6949...  0.1485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7425...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7221...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7630...  0.1506 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7289...  0.1524 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7209...  0.1507 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7178...  0.1491 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7380...  0.1469 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.7701...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7275...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7092...  0.1467 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7426...  0.1468 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7717...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7374...  0.1502 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7544...  0.1488 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7188...  0.1501 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7587...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7363...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7375...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7333...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.7005...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.6989...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7422...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7633...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7395...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7235...  0.1485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.6886...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7383...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7518...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7159...  0.1469 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7137...  0.1487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7049...  0.1491 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.6854...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.6822...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.6900...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.6920...  0.1498 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7310...  0.1487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.6932...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.6810...  0.1466 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7253...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.6663...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7090...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.6952...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.6926...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7487...  0.1503 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.6862...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7745...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7070...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7145...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.7070...  0.1510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7155...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7268...  0.1493 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6970...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.6880...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7485...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.6978...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7593...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7490...  0.1482 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7328...  0.1477 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.7075...  0.1485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7204...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7185...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.6998...  0.1487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.7002...  0.1487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.6964...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7392...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7227...  0.1494 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7394...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.6878...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.7105...  0.1494 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7335...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.7020...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.7090...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6510...  0.1470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.6916...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6595...  0.1487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7046...  0.1470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6614...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.6956...  0.1482 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6664...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.6768...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6670...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.6824...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6486...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.7052...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6641...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.6731...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6655...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6667...  0.1467 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.6693...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.6929...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.6827...  0.1469 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6399...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6689...  0.1462 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6416...  0.1477 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.6859...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.6714...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6769...  0.1488 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.6658...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6714...  0.1466 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.6695...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.6820...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6909...  0.1489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.6917...  0.1492 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.6888...  0.1516 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6703...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.6758...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6654...  0.1486 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6591...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6574...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6331...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.6867...  0.1505 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6664...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6735...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6667...  0.1467 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.6842...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6376...  0.1489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6422...  0.1484 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.6864...  0.1497 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6674...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6221...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.6918...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.6808...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6550...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6593...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6346...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6391...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.6924...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.6765...  0.1486 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.6799...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6710...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.7040...  0.1485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.6761...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.6987...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6606...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7163...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6633...  0.1491 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6625...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.6795...  0.1488 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6489...  0.1485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.6780...  0.1472 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.6688...  0.1465 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.6957...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.6739...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6495...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6379...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.6689...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6606...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6564...  0.1492 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6598...  0.1489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6597...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6781...  0.1476 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6616...  0.1501 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6135...  0.1481 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.6925...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.6928...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6603...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.6693...  0.1489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6659...  0.1489 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6568...  0.1479 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6528...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.6655...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7183...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6603...  0.1477 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6396...  0.1498 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6405...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6485...  0.1483 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.6741...  0.1502 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6658...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6783...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6502...  0.1486 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6366...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.6659...  0.1469 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6236...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6262...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6219...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6428...  0.1506 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6469...  0.1463 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6528...  0.1478 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6516...  0.1493 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6326...  0.1487 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6525...  0.1475 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6316...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6294...  0.1505 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6409...  0.1471 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6362...  0.1473 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6309...  0.1467 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6447...  0.1474 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6169...  0.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6071...  0.1490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6445...  0.1480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6245...  0.1485 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6190...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7301...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6469...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6251...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6331...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6212...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.6004...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6369...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6241...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6539...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6327...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.6149...  0.1516 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6107...  0.1513 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6318...  0.1499 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6660...  0.1491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6310...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6100...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6351...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6509...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6367...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6554...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6193...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6566...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6212...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6382...  0.1495 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6304...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.5876...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.5969...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6384...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6513...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6404...  0.1467 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6202...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.6071...  0.1486 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6465...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6346...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6208...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6288...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.5965...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.5798...  0.1465 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.5732...  0.1495 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.5996...  0.1511 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.6001...  0.1498 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6437...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.5960...  0.1477 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.5838...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6264...  0.1491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.5811...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6036...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.5956...  0.1469 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.5994...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6522...  0.1502 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.5910...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6647...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6237...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6162...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6090...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6218...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6396...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.6017...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.5923...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6498...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6188...  0.1469 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6725...  0.1486 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6505...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6313...  0.1477 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6069...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6270...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6368...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.5919...  0.1505 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6085...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6090...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6707...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6296...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6444...  0.1488 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.6027...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6039...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6267...  0.1467 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.5987...  0.1487 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6124...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5631...  0.1498 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.6003...  0.1485 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5671...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6155...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5655...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.6073...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.5747...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.5893...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5750...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.5749...  0.1485 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5648...  0.1494 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6089...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5726...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.5805...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5735...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.5719...  0.1488 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.5729...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.6022...  0.1487 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.5865...  0.1467 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5587...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.5723...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5482...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.5978...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.5888...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.5796...  0.1477 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.5794...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.5769...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.5800...  0.1472 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.5794...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.5974...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.5886...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6141...  0.1467 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.5714...  0.1489 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.5837...  0.1484 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.5897...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.5808...  0.1491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.5662...  0.1485 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.5490...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.6017...  0.1477 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.5915...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.5868...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.5805...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.5930...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.5479...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.5420...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.6097...  0.1488 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.5836...  0.1485 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5426...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.6001...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.5923...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.5683...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.5608...  0.1493 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5434...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.5512...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.6024...  0.1474 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.5921...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.5976...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.5885...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.6142...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.5850...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.5964...  0.1488 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.5824...  0.1497 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6382...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.5792...  0.1514 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.5739...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6074...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.5682...  0.1477 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.6087...  0.1481 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.5919...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6168...  0.1489 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.6072...  0.1511 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.5777...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.5429...  0.1492 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.5788...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.5989...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.5864...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.5788...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.5771...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.5857...  0.1492 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.5736...  0.1498 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5387...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.6008...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6185...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.5883...  0.1477 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.5834...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.5609...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.5792...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.5727...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.5880...  0.1475 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6455...  0.1491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.5744...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.5585...  0.1470 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.5676...  0.1466 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.5438...  0.1476 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.6077...  0.1483 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.5783...  0.1468 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.5853...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.5450...  0.1495 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.5579...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.5849...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.5357...  0.1480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5413...  0.1479 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5387...  0.1471 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.5522...  0.1469 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.5594...  0.1497 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.5676...  0.1491 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.5700...  0.1510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.5562...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.5823...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.5584...  0.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.5600...  0.1473 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.5737...  0.1465 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.5550...  0.1469 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.5433...  0.1500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.5631...  0.1485 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5392...  0.1484 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5336...  0.1482 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.5680...  0.1478 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.5493...  0.1492 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5386...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.6650...  0.1465 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.5669...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.5523...  0.1495 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.5627...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5409...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5218...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.5619...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5515...  0.1485 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.5735...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.5628...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5363...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5407...  0.1508 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.5549...  0.1497 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.5920...  0.1502 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5409...  0.1493 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5255...  0.1499 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.5606...  0.1492 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.5955...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.5632...  0.1488 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.5796...  0.1485 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5454...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.5719...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.5396...  0.1468 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.5595...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.5684...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.5218...  0.1493 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5182...  0.1474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.5745...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.5699...  0.1474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.5659...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.5428...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5260...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.5675...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.5578...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.5415...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.5477...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5296...  0.1474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.5158...  0.1498 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.5022...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5292...  0.1485 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5218...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.5708...  0.1467 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5281...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5108...  0.1472 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.5524...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.5029...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5344...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5218...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5288...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.5726...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5144...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.5976...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5414...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.5451...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5369...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.5541...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.5610...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5257...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5266...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.5748...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.5469...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5968...  0.1505 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.5654...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.5593...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5270...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.5505...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.5593...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5163...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5359...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5379...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.5838...  0.1474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.5560...  0.1511 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.5743...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5271...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5316...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.5668...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5406...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5301...  0.1496 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.5015...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5312...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.4920...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5454...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.5045...  0.1488 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5325...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.5194...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.5227...  0.1465 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.5021...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.5128...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.4972...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5447...  0.1511 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.5152...  0.1466 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.5219...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.5094...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.5137...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.5076...  0.1472 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5367...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5312...  0.1491 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.4939...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.5087...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.4876...  0.1468 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5339...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5110...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5200...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5123...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5123...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5284...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5199...  0.1508 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5206...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5202...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5422...  0.1485 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.5138...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5068...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5189...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.5143...  0.1476 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.4940...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.4803...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5175...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5203...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5077...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.5063...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5177...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.4801...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.4667...  0.1467 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5250...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5106...  0.1470 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.4663...  0.1512 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5281...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5275...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.5018...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.4785...  0.1488 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.4656...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.4922...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5304...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5205...  0.1492 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5215...  0.1494 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5036...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5312...  0.1488 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5311...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5231...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.5037...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.5617...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5117...  0.1502 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.4989...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.5377...  0.1470 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.4840...  0.1482 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5291...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5110...  0.1485 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.5575...  0.1474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5244...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.4957...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.4667...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.4982...  0.1494 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5152...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.5005...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.4923...  0.1479 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.4952...  0.1490 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5177...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.5055...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.4697...  0.1468 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5205...  0.1487 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5358...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.5034...  0.1474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.5108...  0.1461 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.5022...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.5031...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.5041...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5202...  0.1478 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.5656...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.4969...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.5032...  0.1489 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.5005...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.4797...  0.1481 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5299...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5080...  0.1487 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5224...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.4741...  0.1484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.4768...  0.1488 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5216...  0.1493 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.4942...  0.1475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.4695...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.4735...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.4970...  0.1486 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.4910...  0.1488 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.4895...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.4838...  0.1477 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.4748...  0.1473 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5100...  0.1476 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.4765...  0.1501 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.4972...  0.1465 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.4914...  0.1483 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.4813...  0.1469 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.4773...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.4998...  0.1480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.4678...  0.1471 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.4633...  0.1470 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.4959...  0.1468 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.4798...  0.1492 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.4815...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.6025...  0.1469 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.5013...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.4855...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5023...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.4702...  0.1475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.4559...  0.1472 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.4965...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.4712...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.4910...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.4757...  0.1486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.4640...  0.1486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.4706...  0.1494 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.4720...  0.1470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5060...  0.1495 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.4823...  0.1520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.4578...  0.1531 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.4952...  0.1496 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5114...  0.1473 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.4944...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5147...  0.1488 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.4711...  0.1491 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.5059...  0.1485 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.4710...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.4907...  0.1470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.4828...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4354...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.4407...  0.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.5031...  0.1488 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.4934...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.5071...  0.1491 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.4729...  0.1470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.4564...  0.1501 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.4960...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.4887...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.4690...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.4859...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.4673...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4423...  0.1467 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4364...  0.1525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.4668...  0.1472 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.4711...  0.1514 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5294...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.4695...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4562...  0.1488 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.4889...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4585...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.4687...  0.1468 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.4687...  0.1505 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.4788...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.5110...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.4584...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5312...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.4908...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.4872...  0.1467 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.4730...  0.1472 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.4822...  0.1489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.4945...  0.1486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.4577...  0.1489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.4494...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5105...  0.1495 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.4768...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5268...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.4973...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4740...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.4698...  0.1498 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.4819...  0.1492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.4786...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.4444...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.4707...  0.1498 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.4612...  0.1485 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5128...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.5021...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5083...  0.1474 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.4565...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.4634...  0.1501 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.4889...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.4616...  0.1489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.4687...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4224...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.4718...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4203...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.4675...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4339...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.4685...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.4344...  0.1497 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.4615...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.4412...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.4489...  0.1485 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4357...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.4801...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4447...  0.1475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.4604...  0.1488 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4453...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4503...  0.1497 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.4370...  0.1475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.4708...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.4674...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4244...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4450...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4236...  0.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.4614...  0.1504 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.4514...  0.1475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.4581...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.4486...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.4479...  0.1475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.4628...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.4637...  0.1493 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.4562...  0.1492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.4518...  0.1494 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.4789...  0.1499 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.4433...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4416...  0.1492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.4565...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.4366...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4277...  0.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4171...  0.1482 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.4567...  0.1475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.4613...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.4481...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4450...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4602...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4248...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4100...  0.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.4655...  0.1472 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4543...  0.1474 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4094...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.4771...  0.1473 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.4655...  0.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4320...  0.1492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4208...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.3964...  0.1494 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4309...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.4724...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.4544...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.4582...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4567...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.4857...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.4646...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.4599...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4548...  0.1492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5023...  0.1495 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.4565...  0.1492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.4400...  0.1491 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.4808...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4341...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.4782...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.4614...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.4829...  0.1476 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.4633...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4362...  0.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4084...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4447...  0.1497 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4645...  0.1486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4464...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4412...  0.1473 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4433...  0.1479 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.4523...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.4465...  0.1489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4158...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.4729...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.4851...  0.1474 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.4465...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.4545...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.4432...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.4484...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.4377...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.4738...  0.1472 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5184...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.4388...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4355...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4330...  0.1481 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4265...  0.1477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.4655...  0.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.4590...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.4713...  0.1486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4225...  0.1538 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4286...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.4678...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4265...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4217...  0.1494 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4142...  0.1488 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4376...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.4488...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4267...  0.1491 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4385...  0.1486 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4331...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.4575...  0.1498 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4296...  0.1489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4339...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.4403...  0.1482 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4232...  0.1480 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4246...  0.1478 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4438...  0.1498 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4271...  0.1483 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4145...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4551...  0.1487 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4372...  0.1484 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4298...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.5735...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4644...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4424...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4651...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4306...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4114...  0.1486 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4409...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4278...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4521...  0.1504 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4406...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4265...  0.1492 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4219...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4434...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.4670...  0.1486 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4177...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4238...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4458...  0.1511 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.4603...  0.1530 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4514...  0.1505 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.4623...  0.1495 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4246...  0.1469 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.4517...  0.1468 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4298...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4485...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4318...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.3958...  0.1473 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.4041...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4450...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4563...  0.1471 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4479...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4315...  0.1472 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.4058...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4532...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4452...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4157...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4337...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.4115...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.3853...  0.1491 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.3856...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4130...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4072...  0.1500 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.4570...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4112...  0.1486 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4006...  0.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4325...  0.1486 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.3905...  0.1508 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4246...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4236...  0.1504 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4129...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4502...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4010...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.4718...  0.1472 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4348...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4317...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4183...  0.1499 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4321...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4586...  0.1520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4132...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4074...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.4580...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4323...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.4789...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4481...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4374...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4193...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4365...  0.1497 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4383...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4119...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4244...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4079...  0.1471 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.4746...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4377...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.4577...  0.1496 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4071...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4307...  0.1504 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4403...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4142...  0.1472 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4158...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.3876...  0.1471 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4339...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.3844...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4191...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.3903...  0.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4182...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.3964...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4273...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.3971...  0.1472 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4114...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.3952...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4249...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.4023...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4086...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.4031...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.3943...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.3934...  0.1491 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4335...  0.1491 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4261...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.3898...  0.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.3953...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.3759...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4200...  0.1486 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4110...  0.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4077...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4008...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4110...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4139...  0.1473 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4091...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4187...  0.1531 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.4113...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4218...  0.1497 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.4013...  0.1501 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4103...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4252...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.4028...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.3991...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3699...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4250...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4252...  0.1487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4031...  0.1511 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.4012...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4125...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.3777...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.3695...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4179...  0.1472 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.3994...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.3607...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4256...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4134...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.3955...  0.1493 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.3746...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.3539...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.3772...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4263...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4322...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4213...  0.1473 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4056...  0.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4333...  0.1480 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4313...  0.1495 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4164...  0.1476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4166...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.4717...  0.1502 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4179...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4053...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4378...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.3907...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4250...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4232...  0.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4489...  0.1501 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4345...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4014...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.3797...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.3961...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4186...  0.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4082...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4015...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4057...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4143...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.3965...  0.1483 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.3792...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4241...  0.1478 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4296...  0.1505 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4091...  0.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4142...  0.1477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.4026...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4086...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4138...  0.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4321...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.4752...  0.1503 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4140...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.4051...  0.1479 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.4020...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.3883...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4377...  0.1490 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4136...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4138...  0.1496 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.3785...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.3918...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4294...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.3848...  0.1474 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.3679...  0.1464 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.3882...  0.1482 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.3868...  0.1499 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4079...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.3904...  0.1498 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.3917...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.3828...  0.1485 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4304...  0.1487 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.3919...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.3972...  0.1466 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.3870...  0.1484 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.3804...  0.1497 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.3859...  0.1475 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.3977...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.3695...  0.1488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.3609...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.4038...  0.1489 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.3991...  0.1481 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.3841...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5497...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4488...  0.1499 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4285...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4219...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.3915...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.3795...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.4252...  0.1481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.3952...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4120...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.4069...  0.1485 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.3868...  0.1502 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.3997...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.4009...  0.1503 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4268...  0.1506 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.3976...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.3845...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4198...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4345...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.4109...  0.1524 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4296...  0.1527 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.3944...  0.1502 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4174...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.3929...  0.1513 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4130...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4055...  0.1487 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3635...  0.1492 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.3728...  0.1492 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4178...  0.1474 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.4255...  0.1481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4180...  0.1489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.3897...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.3694...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4076...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4073...  0.1486 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.3842...  0.1469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.3963...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.3774...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3601...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3419...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.3734...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.3786...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4339...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.3799...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.3671...  0.1468 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4033...  0.1462 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.3648...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.3868...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.3925...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.3905...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4130...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.3642...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4290...  0.1485 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.3827...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.3989...  0.1485 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.3774...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.3928...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4162...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.3779...  0.1472 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.3730...  0.1474 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4296...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.4058...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4374...  0.1507 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4218...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4012...  0.1473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.3836...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.4050...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.3970...  0.1473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3729...  0.1468 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.3899...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.3751...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4388...  0.1496 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4133...  0.1496 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4292...  0.1474 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.3764...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.3909...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4164...  0.1473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.3830...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.3702...  0.1467 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3512...  0.1468 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.3993...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3440...  0.1487 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.3977...  0.1481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.3568...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.3824...  0.1471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.3644...  0.1509 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.3863...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.3635...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.3648...  0.1472 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3516...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.3994...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.3732...  0.1474 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.3711...  0.1489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.3530...  0.1471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.3641...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.3751...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.3883...  0.1469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.3862...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3526...  0.1467 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.3534...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.3526...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.3828...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.3683...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.3832...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.3796...  0.1466 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.3787...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.3796...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.3824...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.3802...  0.1494 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.3646...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.3964...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.3667...  0.1473 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.3792...  0.1491 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.3937...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.3648...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3502...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3453...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.3728...  0.1472 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3845...  0.1471 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.3700...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.3781...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.3792...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3491...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3428...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.3857...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.3753...  0.1516 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3374...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.3931...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.3878...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.3640...  0.1487 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3451...  0.1472 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3251...  0.1468 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3578...  0.1471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.3925...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.3876...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.4031...  0.1470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.3721...  0.1478 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.4144...  0.1487 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.3886...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.3916...  0.1472 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.3904...  0.1470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4424...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.3919...  0.1500 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.3779...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4128...  0.1489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.3632...  0.1486 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.3990...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.3900...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4121...  0.1470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.4086...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.3637...  0.1485 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3442...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.3599...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.3949...  0.1487 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.3820...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.3690...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.3770...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.3907...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.3756...  0.1468 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3441...  0.1482 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.4001...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.4002...  0.1493 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.3814...  0.1506 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.3746...  0.1469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.3716...  0.1469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.3776...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.3738...  0.1477 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.4020...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4389...  0.1481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.3780...  0.1490 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.3678...  0.1501 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.3730...  0.1475 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3574...  0.1471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4040...  0.1470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.3780...  0.1489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.3994...  0.1495 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3566...  0.1501 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.3611...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.3984...  0.1469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3519...  0.1481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3450...  0.1506 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3515...  0.1472 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3728...  0.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.3673...  0.1489 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.3674...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.3667...  0.1469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3562...  0.1493 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.4006...  0.1483 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.3580...  0.1484 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.3610...  0.1486 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.3786...  0.1491 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3470...  0.1487 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3518...  0.1486 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.3731...  0.1476 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3566...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3382...  0.1488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.3818...  0.1480 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3623...  0.1481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3558...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.5137...  0.1507 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.3925...  0.1488 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.3885...  0.1487 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.3924...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3510...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3331...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.3796...  0.1489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3545...  0.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.3744...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.3716...  0.1508 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3607...  0.1491 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3585...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.3773...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.3829...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3648...  0.1474 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3405...  0.1466 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.3817...  0.1502 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.3982...  0.1489 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.3831...  0.1488 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.3995...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3572...  0.1544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.3826...  0.1515 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3633...  0.1507 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.3868...  0.1532 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.3715...  0.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3298...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3356...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.3804...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.3832...  0.1488 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.3925...  0.1474 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3624...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3416...  0.1461 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.3714...  0.1471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.3703...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3516...  0.1474 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.3765...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3416...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3260...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3168...  0.1494 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3494...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3448...  0.1498 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.4003...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3493...  0.1476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3357...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.3768...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3297...  0.1506 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3511...  0.1460 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3595...  0.1469 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3550...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.3786...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3319...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.4072...  0.1486 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.3618...  0.1469 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.3757...  0.1467 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.3526...  0.1465 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.3658...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.3769...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3524...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3324...  0.1471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.4001...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.3755...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4063...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.3886...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.3715...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3582...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.3660...  0.1534 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.3801...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3457...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.3741...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3468...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4128...  0.1469 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.3804...  0.1467 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.3962...  0.1503 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3520...  0.1489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3670...  0.1494 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.3822...  0.1475 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3530...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3457...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3144...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.3714...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3255...  0.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3568...  0.1488 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3271...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3542...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3333...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3576...  0.1474 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3307...  0.1493 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3388...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3305...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.3732...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3358...  0.1486 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3490...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3335...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3293...  0.1476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3372...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.3556...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.3713...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3235...  0.1475 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3288...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3283...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.3621...  0.1482 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3471...  0.1488 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3471...  0.1491 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3624...  0.1469 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3541...  0.1464 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3549...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.3596...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3578...  0.1484 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3373...  0.1487 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.3690...  0.1491 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3430...  0.1502 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.3557...  0.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3586...  0.1476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3376...  0.1484 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3302...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3102...  0.1472 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3497...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3562...  0.1508 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3467...  0.1487 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3508...  0.1501 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3562...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3102...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3149...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3524...  0.1491 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3405...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.2997...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.3660...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3551...  0.1489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3384...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3177...  0.1492 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3058...  0.1489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3287...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.3739...  0.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3601...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.3614...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3548...  0.1475 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.3779...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3582...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3562...  0.1505 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3524...  0.1484 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4070...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3647...  0.1493 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3398...  0.1467 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.3868...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3294...  0.1472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.3782...  0.1494 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.3655...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.3921...  0.1489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.3798...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3439...  0.1475 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3162...  0.1486 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3187...  0.1475 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3625...  0.1498 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3440...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3335...  0.1482 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3517...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3683...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3504...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3138...  0.1480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.3742...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.3849...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3426...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3520...  0.1471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3405...  0.1467 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3628...  0.1482 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3498...  0.1490 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.3663...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4183...  0.1482 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3579...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3491...  0.1471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3413...  0.1482 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3335...  0.1485 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.3712...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3459...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3649...  0.1500 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3186...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3278...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.3730...  0.1476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3282...  0.1473 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3184...  0.1470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3264...  0.1481 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3417...  0.1479 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3536...  0.1489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3307...  0.1495 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3381...  0.1477 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3261...  0.1487 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.3768...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3395...  0.1471 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3449...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3475...  0.1476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3214...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3285...  0.1493 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3482...  0.1483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3282...  0.1511 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3105...  0.1478 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3521...  0.1476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3445...  0.1468 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3204...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.4756...  0.1466 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3727...  0.1489 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3658...  0.1479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3756...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3249...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3180...  0.1502 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3601...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3400...  0.1472 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3605...  0.1483 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3398...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3308...  0.1507 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3518...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3577...  0.1486 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3663...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3306...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3252...  0.1471 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3605...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.3758...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3522...  0.1466 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3659...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3411...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3617...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3426...  0.1519 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3594...  0.1531 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3515...  0.1502 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.3065...  0.1501 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3095...  0.1492 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3698...  0.1501 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3599...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3650...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3397...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3196...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3529...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3498...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3398...  0.1505 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3469...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3261...  0.1482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.2937...  0.1491 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.2835...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3274...  0.1498 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3170...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.3793...  0.1488 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3310...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3181...  0.1471 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3451...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3102...  0.1482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3358...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3332...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3293...  0.1479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3538...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3126...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.3869...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3407...  0.1489 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3508...  0.1491 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3313...  0.1511 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3435...  0.1512 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3527...  0.1467 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3216...  0.1482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3081...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.3696...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3547...  0.1470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.3907...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3651...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3511...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3351...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3453...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3433...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3182...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3468...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3286...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.3827...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3562...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.3740...  0.1497 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3329...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3323...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3538...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3398...  0.1489 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3286...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.3032...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3457...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.2986...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3276...  0.1486 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.3034...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3392...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3193...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3243...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3147...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3214...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.3074...  0.1479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3446...  0.1472 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3059...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3286...  0.1489 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.2940...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3023...  0.1500 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3105...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3506...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3319...  0.1482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.2976...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3155...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.2991...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3407...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3237...  0.1472 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3268...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3147...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3247...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3295...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3371...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3356...  0.1491 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3190...  0.1470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3455...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3077...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3333...  0.1494 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3300...  0.1492 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3202...  0.1474 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.3000...  0.1467 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.2860...  0.1497 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3298...  0.1482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3370...  0.1507 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3246...  0.1501 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3330...  0.1486 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3312...  0.1492 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2986...  0.1493 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.2864...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3340...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3253...  0.1492 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.2847...  0.1495 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3379...  0.1490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3414...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3170...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.2916...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.2772...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3030...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3569...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3437...  0.1479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3384...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3210...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3636...  0.1483 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3458...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3404...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3323...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.3820...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3467...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3141...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.3639...  0.1482 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3125...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3558...  0.1483 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3404...  0.1465 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.3620...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3562...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3288...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.2917...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3012...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3415...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3295...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3213...  0.1485 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3299...  0.1514 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3330...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3160...  0.1486 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.2977...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3468...  0.1465 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3541...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3358...  0.1479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3238...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3206...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3247...  0.1483 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3186...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3509...  0.1488 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.3876...  0.1479 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3427...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3236...  0.1504 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3205...  0.1487 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3176...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3563...  0.1493 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3233...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3413...  0.1496 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.2944...  0.1477 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3180...  0.1475 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3525...  0.1473 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.3048...  0.1467 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.3009...  0.1468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.3037...  0.1483 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3213...  0.1503 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3216...  0.1489 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3161...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3171...  0.1481 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3148...  0.1476 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3452...  0.1467 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3088...  0.1465 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3220...  0.1492 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3233...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.2984...  0.1480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.2999...  0.1474 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3229...  0.1469 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.3008...  0.1509 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.2751...  0.1478 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3290...  0.1470 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3210...  0.1484 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3003...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4554...  0.1468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3499...  0.1469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3362...  0.1503 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3479...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3061...  0.1489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.2889...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3429...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3222...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3246...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3232...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3128...  0.1492 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3306...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3256...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3403...  0.1480 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.3154...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.3019...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3365...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3491...  0.1488 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3321...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3515...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3160...  0.1469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3321...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3159...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3318...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3235...  0.1556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.2828...  0.1534 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.2878...  0.1503 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3372...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3428...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3403...  0.1489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3100...  0.1496 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.2999...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3319...  0.1468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3273...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.3038...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3225...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.2941...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.2895...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.2704...  0.1497 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3079...  0.1484 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.3062...  0.1484 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3551...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3070...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.2848...  0.1467 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3313...  0.1487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.2839...  0.1487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3073...  0.1468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.3108...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3115...  0.1487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3388...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.2869...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3657...  0.1484 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3282...  0.1493 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3308...  0.1472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3046...  0.1467 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3220...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3323...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3155...  0.1506 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.2982...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3476...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3315...  0.1495 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.3694...  0.1487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3525...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3275...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3177...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3327...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3395...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.3026...  0.1472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3209...  0.1466 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.2918...  0.1468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3635...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3407...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3470...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.3096...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3212...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3323...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3165...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3060...  0.1498 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.2730...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3167...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.2815...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3141...  0.1495 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.2833...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3072...  0.1465 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.2946...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3074...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.2827...  0.1511 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.3028...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.2827...  0.1484 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3159...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.2938...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.3088...  0.1472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.2824...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.2875...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.3003...  0.1489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3236...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3181...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.2739...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.2892...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.2803...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3181...  0.1472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.3064...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.3080...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3060...  0.1481 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.3071...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3185...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3186...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3157...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.3021...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3215...  0.1472 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.3030...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3177...  0.1488 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3221...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.3075...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.2870...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.2638...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3111...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3298...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3051...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3205...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3107...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2791...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2622...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3103...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.3100...  0.1496 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.2679...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3145...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3134...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.2864...  0.1486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2802...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2584...  0.1503 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.2907...  0.1489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3304...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3240...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3213...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3145...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3402...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3188...  0.1493 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3210...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3058...  0.1501 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3710...  0.1496 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3335...  0.1497 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.3074...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3427...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.2907...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3375...  0.1468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3159...  0.1488 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3415...  0.1488 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3302...  0.1476 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.2983...  0.1484 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.2791...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.2933...  0.1487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3250...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.3056...  0.1491 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3021...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.3080...  0.1480 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3152...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.2979...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.2793...  0.1490 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3287...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3268...  0.1470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3141...  0.1477 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3029...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3069...  0.1474 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3127...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.3061...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3322...  0.1510 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.3674...  0.1494 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3195...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3109...  0.1469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.3060...  0.1469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.2982...  0.1465 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3444...  0.1469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3100...  0.1471 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3130...  0.1502 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.2852...  0.1497 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.2941...  0.1504 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3392...  0.1492 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.2950...  0.1487 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.2802...  0.1485 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.2812...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.3029...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.3086...  0.1475 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.2931...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.2955...  0.1481 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.2868...  0.1482 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3319...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.2985...  0.1489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.2988...  0.1489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.3123...  0.1479 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.2775...  0.1473 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.2860...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.2960...  0.1468 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.2858...  0.1469 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2714...  0.1483 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.3065...  0.1478 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.3000...  0.1502 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.2906...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4465...  0.1497 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3276...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.3114...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3230...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.2798...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2752...  0.1496 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3076...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.2926...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3185...  0.1478 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.3006...  0.1504 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.2958...  0.1472 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.3024...  0.1469 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3108...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3204...  0.1492 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.2850...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.2799...  0.1467 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3130...  0.1474 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3244...  0.1491 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.3083...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3344...  0.1474 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.3001...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3115...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.2944...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3284...  0.1474 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3072...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2749...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.2772...  0.1557 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3287...  0.1518 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3219...  0.1512 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3211...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.2868...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.2801...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3002...  0.1470 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3100...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.2942...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3115...  0.1472 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.2850...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2582...  0.1494 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2514...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.2852...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2789...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3332...  0.1474 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.2943...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2773...  0.1473 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3056...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.2754...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.2885...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.2988...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.2915...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3189...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.2815...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3408...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.3098...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3096...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.3003...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.3018...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3207...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.2893...  0.1499 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.2785...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3318...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.3062...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3447...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3250...  0.1491 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3142...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.2966...  0.1474 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3151...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3202...  0.1511 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.2838...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3081...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.2807...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3527...  0.1503 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3259...  0.1475 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3346...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.2849...  0.1495 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.2956...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3212...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.3038...  0.1478 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.2913...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2668...  0.1475 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.3024...  0.1478 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2679...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.3022...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2761...  0.1494 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.2898...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2831...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.2943...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.2795...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.2810...  0.1475 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2712...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.3055...  0.1495 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2756...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.2904...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2707...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.2596...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.2833...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3087...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.3034...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2609...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.2718...  0.1493 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2616...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.2946...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.2884...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.3000...  0.1509 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.2880...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.2900...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.2882...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.2964...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.2952...  0.1491 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.2794...  0.1496 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.3105...  0.1481 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.2830...  0.1493 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.2971...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.3018...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.2809...  0.1510 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2612...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2511...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.2972...  0.1474 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.3025...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.2962...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.2894...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.2868...  0.1496 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2711...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2519...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.3010...  0.1492 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.2812...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2459...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.3030...  0.1479 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.3020...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.2768...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2733...  0.1495 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2390...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2699...  0.1517 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3149...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.2898...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.3015...  0.1478 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.2922...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3209...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.3213...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.3040...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.2982...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3493...  0.1515 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3163...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.2831...  0.1483 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3243...  0.1475 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2820...  0.1472 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3179...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.3003...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3274...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3234...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.2834...  0.1509 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2686...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2700...  0.1478 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3023...  0.1477 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.2849...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.2844...  0.1493 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.2908...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.3009...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.2818...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2641...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3144...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3159...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.2976...  0.1473 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.2955...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.2879...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.2907...  0.1481 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.2883...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3101...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3509...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.3036...  0.1491 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.2951...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.2828...  0.1471 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.2783...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3225...  0.1492 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.2934...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.3084...  0.1486 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2631...  0.1492 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.2820...  0.1492 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3215...  0.1492 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.2698...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.2670...  0.1491 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.2740...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.2807...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.2959...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.2724...  0.1488 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.2869...  0.1514 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.2726...  0.1504 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3172...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.2811...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.2881...  0.1476 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.2903...  0.1480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2626...  0.1489 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.2682...  0.1485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.2874...  0.1484 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2667...  0.1490 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2496...  0.1506 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.2938...  0.1482 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2760...  0.1487 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2825...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4272...  0.1470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.3114...  0.1482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.2928...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3199...  0.1512 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2779...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2483...  0.1484 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.2913...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.2912...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.3038...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.2817...  0.1485 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.2741...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.2889...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.2950...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.2912...  0.1502 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.2777...  0.1482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2636...  0.1470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3132...  0.1471 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.3060...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.2898...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3115...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.2850...  0.1487 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.3076...  0.1489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.2831...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.3003...  0.1472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.2938...  0.1504 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2427...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2650...  0.1482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.3126...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.2917...  0.1523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3023...  0.1527 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2724...  0.1496 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2568...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.2879...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.2891...  0.1484 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.2745...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.2931...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2667...  0.1472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2470...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2379...  0.1496 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2722...  0.1497 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2648...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3266...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2755...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2634...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.2933...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2565...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2822...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.2793...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.2775...  0.1489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.3046...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2578...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3235...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.2850...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.2930...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2817...  0.1489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.2873...  0.1483 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.3026...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2758...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2593...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3228...  0.1469 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.2946...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3281...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3140...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.2870...  0.1512 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.2839...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.2988...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3005...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2767...  0.1471 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.2943...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2690...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3261...  0.1489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.3121...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3147...  0.1503 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2651...  0.1492 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.2924...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.3187...  0.1487 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.2943...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2689...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2440...  0.1470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.2836...  0.1515 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2491...  0.1489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.2874...  0.1494 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2529...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2756...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2549...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.2809...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2604...  0.1471 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2686...  0.1461 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2545...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.2849...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2628...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2735...  0.1497 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2563...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2543...  0.1482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2581...  0.1484 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.3007...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.2914...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2408...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2608...  0.1499 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2567...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.2795...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2719...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.2833...  0.1496 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2686...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2673...  0.1471 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2642...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2904...  0.1487 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.2892...  0.1487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2646...  0.1471 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.2896...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2661...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.2914...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2880...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2697...  0.1466 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2506...  0.1483 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2400...  0.1492 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.2702...  0.1472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.2921...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2744...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2811...  0.1482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.2779...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2480...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2403...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2729...  0.1466 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2733...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2374...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.2887...  0.1489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2811...  0.1470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2609...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2377...  0.1492 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2364...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2633...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.2940...  0.1481 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.2818...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.2831...  0.1484 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.2811...  0.1514 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.3077...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.2937...  0.1491 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.2864...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.2858...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3296...  0.1500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.2903...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.2697...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3161...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2636...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3043...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.2862...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3089...  0.1486 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3014...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.2766...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2431...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2539...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.2935...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2691...  0.1487 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2731...  0.1511 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2690...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.2812...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2704...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2433...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.2904...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.2945...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.2874...  0.1483 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.2822...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2765...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2751...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2681...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.3096...  0.1497 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3313...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.2911...  0.1491 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.2785...  0.1488 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2648...  0.1471 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2701...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.3068...  0.1479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.2724...  0.1490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.2829...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2465...  0.1492 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2700...  0.1476 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3118...  0.1465 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2557...  0.1473 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2486...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2499...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2712...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2849...  0.1475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2701...  0.1474 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2765...  0.1472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2512...  0.1472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3069...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2638...  0.1467 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2715...  0.1503 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.2722...  0.1485 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2489...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2556...  0.1472 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.2769...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2580...  0.1480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2382...  0.1478 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.2751...  0.1477 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2738...  0.1491 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2498...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.4048...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.2962...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.2768...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.2993...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2541...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2385...  0.1472 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2833...  0.1487 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2694...  0.1489 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2837...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2713...  0.1527 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2646...  0.1495 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2676...  0.1493 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2773...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.2810...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2569...  0.1476 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2544...  0.1464 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2871...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.2888...  0.1471 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.2761...  0.1491 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.2928...  0.1489 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2671...  0.1502 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.2865...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2713...  0.1486 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.3016...  0.1476 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2769...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2374...  0.1498 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2452...  0.1473 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.2891...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.2760...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.2803...  0.1503 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2595...  0.1507 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2470...  0.1529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.2774...  0.1500 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.2884...  0.1486 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2584...  0.1475 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2704...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2484...  0.1494 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2375...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2300...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2668...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2423...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3111...  0.1487 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2524...  0.1506 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2387...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.2701...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2437...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2607...  0.1473 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2620...  0.1487 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.2694...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.2865...  0.1504 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2484...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3085...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2715...  0.1493 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2800...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2661...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2735...  0.1472 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.2836...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2669...  0.1468 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2357...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.2945...  0.1497 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.2753...  0.1472 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3133...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.2946...  0.1495 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.2862...  0.1475 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2610...  0.1471 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.2774...  0.1468 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.2796...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2517...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2769...  0.1503 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2635...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3080...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.2875...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.3020...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2566...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2658...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.2975...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2770...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2573...  0.1529 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2284...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2723...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2341...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2749...  0.1475 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2458...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2664...  0.1473 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2473...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2595...  0.1472 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2473...  0.1501 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2565...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2411...  0.1486 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2827...  0.1492 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2537...  0.1489 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2664...  0.1487 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2408...  0.1489 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2387...  0.1491 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2507...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.2799...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2787...  0.1497 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2318...  0.1484 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2468...  0.1487 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2339...  0.1492 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2659...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2518...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2644...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2658...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2621...  0.1485 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2610...  0.1487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2671...  0.1484 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2724...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2559...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.2791...  0.1496 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2506...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2799...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2730...  0.1494 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2610...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2424...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2255...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2682...  0.1509 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.2820...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2689...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2662...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2649...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2338...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2265...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2652...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2571...  0.1471 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2272...  0.1471 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2801...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2687...  0.1476 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2507...  0.1471 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2304...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2214...  0.1473 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2425...  0.1468 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.2878...  0.1475 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2697...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2705...  0.1475 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2575...  0.1484 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.2935...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2909...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2696...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2777...  0.1495 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3147...  0.1507 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2787...  0.1472 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2689...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.2995...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2508...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.2866...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2850...  0.1480 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.2979...  0.1490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.2971...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2631...  0.1476 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2376...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2337...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2724...  0.1494 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2581...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2593...  0.1506 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2590...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2681...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2540...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2367...  0.1486 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.2850...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2901...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2640...  0.1491 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2629...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2683...  0.1468 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2670...  0.1473 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2532...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.2838...  0.1501 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3269...  0.1470 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2731...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2725...  0.1468 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2613...  0.1484 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2581...  0.1495 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.2943...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2689...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2777...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2370...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2555...  0.1481 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.2937...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2456...  0.1498 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2408...  0.1479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2460...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2609...  0.1484 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2641...  0.1483 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2537...  0.1487 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2624...  0.1491 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2541...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.2874...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2575...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2596...  0.1488 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2605...  0.1489 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2383...  0.1486 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2447...  0.1482 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2610...  0.1478 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2421...  0.1477 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2242...  0.1510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2652...  0.1474 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2565...  0.1501 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2520...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.3944...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.2867...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2664...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.2796...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2375...  0.1477 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2220...  0.1513 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2730...  0.1476 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2584...  0.1496 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2777...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2723...  0.1504 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2478...  0.1481 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2613...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2692...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2838...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2395...  0.1488 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2392...  0.1489 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2700...  0.1508 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.2845...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2628...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.2942...  0.1488 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2471...  0.1481 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2715...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2588...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.2839...  0.1499 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2576...  0.1475 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2239...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2296...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.2833...  0.1481 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2672...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2792...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2536...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2266...  0.1477 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2633...  0.1512 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2697...  0.1512 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2521...  0.1519 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2624...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2322...  0.1510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2141...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2013...  0.1521 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2490...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2370...  0.1477 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.2975...  0.1487 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2492...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2299...  0.1470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2702...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2396...  0.1504 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2537...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2523...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2480...  0.1487 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2765...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2269...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.2987...  0.1470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2663...  0.1477 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2730...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2535...  0.1481 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2550...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2602...  0.1471 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2524...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2333...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.2938...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2693...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.3012...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.2761...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2677...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2545...  0.1491 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2711...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2862...  0.1476 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2490...  0.1503 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2658...  0.1489 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2483...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.2965...  0.1469 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.2804...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.2982...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2470...  0.1477 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2674...  0.1485 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.2862...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2658...  0.1498 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2556...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2074...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2719...  0.1468 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2283...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2687...  0.1514 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2389...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2486...  0.1485 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2342...  0.1467 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2455...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2330...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2381...  0.1471 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2342...  0.1466 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2643...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2361...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2432...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2291...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2297...  0.1506 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2409...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2674...  0.1512 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2687...  0.1491 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2129...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2306...  0.1487 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2375...  0.1496 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2620...  0.1488 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2554...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2493...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2541...  0.1480 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2404...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2471...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2534...  0.1475 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2622...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2507...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2618...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2445...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2590...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2557...  0.1493 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2503...  0.1492 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2273...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2144...  0.1477 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2482...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2597...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2523...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2556...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2439...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2091...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2120...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2524...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2473...  0.1468 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2073...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2578...  0.1489 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2583...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2341...  0.1512 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.2150...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.2046...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2314...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2773...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2528...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2616...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2523...  0.1488 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2774...  0.1465 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2721...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2672...  0.1470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2635...  0.1493 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3069...  0.1491 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2739...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2529...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.2870...  0.1475 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2346...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2784...  0.1483 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2698...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.2892...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2766...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2479...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2287...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2281...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2786...  0.1481 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2418...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2536...  0.1487 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2449...  0.1496 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2561...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2399...  0.1470 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2338...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2742...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2713...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2623...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2450...  0.1485 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2531...  0.1490 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2499...  0.1503 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2472...  0.1478 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2752...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.3160...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2642...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2565...  0.1471 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2519...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2484...  0.1493 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2861...  0.1488 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2614...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2533...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2226...  0.1505 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2436...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.2840...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2259...  0.1474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2234...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2409...  0.1479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2546...  0.1482 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2443...  0.1508 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2354...  0.1473 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2474...  0.1475 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2273...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.2810...  0.1485 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2532...  0.1476 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2392...  0.1469 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2526...  0.1494 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2220...  0.1472 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2333...  0.1485 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2462...  0.1480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2392...  0.1476 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2151...  0.1484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2526...  0.1487 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2507...  0.1486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2456...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3820...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2677...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2394...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2669...  0.1491 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2325...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2207...  0.1480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2490...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2487...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2643...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2587...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2436...  0.1510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2519...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2559...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2648...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2332...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2333...  0.1489 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2707...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2733...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2555...  0.1472 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2789...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2509...  0.1491 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2706...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2479...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.2774...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2601...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2190...  0.1472 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2220...  0.1468 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2718...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2613...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2712...  0.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2408...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2350...  0.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2499...  0.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2471...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2411...  0.1535 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2537...  0.1531 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2349...  0.1516 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.2097...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.1983...  0.1485 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2306...  0.1469 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2282...  0.1492 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.2880...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2372...  0.1513 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2246...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2528...  0.1491 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2240...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2354...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2352...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2493...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2616...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2255...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2837...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2491...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2601...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2361...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2421...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2700...  0.1492 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2362...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2307...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2856...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2638...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.2862...  0.1504 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2636...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2579...  0.1502 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2519...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2562...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2669...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2337...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2526...  0.1492 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2303...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.2858...  0.1514 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2746...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2790...  0.1489 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2366...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2471...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2645...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2486...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2363...  0.1497 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.2097...  0.1469 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2464...  0.1516 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2069...  0.1471 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2429...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2185...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2415...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2215...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2365...  0.1468 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2215...  0.1469 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2277...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2146...  0.1469 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2452...  0.1497 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2278...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2372...  0.1485 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2149...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2174...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2320...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2589...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2479...  0.1511 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.1982...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2277...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2242...  0.1480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2510...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2339...  0.1478 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2463...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2374...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2280...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2351...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2462...  0.1507 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2373...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2382...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2594...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2281...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2492...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2588...  0.1471 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2280...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2197...  0.1485 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.2021...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2429...  0.1464 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2487...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2335...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2386...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2398...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2061...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.1985...  0.1496 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2431...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2235...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.2015...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2427...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2389...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2262...  0.1472 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.2042...  0.1464 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.1953...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2273...  0.1495 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2603...  0.1499 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2456...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2465...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2488...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2743...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2607...  0.1475 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2414...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2521...  0.1506 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.2918...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2602...  0.1499 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2436...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2711...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2192...  0.1480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2664...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2494...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2716...  0.1468 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2639...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2431...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2091...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2137...  0.1504 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2622...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2283...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2276...  0.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2409...  0.1480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2467...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2395...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2117...  0.1476 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2636...  0.1469 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2677...  0.1491 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2521...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2442...  0.1483 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2482...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2342...  0.1481 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2434...  0.1466 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2553...  0.1468 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.3007...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2562...  0.1468 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2398...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2400...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2324...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2682...  0.1503 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2469...  0.1495 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2493...  0.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2098...  0.1493 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2296...  0.1495 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2631...  0.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2272...  0.1482 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2233...  0.1486 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2286...  0.1487 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2410...  0.1498 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2328...  0.1471 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2266...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2275...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2132...  0.1470 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2686...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2348...  0.1484 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2281...  0.1494 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2369...  0.1474 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2106...  0.1478 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2260...  0.1479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2445...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2271...  0.1473 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.2060...  0.1477 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2425...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2326...  0.1488 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2208...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3620...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2557...  0.1468 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2384...  0.1494 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2587...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2143...  0.1473 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.2050...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2385...  0.1473 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2453...  0.1517 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2494...  0.1493 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2538...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2208...  0.1471 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2331...  0.1486 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2458...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2431...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2218...  0.1497 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2130...  0.1467 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2580...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2633...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2526...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2684...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2382...  0.1486 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2524...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2335...  0.1473 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2654...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2404...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.2013...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2032...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2556...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2477...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2477...  0.1488 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2247...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2147...  0.1486 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2401...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2399...  0.1482 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2241...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2387...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2158...  0.1512 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.2014...  0.1528 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.1994...  0.1515 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2255...  0.1501 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2095...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2797...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2211...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.2161...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2418...  0.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2169...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2215...  0.1514 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2285...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2377...  0.1508 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2573...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2116...  0.1474 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2691...  0.1474 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2437...  0.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2514...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2256...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2381...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2438...  0.1487 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2280...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2109...  0.1508 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2874...  0.1486 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2452...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2779...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2526...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2435...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2312...  0.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2472...  0.1503 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2553...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2265...  0.1498 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2400...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2313...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2742...  0.1473 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2517...  0.1465 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2634...  0.1512 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2148...  0.1488 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2354...  0.1498 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2529...  0.1496 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2325...  0.1503 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2273...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.1937...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2439...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.2062...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2387...  0.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2143...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2297...  0.1488 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2156...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2263...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2125...  0.1491 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2196...  0.1473 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.2103...  0.1504 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2449...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.2243...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2301...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2053...  0.1482 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2056...  0.1489 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2204...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2419...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2462...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.1937...  0.1465 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2207...  0.1474 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.2169...  0.1463 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2354...  0.1471 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2226...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2340...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2251...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2215...  0.1466 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2192...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2418...  0.1494 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2358...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2199...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2588...  0.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2083...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2264...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2437...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2257...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2079...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.1940...  0.1465 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2293...  0.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2495...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2298...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2302...  0.1503 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2293...  0.1487 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.1971...  0.1474 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.1924...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2303...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2284...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.1895...  0.1495 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2327...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2326...  0.1475 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2133...  0.1489 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.1992...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1783...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2209...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2512...  0.1482 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2347...  0.1471 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2354...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2400...  0.1467 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2552...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2561...  0.1496 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2464...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2359...  0.1491 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2815...  0.1485 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2372...  0.1480 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2268...  0.1473 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2670...  0.1468 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2135...  0.1507 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2544...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2474...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2572...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2603...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2295...  0.1495 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.2001...  0.1482 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.2062...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2443...  0.1469 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2291...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2235...  0.1489 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2269...  0.1482 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2282...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2181...  0.1486 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.2051...  0.1502 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2620...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2462...  0.1494 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2378...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2275...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2434...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2347...  0.1482 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2281...  0.1500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2605...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.2883...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2489...  0.1474 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2392...  0.1474 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2333...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2260...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2577...  0.1513 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2291...  0.1492 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2491...  0.1501 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.2146...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2145...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2678...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2129...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.2098...  0.1477 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2204...  0.1478 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2312...  0.1479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2257...  0.1471 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2208...  0.1472 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2232...  0.1493 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2099...  0.1502 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2581...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2183...  0.1470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2243...  0.1481 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2315...  0.1487 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2058...  0.1483 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.2122...  0.1499 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2345...  0.1488 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2124...  0.1496 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.1955...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2256...  0.1484 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2189...  0.1476 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2118...  0.1481 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3555...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2519...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2305...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2518...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2007...  0.1499 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.2055...  0.1476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2339...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2224...  0.1474 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2373...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2310...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2217...  0.1489 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2381...  0.1510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2419...  0.1478 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2415...  0.1503 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2131...  0.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.2100...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2463...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2538...  0.1471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2331...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2606...  0.1471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2269...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2432...  0.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2203...  0.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2517...  0.1476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2335...  0.1476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1924...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.1993...  0.1476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2501...  0.1468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2445...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2472...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2188...  0.1500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.2073...  0.1475 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2316...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2236...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.2137...  0.1468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2296...  0.1471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.2090...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1808...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1805...  0.1518 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2263...  0.1520 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.2027...  0.1516 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2632...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2132...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.2011...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2388...  0.1501 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.1978...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2149...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2292...  0.1472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2241...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2416...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.1988...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2630...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2269...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2330...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2187...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2269...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2310...  0.1471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2248...  0.1478 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2128...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2699...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2343...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2702...  0.1494 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2418...  0.1495 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2356...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2258...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2351...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2383...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2122...  0.1494 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2278...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2113...  0.1474 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2687...  0.1494 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2429...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2583...  0.1498 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2088...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2307...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2492...  0.1478 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2255...  0.1508 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2107...  0.1472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.1838...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2237...  0.1475 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.1985...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2229...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.2047...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2164...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.1972...  0.1476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2263...  0.1474 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.2092...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2111...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.1955...  0.1502 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2304...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.2039...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2151...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.1955...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.1889...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2217...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2343...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2273...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.1932...  0.1494 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.2007...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.1959...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2267...  0.1493 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2114...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2234...  0.1468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2173...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2187...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2237...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2189...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2217...  0.1475 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2078...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2391...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.2167...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2237...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2267...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2156...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.2038...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.1850...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2276...  0.1489 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2270...  0.1489 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2192...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2264...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2149...  0.1476 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.1959...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1726...  0.1475 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2180...  0.1494 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2038...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1782...  0.1517 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2293...  0.1469 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2189...  0.1471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.2015...  0.1478 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1900...  0.1472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1796...  0.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2023...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2399...  0.1472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2208...  0.1474 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2286...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2272...  0.1496 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2430...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2484...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2236...  0.1469 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2291...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2705...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2353...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2173...  0.1501 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2562...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2092...  0.1498 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2469...  0.1478 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2355...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2479...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2489...  0.1474 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2219...  0.1472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.1993...  0.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.1994...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2341...  0.1479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2149...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2107...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2111...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2203...  0.1490 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2154...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.1971...  0.1478 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2383...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2396...  0.1483 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2338...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2206...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2254...  0.1501 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2200...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2171...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2431...  0.1474 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2823...  0.1465 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2324...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2261...  0.1485 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2114...  0.1504 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.2158...  0.1471 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2500...  0.1475 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2349...  0.1480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2348...  0.1468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.1911...  0.1465 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2188...  0.1486 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2530...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.1968...  0.1512 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.1948...  0.1495 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2086...  0.1487 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2236...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2191...  0.1473 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2147...  0.1489 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2133...  0.1481 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.2047...  0.1488 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2459...  0.1491 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2035...  0.1477 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2023...  0.1531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2119...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.2010...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.1981...  0.1491 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2165...  0.1482 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2050...  0.1468 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.1838...  0.1484 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2309...  0.1487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2127...  0.1472 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2155...  0.1483 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
<<<<<<< HEAD
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
=======
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
<<<<<<< HEAD
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3570_l128_v1.372.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l128_v2.002.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l128_v1.746.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l128_v1.628.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l128_v1.564.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l128_v1.521.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200_l128_v1.492.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400_l128_v1.465.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600_l128_v1.451.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800_l128_v1.437.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l128_v1.422.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200_l128_v1.413.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400_l128_v1.408.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600_l128_v1.401.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800_l128_v1.390.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l128_v1.387.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200_l128_v1.380.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400_l128_v1.376.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3570_l128_v1.372.ckpt\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
=======
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 23,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 24,
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fars,\n",
      "and was all to the stronged, but\n",
      "and how till the penced to stopped about. There a porsery as something.\n",
      "\n",
      "\"Why,\n",
      "see he was to have been and that, and the protence, and stell a lone is a sounded and the cally of his bow and the hands at its offer.\n",
      "\n",
      "\"Oh, and\n",
      "his than heard that would the mestonite.\"\n",
      "\n",
      "The consceoting too was to\n",
      "and send these threw to be\n",
      "headed of the servant. Stepan Arkadyevitch, saw he had been to anything too, she had as the serving at herself, her\n",
      "waired on the simple of asteritation.\n",
      "\n",
      "\"Oh, that was and with a bither and terrenting\n",
      "her of tell that, and a man it a bit, and that, and what we say any sours, and thinker that who will be brought out of that son's had to\n",
      "help\n",
      "her at in my\n",
      "bear in the sort of as as time impossible hald.\"\n",
      "\n",
      "\"I'm began,\" answered Alexey Alexandrovitch his conversation.\n",
      "\n",
      "\"Oh what's the money than's house at his brother's\n",
      "something, that he can't be so had\n",
      "and as into me his stounds,\" said Agitch. They was all the step that was an excrituous was\n",
      "a proprait all as the\n",
      "hat with a\n",
      "sollors of servade with section of\n",
      "him, and with the headed that this also tathed. The second of\n",
      "shance and as as into the comated with whone home\n",
      "and\n",
      "sat to see him and his stread, and he with his thought of\n",
      "himself.\n",
      "\n",
      "\"Oh, he wanted to the marthan of a charper of them is\n",
      "share of the man, that his stop, and through her, I\n",
      "said as they had been all to\n",
      "something of say in a cheachers of the most to he were time. They said to and who see it's a preture, as it had the stream with the\n",
      "passent of the stince, to such tealist.\"\n",
      "\n",
      "The part as was a stood and the same at the sisters, with the hale.\n",
      "\n",
      "The sorts of her than hustion with his whole hondentice. \"Tell the barras of at into the consection to say.\" Stepan Arkadyevitch had saw if she to contention of the starch hand and with a\n",
      "sailing, husbands that steps as seemed to his browing was one to say in a strack. \"Alexey Alexandrovitch was a constrost about her,\n",
      "when the part on the means of meance. I have be\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = \"checkpoints/____.ckpt\"\n",
    "checkpoint = \"checkpoints\\\\i3570_l128_v1.372.ckpt\"\n",
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farens. The man and this presence there was a long whell against the\n",
      "serfice. And she would have seen a dest of sorry of any arrangements,\n",
      "true. But the side house was in the money about the chief same, and\n",
      "that he had to say to all his face, with some significance for his wife\n",
      "that he had come and done to the penslition of a little and sounds of\n",
      "the society, and always was in her and horses, and that that they\n",
      "could not be a chalacteristic on her so second become in her soul that it\n",
      "was all sincerity, and her clothest having been talking to horror of\n",
      "a presine that was something was all his mother had no station.\n",
      "\n",
      "The conversation had suddenly had not been anything.\n",
      "\n",
      "Sometying her health she came over the services, had sat down again, and\n",
      "at the conversation was not in the strange shreed to the door, and with\n",
      "his sight of the party, the sick man thinking the secret of the mother\n",
      "when they had brought a humored friends, and the prayers that he could not\n",
      "let him any more and marking this subject, and they. The most of the\n",
      "steps about indeed and triude of accordance of the position that was\n",
      "steping himself. They went to those on its ordered and could only to\n",
      "chart how when they all and the more angry and the sight about it. The\n",
      "peasants would his chillish accounts and at the partious feeling of\n",
      "particularly ares, and so thore was some shorting or them. And he so\n",
      "see the place, who was seeing that something coughed hands, as though\n",
      "he was at the same, was not that she was insulted. Anna had been at\n",
      "the prince tell out his hat this article and to begin to say.\n",
      "\n",
      "\"Yive were a selvated the members who have taken the station in which the\n",
      "committee? And is to darling a minutate of the carriage of marnated. I have\n",
      "took to myself in the steps and a single morn and happy.\n",
      "\n",
      "\"If that's into a single morning, this seems thanks to her and thinks\n",
      "in the past and married that.\" The side had been carrying all the\n",
      "portrait of his words. These wires he was sitting at the past of the\n",
      "room, t\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
=======
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fard,. \"\"\n",
      "I\"\n",
      "\"The tat he winse he wos al alt ong the hor an thot hed the\n",
      "the sin the sand we he sos and se th the an soris he as oth thes sosed to he the he ad ond on har soron sha sas sitis he\n",
      "sentes the he wis as arton thir ans on ond wome to whe the sos thes sosit to the sos at hos ho she sis the she him an tot thes th an setit and wo he angeras,. Tho hor ho wot thime\n",
      "ne tot ha to we we he won thar al thir send tougtint ting and wh wamed, the the her what hot wit he wat ta hor,\n",
      "thing th whe tousederede sad sithe the the he se the han shes oress ho tot on here an on ot to he the hhressasesid thise where ans andentor tat the\n",
      "t the won he ans te hite wost hin sos torinnd\n",
      "tare wes to se her\n",
      "inntor ond tot ont ate\n",
      "silt wens\n",
      "thithe serede ho he the soros, sers on asis and af on he tare sin hos tot this won oth the he tins, and ang tee ar the ant oud thot woth shat ha lan and sithe to she sos thoth sar te sesilt ther singetin to he te artor to ho he\n",
      "wans he ans her han ase and. \n",
      "I hind wasd we\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farnt out a seenen a doons the werly a prostines, bet of to as sone him, al her his and and she had booke word and wert on her had and said to hoult, sto stat to she whon he wond to\n",
      "denting andond to stat had he he seed a sele to the corsions.\n",
      "\n",
      "\"The more when she shead, thourg that so he had and to the sort, sunding him his and and sace and where her\n",
      "headed he said\n",
      "the haress and than sheard her, her has thing his stalith his tongenter,\" she said\n",
      "he dasted the monting the sond of\n",
      "their that allong her ald\n",
      "have a tare him that to the somesily wourd to dive in and som her and whene the sead, and withon te said whine and had sail he having had\n",
      "same to sime her has\n",
      "antanter that\n",
      "she she distlidion of her heally.\n",
      "\n",
      "\"When, an the pinter on atone with\n",
      "the pasting, and sooked hus a sant of her his her ander was having to the panting to shingher at a shiss to thow to and walk nover as to the portanion,\" said Stepan'Akknavitcch with which all she coundsess in this his,\n",
      "boring the panitiou, and\n",
      "atonge\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farcial the\n",
      "confiring to the mone of the correm and thinds. She\n",
      "she saw the\n",
      "streads of herself hand only astended of the carres to her his some of the princess of which he came him of\n",
      "all that his white the dreasing of\n",
      "thisking the princess and with she was she had\n",
      "bettee a still and he was happined, with the pood on the mush to the peaters and seet it.\n",
      "\n",
      "\"The possess a streatich, the may were notine at his mate a misted\n",
      "and the\n",
      "man of the mother at the same of the seem her\n",
      "felt. He had not here.\n",
      "\n",
      "\"I conest only be alw you thinking that the partion\n",
      "of their said.\"\n",
      "\n",
      "\"A much then you make all her\n",
      "somether. Hower their centing\n",
      "about\n",
      "this, and I won't give it in\n",
      "himself.\n",
      "I had not come at any see it will that there she chile no one that him.\n",
      "\n",
      "\"The distiction with you all.... It was\n",
      "a mone of the mind were starding to the simple to a mone. It to be to ser in the place,\" said Vronsky.\n",
      "\"And a plais in\n",
      "his face, has alled in the consess on at they to gan in the sint\n",
      "at as that\n",
      "he would not be and t\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.5.3"
=======
   "version": "3.6.1"
>>>>>>> f86f04cf53bf8c932b58ac3579edea0236163650
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
